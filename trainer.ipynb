{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTShGp7DAxaloxqZqWNwR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/BabyGPT/blob/main/trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BabyGPT trainer."
      ],
      "metadata": {
        "id": "kG2T8jLsgekR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # these are default GPT-2 hyperparameters\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    bias :bool = False\n",
        "\n",
        "\n",
        "### other hyperparametres\n",
        "batch_size = 64 \n",
        "max_iters = 11000\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "words = open(r\"/content/ALL_eminem.txt\", 'r', encoding='utf-8').read()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "\n",
        "\n",
        "## train and split the data\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+ config.block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+ config.block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "### from pytorch GPT tutorial\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "\n",
        "    self.atten = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "    self.projection = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.n_embd, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y\n",
        "\n",
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),\n",
        "    nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BabyGPTmodel, self).__init__()\n",
        "\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "\n",
        "        self.config = config\n",
        "        self.token = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Transformer(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps = 1e-12) # final layer norm\n",
        "        self.lnum_heads = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        ## init all weights\n",
        "        ## from karpathy\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "          if pn.endswith('projection.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token(idx)\n",
        "        position_ids = torch.arange(0, T, dtype = torch.long, device = device).unsqueeze(0)\n",
        "        pos_emb =  self.positional_embeddings(position_ids)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "          x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lnum_heads(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "        \n",
        "    ## from karpathy's youtube videos.\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -config.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "config = GPTConfig(\n",
        "    block_size = 4,\n",
        "    vocab_size = len(chars),\n",
        "    n_head = 4,\n",
        "    n_layer = 4,\n",
        "    n_embd = 16)\n",
        "\n",
        "model = BabyGPTmodel(config)\n",
        "\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kflnFouSHdM1",
        "outputId": "bae4100f-a73a-4d93-8c65-29cf85d05b16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 16237\n",
            "step 0: train loss 4.6938, val loss 4.6940\n",
            "step 500: train loss 3.1438, val loss 3.1528\n",
            "step 1000: train loss 2.7306, val loss 2.7447\n",
            "step 1500: train loss 2.5631, val loss 2.5678\n",
            "step 2000: train loss 2.4872, val loss 2.4881\n",
            "step 2500: train loss 2.4434, val loss 2.4352\n",
            "step 3000: train loss 2.3868, val loss 2.4054\n",
            "step 3500: train loss 2.3752, val loss 2.3796\n",
            "step 4000: train loss 2.3494, val loss 2.3544\n",
            "step 4500: train loss 2.3332, val loss 2.3378\n",
            "step 5000: train loss 2.3089, val loss 2.3351\n",
            "step 5500: train loss 2.2960, val loss 2.3018\n",
            "step 6000: train loss 2.2871, val loss 2.3183\n",
            "step 6500: train loss 2.2893, val loss 2.2919\n",
            "step 7000: train loss 2.2833, val loss 2.2975\n",
            "step 7500: train loss 2.2749, val loss 2.3039\n",
            "step 8000: train loss 2.2697, val loss 2.2862\n",
            "step 8500: train loss 2.2619, val loss 2.2645\n",
            "step 9000: train loss 2.2641, val loss 2.2676\n",
            "step 9500: train loss 2.2591, val loss 2.2803\n",
            "step 10000: train loss 2.2590, val loss 2.2487\n",
            "step 10500: train loss 2.2496, val loss 2.2659\n",
            "step 10999: train loss 2.2437, val loss 2.2623\n",
            "\n",
            "What-pirh\n",
            "Poat be at \"Anppeale niter to gotbmich we Jaskever sacle thel hit, it's toua nonextrickivever with stack\n",
            "Toran mow\n",
            "It mes and I'm sublough and latsin'isite porate I'r tupkes bayntace wo0ch latck off it blike, to wolk is\n",
            "I you thing rums as, no hoome the samaste boxsio inert but no to frink that\n",
            "You kig?\n",
            "'I gottin the praGers what\n",
            "Tiersts wunke breay din for connet hin\n",
            "\n",
            "Bowe way yous preeame\n",
            "You gagke\n",
            "\n",
            "Bit a byuf iib? Brave be some opt alean 6 Therisose walfat\n",
            "Ind'rifp SrinÃºume as wit'e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### llama trainer"
      ],
      "metadata": {
        "id": "VCsAmIIPgVO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## from lit-llama repo (partially).\n",
        "\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from typing_extensions import Self\n",
        "\n",
        "@dataclass\n",
        "class LLaMAConfig:\n",
        "    ## LLaMa parametres\n",
        "    block_size: int = 2048\n",
        "    vocab_size: int = 32000\n",
        "    n_layer: int = 32\n",
        "    n_head: int = 32\n",
        "    n_embd: int = 4096\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_name(cls, name: str) -> Self:\n",
        "        return cls(**llama_configs[name])\n",
        "\n",
        "\n",
        "llama_configs = {\n",
        "    \"7B\": dict(n_layer=32, n_head = 32, n_embd=4096),\n",
        "    \"13B\": dict(n_layer=40, n_head =40, n_embd=5120),\n",
        "    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n",
        "    \"65B\": dict(n_layer=80, n_head =64, n_embd=8192),\n",
        "}\n",
        "\n",
        "\n",
        "### other hyperparametres\n",
        "\n",
        "batch_size = 64 \n",
        "max_iters = 11000\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "\n",
        "words = open(r\"/content/ALL_eminem.txt\", 'r', encoding='utf-8').read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "\n",
        "\n",
        "## train and split the data\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+ config.block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+ config.block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "### from pytorch GPT tutorial\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, config : LLaMAConfig):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.config = config\n",
        "    self.atten = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    self.projection = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.block_size = config.block_size\n",
        "    self.rope_cache: Optional[torch.Tensor] = None\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.n_embd, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "\n",
        "    if self.rope_cache is None:\n",
        "      # cache for future forward calls\n",
        "      self.rope_cache = build_rope_cache(\n",
        "      seq_len=self.block_size,\n",
        "      n_elem=self.n_embd // self.n_head, \n",
        "      dtype=x.dtype,\n",
        "      device=x.device,\n",
        "            )\n",
        "\n",
        "      q = apply_rope(q, self.rope_cache)\n",
        "      k = apply_rope(k, self.rope_cache)\n",
        "\n",
        "\n",
        "\n",
        "    #manual implementation of attention\n",
        "    #from karpathy\n",
        "    # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    # att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    # y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # efficient attention using Flash Attention CUDA kernels\n",
        "    y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
        "\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config: LLaMAConfig) -> None:\n",
        "    super().__init__()\n",
        "    hidden_dim = 4 * config.n_embd\n",
        "    n_hidden = int(2 * hidden_dim / 3)\n",
        "\n",
        "\n",
        "    self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
        "    self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
        "    self.c_proj = nn.Linear(n_hidden,  config.n_embd, bias=False)\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, config : LLaMAConfig) -> None:\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "    self.layer_norm_1 = RMSNorm(config.n_embd)\n",
        "    self.layer_norm_2 = RMSNorm(config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization.\n",
        "\n",
        "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
        "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(size))\n",
        "        self.eps = eps\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
        "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
        "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
        "        # x_normed = x / (rms_x + self.eps)\n",
        "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
        "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
        "        return self.scale * x_normed\n",
        "\n",
        "\n",
        "def build_rope_cache(seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000) -> torch.Tensor:\n",
        "\n",
        "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
        "\n",
        "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
        "    transformers/rope/__init__.py. MIT License:\n",
        "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
        "    \"\"\"\n",
        "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
        "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
        "\n",
        "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
        "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
        "\n",
        "    # Calculate the product of position index and $\\theta_i$\n",
        "    idx_theta = torch.outer(seq_idx, theta).float()\n",
        "\n",
        "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
        "\n",
        "    # this is to mimic the behaviour of complex32, else we will get different results\n",
        "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
        "        cache = cache.half()\n",
        "    return cache\n",
        "\n",
        "\n",
        "def apply_rope(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n",
        "    x = x.transpose(1, 2)\n",
        "\n",
        "    # truncate to support variable sizes\n",
        "    T = x.size(1)\n",
        "    rope_cache = rope_cache[:T]\n",
        "\n",
        "    # cast because the reference does\n",
        "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
        "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
        "    x_out2 = torch.stack(\n",
        "        [xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
        "         xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
        "        ], -1)\n",
        "\n",
        "    x_out2 = x_out2.flatten(3)\n",
        "    return x_out2.transpose(1, 2).type_as(x)\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BabyGPTmodel, self).__init__()\n",
        "\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "\n",
        "        self.config = config\n",
        "        self.token = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Transformer(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = RMSNorm(config.n_embd, eps = 1e-12) # final layer norm\n",
        "        self.lnum_heads = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        ## init all weights\n",
        "        ## from karpathy\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "          if pn.endswith('attention.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token(idx)\n",
        "        position_ids = torch.arange(0, T, dtype = torch.long, device = device).unsqueeze(0)\n",
        "        pos_emb =  self.positional_embeddings(position_ids)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "          x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lnum_heads(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "        \n",
        "    ## from karpathy's youtube videos.\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -config.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "config = LLaMAConfig(\n",
        "    block_size = 4,\n",
        "    vocab_size = len(chars),\n",
        "    n_head = 4,\n",
        "    n_layer = 4,\n",
        "    n_embd = 16)\n",
        "\n",
        "model = BabyGPTmodel(config)\n",
        "\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE_NRBZXbLW7",
        "outputId": "31fe9627-e67e-4a31-8990-94261a115bdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 16221\n",
            "step 0: train loss 4.6937, val loss 4.6937\n",
            "step 500: train loss 3.1929, val loss 3.1976\n",
            "step 1000: train loss 2.7869, val loss 2.7994\n",
            "step 1500: train loss 2.5804, val loss 2.5644\n",
            "step 2000: train loss 2.4740, val loss 2.4840\n",
            "step 2500: train loss 2.4136, val loss 2.4224\n",
            "step 3000: train loss 2.3908, val loss 2.3613\n",
            "step 3500: train loss 2.3396, val loss 2.3496\n",
            "step 4000: train loss 2.3144, val loss 2.3300\n",
            "step 4500: train loss 2.2939, val loss 2.2975\n",
            "step 5000: train loss 2.2758, val loss 2.2965\n",
            "step 5500: train loss 2.2653, val loss 2.2642\n",
            "step 6000: train loss 2.2356, val loss 2.2579\n",
            "step 6500: train loss 2.2375, val loss 2.2483\n",
            "step 7000: train loss 2.2300, val loss 2.2480\n",
            "step 7500: train loss 2.2402, val loss 2.2513\n",
            "step 8000: train loss 2.2317, val loss 2.2301\n",
            "step 8500: train loss 2.2147, val loss 2.2079\n",
            "step 9000: train loss 2.2162, val loss 2.2254\n",
            "step 9500: train loss 2.1962, val loss 2.2249\n",
            "step 10000: train loss 2.1895, val loss 2.2101\n",
            "step 10500: train loss 2.1978, val loss 2.2040\n",
            "step 10999: train loss 2.2015, val loss 2.2009\n",
            "\n",
            "Oh a he spoments\n",
            "Ohs and whet me.0\n",
            "Jay, Dre hanf inche, buh, I'm herere a win'\n",
            "So to saiidid\n",
            "I I'mf neved, I donse efy fake'ind merile (I'd nobqu cad-shegeds..]\n",
            "Get withank to I what baut the gudin' I'm no]\n",
            "Now oun the cuckedid in\n",
            "I gine we dey (Be be doup I cloe that in a treve I ack pith my!\n",
            "?\n",
            "Thhide was ueves sueel The ray heB*Red, yeu the me won beall you cham?\n",
            "This if to my ona um a lating ut Dof'\n",
            "Just to to sarse mighior mey (namale me\n",
            "Heats to ine Ack moy t'\n",
            "She good whene core pruting yo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both The models starts to converge towards the end, which can be fixed. Due to the use of Cuda efficient attention, the llama tokens seem more readable. "
      ],
      "metadata": {
        "id": "-je2R6-HgkQK"
      }
    }
  ]
}