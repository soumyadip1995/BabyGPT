{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAhRGKJGPIV6xlWdk6m3Fj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/BabyGPT/blob/main/Notebook/lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads, rank):\n",
        "    super(Attention, self).__init__()\n",
        "    self.rank = rank\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.W_A = nn.Parameter(torch.empty(embedded_dim, rank))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    \n",
        "\n",
        "    \n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * ((1.0 / math.sqrt(k.size(-1)) * self.rank))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = att @ (self.W_A)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "_PF4pa1OUkIQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "srO-4qIOUlSd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads, rank):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim, num_heads, rank)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "mKR1Y7p2Us5F"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, rank):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads, rank) for _ in range(num_layers)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "f3vDjpPuUtsu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size =  478\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "rank = 4\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads, rank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWQ8_ioEmwkf",
        "outputId": "71f3c9cb-9e22-46ed-8a99-89c3daeb2651"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29246\n",
            "number of parameters: 29246\n",
            "number of parameters: 29246\n",
            "number of parameters: 29246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A comparison between BabyGPT and Low rank adaptation. BabyGPT is b/w  28k-29k parametres. Low rank adaptation improves the parametre efficiency. 15k parametres. \n",
        "\n",
        "Note: Parametre size is also directly linked to context length."
      ],
      "metadata": {
        "id": "KPKToZKsqPHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 16\n",
        "\n",
        "W_A = nn.Parameter(torch.empty(input_dim, rank))\n",
        "W_A.shape"
      ],
      "metadata": {
        "id": "N3uQZD2AoTeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LowRankAttention(nn.Module):\n",
        "    def __init__(self, dim, rank):\n",
        "        super(LowRankAttention, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.Wq = nn.Linear(dim, rank, bias=False)\n",
        "        self.Wk = nn.Linear(dim, rank, bias=False)\n",
        "        self.Wv = nn.Linear(dim, rank, bias=False)\n",
        "        self.Wo = nn.Linear(rank, dim, bias=False)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        Q = self.Wq(q)\n",
        "        K = self.Wk(k)\n",
        "        V = self.Wv(v)\n",
        "\n",
        "        # Compute the attention scores using low-rank approximation\n",
        "        A = torch.bmm(Q, K.transpose(-2, -1)) / (self.rank ** 0.5)\n",
        "\n",
        "        # Softmax along the key dimension\n",
        "        A = torch.softmax(A, dim=-1)\n",
        "\n",
        "        # Compute the attention-weighted values using low-rank approximation\n",
        "        AV = torch.bmm(A, V)\n",
        "\n",
        "        # Apply the output layer to the attention-weighted values\n",
        "        out = self.Wo(AV)\n",
        "\n",
        "        return out\n",
        "\n",
        "class LowRankTransformerLayer(nn.Module):\n",
        "    def __init__(self, dim, rank, dropout=0.2):\n",
        "        super(LowRankTransformerLayer, self).__init__()\n",
        "        self.attention = LowRankAttention(dim, rank)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 3),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 3, dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the self-attention layer\n",
        "        attention_out = self.attention(x, x, x)\n",
        "\n",
        "        # Add residual connection and normalize\n",
        "        x = self.norm1(x + self.dropout1(attention_out))\n",
        "\n",
        "        # Feed-forward layer\n",
        "        ff_out = self.feedforward(x)\n",
        "\n",
        "        # Add residual connection and normalize\n",
        "        x = self.norm2(x + self.dropout2(ff_out))\n",
        "\n",
        "        return x\n",
        "\n",
        "class LowRankTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers, dim, rank, num_heads, dropout= 0.2):\n",
        "        super(LowRankTransformer, self).__init__()\n",
        "        self.layers = nn.ModuleList([LowRankTransformerLayer(dim, rank, dropout) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "        self.dim = dim\n",
        "        self.rank = rank\n",
        "        self.num_heads = num_heads\n",
        "        self.pos_embedding = nn.Embedding(vocab_size, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # init all weights\n",
        "        ## from karpathy\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "          if pn.endswith('Wo.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "            # report number of parameters\n",
        "            print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding[:, :x.size(1)]\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "qD35ejNYzi59"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrt = LowRankTransformer(478, 4, 16, 4, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD91mbOizkTG",
        "outputId": "ec89d3ff-dc1c-4648-f1fd-dca2af7e69f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open(r\"/content/text.txt\", 'r', encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "vocab_size = len(chars)\n",
        "lrt = LowRankTransformer(vocab_size, 4, 16, 4, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47TM6blM71y3",
        "outputId": "d9be856f-22d2-4420-c708-e37467e9176c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### scaling. \n",
        "\n",
        "We check for the FLOPs"
      ],
      "metadata": {
        "id": "-jkjnPRWQALv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "print(data)\n",
        "data.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-p730EcKRHo",
        "outputId": "32dee210-1c7d-40bb-ce62-d3d78a0a8a3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"In': 0, '\"No\"': 1, \"'Cause\": 2, \"'bout\": 3, \"'cause\": 4, \"'em\": 5, '(window)': 6, '24/7': 7, 'About': 8, 'Air': 9, 'And': 10, 'Anyways,': 11, 'Before': 12, 'Bonnie': 13, 'But': 14, 'Call': 15, 'Collins,': 16, 'Come': 17, 'Damn!': 18, 'Dear': 19, 'Denver,': 20, \"Don't\": 21, 'Fans': 22, 'For': 23, 'Good': 24, 'Got': 25, 'He': 26, 'Hey': 27, \"How's\": 28, 'I': 29, \"I'd\": 30, \"I'll\": 31, \"I'm\": 32, \"I'ma\": 33, 'If': 34, 'It': 35, \"It's\": 36, 'Just': 37, 'Look,': 38, 'Matthew': 39, 'Mr.': 40, 'My': 41, 'Night\"': 42, 'Now': 43, 'Oh': 44, 'Or': 45, 'P.S.': 46, 'Phil': 47, 'Rawkus': 48, 'Remember': 49, 'Ronnie': 50, 'See': 51, 'See,': 52, 'She': 53, 'Sincerely': 54, 'Skam': 55, 'Slim,': 56, 'So': 57, 'Some': 58, 'Sometimes': 59, 'Stan': 60, 'Stan,': 61, 'Starter': 62, 'That': 63, \"That's\": 64, 'The': 65, 'There': 66, 'This': 67, 'To': 68, 'Too': 69, 'Try': 70, 'We': 71, 'Well,': 72, 'Why': 73, 'Write': 74, 'You': 75, 'a': 76, 'about': 77, 'about,': 78, 'across': 79, 'addresses': 80, 'adrenaline,': 81, 'ago': 82, \"ain't\": 83, 'all': 84, 'all,': 85, 'almost': 86, 'along': 87, 'always': 88, 'am': 89, 'an': 90, 'and': 91, 'answer': 92, 'anyways,': 93, 'are': 94, 'as': 95, 'ass': 96, 'at': 97, 'autograph': 98, 'autumn,': 99, 'away': 100, 'back': 101, 'bad': 102, 'be': 103, 'beat': 104, 'bed': 105, 'been': 106, \"bein'\": 107, 'better': 108, 'biggest': 109, 'bitch': 110, 'bitch,': 111, 'bleeds': 112, 'blistering': 113, 'bottom': 114, 'bouncing': 115, 'breathe': 116, 'bridge': 117, 'brother': 118, 'brother,': 119, 'busy': 120, 'but': 121, 'by': 122, 'call': 123, 'called': 124, \"callin'\": 125, 'can': 126, \"can't\": 127, 'cap': 128, 'car': 129, 'cassette': 130, 'cell,': 131, 'chance': 132, 'chat,': 133, 'cheat': 134, 'chest': 135, 'clouds': 136, \"clownin',\": 137, 'cold': 138, 'cold,': 139, 'come': 140, 'concert': 141, 'conscience': 142, 'could': 143, \"could've\": 144, 'coulda': 145, 'counseling': 146, 'couple': 147, 'crazy': 148, 'cut': 149, 'dare': 150, 'daughter': 151, 'daughter,': 152, 'daughter?': 153, 'dawg,': 154, 'day,': 155, 'depressed': 156, 'deserve': 157, 'did': 158, \"didn't\": 159, \"didn't,\": 160, 'die': 161, 'diss': 162, 'do': 163, 'does': 164, \"doin'\": 165, 'doing': 166, \"don't\": 167, 'down': 168, 'downers': 169, 'drank': 170, 'dream': 171, 'drift': 172, 'drive?': 173, 'drove': 174, 'drowning': 175, 'drowsy': 176, 'drunk': 177, 'dude': 178, 'each': 179, 'eats': 180, 'else,': 181, 'even': 182, 'ever': 183, 'every': 184, 'everything': 185, 'fan': 186, 'fans': 187, 'far': 188, 'father': 189, 'fifth': 190, 'fine': 191, 'flattered': 192, 'for': 193, 'forgot,': 194, 'found': 195, 'four': 196, 'freeway': 197, 'friend': 198, 'from': 199, 'fuck': 200, 'fucked': 201, \"fuckin'\": 202, 'full': 203, 'get': 204, 'girlfriend': 205, \"girlfriend's\": 206, 'glad': 207, 'go,': 208, 'gone': 209, 'got': 210, 'gotta': 211, 'gray': 212, \"growin'\": 213, 'guess': 214, 'guy': 215, 'had': 216, 'have': 217, 'he': 218, \"he's\": 219, 'hear': 220, 'help': 221, 'helps': 222, 'her': 223, 'her?': 224, \"here's\": 225, 'him': 226, 'him?': 227, 'himself': 228, 'his': 229, 'hit': 230, 'home': 231, 'hope': 232, 'hours': 233, 'how': 234, 'hurt': 235, 'idol': 236, 'if': 237, 'in': 238, 'inspire': 239, 'intentionally': 240, 'is': 241, 'is,': 242, 'issues,': 243, 'it': 244, \"it'll\": 245, \"it's\": 246, 'it,': 247, 'it?': 248, 'jealous': 249, 'jot': 250, 'just': 251, 'kid': 252, 'kill': 253, 'kinda': 254, 'knew': 255, 'know': 256, 'last': 257, 'late,': 258, 'left': 259, 'letter': 260, 'letter,': 261, 'letters': 262, 'letters,': 263, 'lied': 264, 'like': 265, 'likes': 266, 'little': 267, 'little,': 268, 'lose': 269, 'lousy': 270, 'love': 271, 'mad': 272, 'mad,': 273, 'mad?': 274, 'made': 275, 'make': 276, 'man': 277, 'man,': 278, 'man?': 279, 'maybe': 280, 'me': 281, 'me,': 282, 'meant': 283, 'meet': 284, 'met': 285, 'missed': 286, 'mom': 287, 'months': 288, 'more': 289, 'more,': 290, 'morning': 291, 'much': 292, 'must': 293, \"must've\": 294, 'my': 295, 'myself': 296, 'name': 297, 'need': 298, 'neither': 299, 'never': 300, 'news': 301, 'ninety': 302, 'no': 303, 'not': 304, \"not've\": 305, 'now': 306, 'now,': 307, 'of': 308, 'off': 309, 'office': 310, 'old': 311, 'on': 312, 'on,': 313, 'one': 314, 'only': 315, 'or': 316, 'other': 317, 'out': 318, 'out?': 319, 'outside': 320, 'over': 321, 'package': 322, 'pager,': 323, 'pain': 324, 'people': 325, 'perfect': 326, 'phat': 327, 'phone': 328, 'picture': 329, 'pictures': 330, 'pictures,': 331, 'post': 332, 'posters': 333, 'pregnant': 334, 'pretty': 335, 'probably': 336, 'problem': 337, 'put': 338, 'rain': 339, 'reaches': 340, 'read': 341, 'real,': 342, 'really': 343, 'relate': 344, 'relax': 345, 'reminds': 346, 'rescued': 347, 'respect': 348, 'right': 349, 'ripped': 350, 'room': 351, 'ruined': 352, 'rush': 353, 'said': 354, 'said,': 355, 'saved': 356, 'saw': 357, 'say': 358, 'saying': 359, 'scream': 360, \"screamin'\": 361, 'scribble': 362, 'see': 363, 'seen': 364, 'send': 365, 'sending': 366, 'sent': 367, 'she': 368, \"she'll\": 369, 'she?': 370, 'shit': 371, \"shit'll\": 372, 'shit,': 373, 'shitty': 374, 'shitty,': 375, 'should': 376, 'show': 377, 'show,': 378, 'shut': 379, 'sick': 380, 'signed': 381, 'six': 382, 'sleep': 383, 'slit': 384, 'sloppy': 385, 'so': 386, 'some': 387, \"somethin'\": 388, 'song': 389, 'songs': 390, 'sooner': 391, 'sorry': 392, 'still': 393, 'such': 394, 'sudden': 395, 'suffer': 396, 'suffocates,': 397, 'supposed': 398, 'talk': 399, 'tape,': 400, 'tattoo': 401, \"tea's\": 402, 'tell': 403, 'than': 404, 'that': 405, \"that's\": 406, 'the': 407, 'then': 408, 'they': 409, 'think': 410, 'this': 411, 'this,': 412, 'though,': 413, 'thousand': 414, 'throat,': 415, 'tied': 416, 'time': 417, 'to': 418, 'to,': 419, 'together': 420, 'together,': 421, 'together?': 422, 'too': 423, 'too,': 424, 'too?': 425, 'treat': 426, 'truly': 427, 'trunk': 428, 'trunk,': 429, 'tryna': 430, 'two': 431, 'type': 432, 'uncle': 433, 'underground': 434, 'understand,': 435, 'up': 436, 'up,': 437, 'us': 438, 'used': 439, 'vodka': 440, 'waited': 441, 'wall': 442, 'walls': 443, 'wanna': 444, 'want': 445, 'wanted': 446, 'wants': 447, 'was': 448, 'was,': 449, 'way': 450, 'we': 451, 'weeks': 452, 'what': 453, \"what's\": 454, 'when': 455, 'who': 456, 'why': 457, 'will': 458, 'window': 459, 'with': 460, 'without': 461, 'wondering': 462, 'word,': 463, 'would': 464, 'wrists': 465, 'write': 466, 'wrote': 467, 'wrote,': 468, 'years': 469, 'you': 470, \"you'll\": 471, \"you're\": 472, 'you,': 473, 'you?': 474, 'your': 475, 'yours,': 476, 'yourself,': 477}\n",
            "<function <lambda> at 0x7fe894118310>\n",
            "<function <lambda> at 0x7fe893a24160>\n",
            "tensor([ 41, 402, 209,  ..., 448, 470,  18])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1346])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# block_size = 32\n",
        "# batch_size = 128\n",
        "# ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "# x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "# x.shape\n",
        "\n",
        "data.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lyu96OpK49q",
        "outputId": "adcd334d-4e98-4056-9cd3-4b5dd074da29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1346])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def gpt_params(seq_len, vocab_size, embedded_dim, num_heads, num_layers):\n",
        "#     \"\"\" Given GPT config calculate total number of parameters \"\"\"\n",
        "#     ffw_size = 4*embedded_dim # in GPT the number of intermediate features is always 4*embedded_dim\n",
        "#     # token and position embeddings\n",
        "#     embeddings = embedded_dim * vocab_size + embedded_dim * seq_len\n",
        "#     # transformer blocks\n",
        "#     attention = 3*embedded_dim**2 + 3*embedded_dim # weights and biases\n",
        "#     attproj = embedded_dim**2 + embedded_dim\n",
        "#     ffw = embedded_dim*(ffw_size) + ffw_size\n",
        "#     ffwproj = ffw_size*embedded_dim + embedded_dim\n",
        "#     layernorms = 2*2*embedded_dim\n",
        "#     # dense\n",
        "#     ln_f = 2*embedded_dim\n",
        "#     dense = embedded_dim*vocab_size # note: no bias here\n",
        "#     # note: embeddings are not included in the param count!\n",
        "#     total_params = num_layers*(attention + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
        "#     return total_params\n",
        "\n",
        "# gpt2 = dict(seq_len = 1346, vocab_size = 478, embedded_dim = 16, num_heads = 4, num_layers = 4)\n",
        "# gpt_params(**gpt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq9WH0cpJ2P8",
        "outputId": "421f85a9-d17f-4a5f-ef71-1026bac3c336"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20800"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_flops(seq_len, vocab_size, embedded_dim, num_heads, num_layers, ffw_size):\n",
        "  key_size = embedded_dim // num_heads\n",
        "  embeddings = 2 * seq_len * vocab_size * embedded_dim\n",
        "  # attention\n",
        "  \n",
        "  attention = 2 * 3 * seq_len * embedded_dim * (key_size * num_heads)\n",
        "  # key @ query logits\n",
        "  attlogits = 2 * seq_len * seq_len * (key_size * num_heads)\n",
        "  # softmax\n",
        "  attsoftmax = 3 * num_heads * seq_len * seq_len # 3* is for subtract (max), exp, divide (?)\n",
        "  # softmax @ value reductions\n",
        "  attvalue = 2 * seq_len * seq_len * (key_size * num_heads)\n",
        "  # final linear\n",
        "  attlinear = 2 * seq_len * (key_size * num_heads) * embedded_dim\n",
        "  att = attention + attlogits + attsoftmax + attvalue + attlinear\n",
        "  # feed forward\n",
        "  dense = 2 * seq_len * (embedded_dim * ffw_size + embedded_dim * ffw_size)\n",
        "\n",
        "  # logits\n",
        "  logits = 2 * seq_len * embedded_dim * vocab_size\n",
        "    \n",
        "  # this is what you'd expect:\n",
        "  # forward_flops = embeddings + num_layers * (att + dense) + logits\n",
        "  # but:\n",
        "  # per author correspondence apparently there is typo in the paper,\n",
        "  # they do not count embeddings and logits to repro table 4. So instead:\n",
        "  forward_flops = num_layers * (att + dense)\n",
        "  backward_flops = 2 * forward_flops # as in Kaplan et al. 2020\n",
        "  total_flops = forward_flops + backward_flops\n",
        "\n",
        "  return total_flops\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, rank):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads, rank) for _ in range(num_layers)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ry0RBUYB2v22"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size =  478\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "rank = 4\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads, rank)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKmIAmaL4F1Q",
        "outputId": "52cc12fc-90ed-4605-c2f2-adbfcd09692c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29246\n",
            "number of parameters: 29246\n",
            "number of parameters: 29246\n",
            "number of parameters: 29246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flops = count_flops(1346, 478, 16, 4, 4, 640)\n",
        "flops/1e9\n",
        "## 2.34 GFLOPs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB6DnDDyE3VA",
        "outputId": "99cfa060-3831-4f14-c1aa-85385b004e57"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.346950208"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}