{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2pwplp8fstL6smCfXLsxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/BabyGPT/blob/main/Notebook/lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads, rank):\n",
        "    super(Attention, self).__init__()\n",
        "    self.rank = rank\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.W_A = nn.Parameter(torch.empty(embedded_dim, rank))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    \n",
        "\n",
        "    \n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * ((1.0 / math.sqrt(k.size(-1)) * self.rank))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = att @ (self.W_A)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "_PF4pa1OUkIQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "srO-4qIOUlSd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads, rank):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim, num_heads, rank)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "mKR1Y7p2Us5F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, rank):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads, rank) for _ in range(num_layers)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "f3vDjpPuUtsu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size =  478\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "rank = 4\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads, rank)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWQ8_ioEmwkf",
        "outputId": "667cda11-92d9-4ac7-f471-ee8aa9dc8091"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29246\n",
            "number of parameters: 29246\n",
            "number of parameters: 29246\n",
            "number of parameters: 29246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A comparison between BabyGPT and Low rank adaptation. BabyGPT is b/w  28k-29k parametres. Low rank adaptation improves the parametre efficiency. 15k parametres. \n",
        "\n",
        "Note: Parametre size is also directly linked to context length."
      ],
      "metadata": {
        "id": "KPKToZKsqPHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 16\n",
        "\n",
        "W_A = nn.Parameter(torch.empty(input_dim, rank))\n",
        "W_A.shape"
      ],
      "metadata": {
        "id": "N3uQZD2AoTeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LowRankAttention(nn.Module):\n",
        "    def __init__(self, dim, rank):\n",
        "        super(LowRankAttention, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.Wq = nn.Linear(dim, rank, bias=False)\n",
        "        self.Wk = nn.Linear(dim, rank, bias=False)\n",
        "        self.Wv = nn.Linear(dim, rank, bias=False)\n",
        "        self.Wo = nn.Linear(rank, dim, bias=False)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        Q = self.Wq(q)\n",
        "        K = self.Wk(k)\n",
        "        V = self.Wv(v)\n",
        "\n",
        "        # Compute the attention scores using low-rank approximation\n",
        "        A = torch.bmm(Q, K.transpose(-2, -1)) / (self.rank ** 0.5)\n",
        "\n",
        "        # Softmax along the key dimension\n",
        "        A = torch.softmax(A, dim=-1)\n",
        "\n",
        "        # Compute the attention-weighted values using low-rank approximation\n",
        "        AV = torch.bmm(A, V)\n",
        "\n",
        "        # Apply the output layer to the attention-weighted values\n",
        "        out = self.Wo(AV)\n",
        "\n",
        "        return out\n",
        "\n",
        "class LowRankTransformerLayer(nn.Module):\n",
        "    def __init__(self, dim, rank, dropout=0.2):\n",
        "        super(LowRankTransformerLayer, self).__init__()\n",
        "        self.attention = LowRankAttention(dim, rank)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 3),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 3, dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the self-attention layer\n",
        "        attention_out = self.attention(x, x, x)\n",
        "\n",
        "        # Add residual connection and normalize\n",
        "        x = self.norm1(x + self.dropout1(attention_out))\n",
        "\n",
        "        # Feed-forward layer\n",
        "        ff_out = self.feedforward(x)\n",
        "\n",
        "        # Add residual connection and normalize\n",
        "        x = self.norm2(x + self.dropout2(ff_out))\n",
        "\n",
        "        return x\n",
        "\n",
        "class LowRankTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers, dim, rank, num_heads, dropout= 0.2):\n",
        "        super(LowRankTransformer, self).__init__()\n",
        "        self.layers = nn.ModuleList([LowRankTransformerLayer(dim, rank, dropout) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "        self.dim = dim\n",
        "        self.rank = rank\n",
        "        self.num_heads = num_heads\n",
        "        self.pos_embedding = nn.Embedding(vocab_size, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # init all weights\n",
        "        ## from karpathy\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "          if pn.endswith('Wo.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "            # report number of parameters\n",
        "            print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding[:, :x.size(1)]\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "qD35ejNYzi59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrt = LowRankTransformer(478, 4, 16, 4, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD91mbOizkTG",
        "outputId": "6f4556e1-b4a0-45b6-9155-2626007a67c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open(r\"/content/text.txt\", 'r', encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "vocab_size = len(chars)\n",
        "lrt = LowRankTransformer(vocab_size, 4, 16, 4, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47TM6blM71y3",
        "outputId": "6e5fc488-5052-42c7-b57f-0ef0381df968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n",
            "number of parameters: 15328\n"
          ]
        }
      ]
    }
  ]
}