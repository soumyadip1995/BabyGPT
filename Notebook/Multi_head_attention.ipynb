{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTYLgxQEgEaumfRqYYybZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/language-models/blob/main/Notebook/Multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-iyjhxM5BKS",
        "outputId": "771dae98-d10f-4583-9127-42e46ab144ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So',\n",
              " 'Ray',\n",
              " 'J',\n",
              " 'went',\n",
              " 'straight',\n",
              " 'to',\n",
              " 'the',\n",
              " 'radio',\n",
              " 'station',\n",
              " 'The',\n",
              " 'very',\n",
              " 'next',\n",
              " 'day,',\n",
              " '\"Hey',\n",
              " 'Fab,',\n",
              " \"I'ma\",\n",
              " 'kill',\n",
              " 'you!\"',\n",
              " 'Lyrics',\n",
              " \"comin'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "words[:20]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuriIPRQ5WA9",
        "outputId": "424977d6-0a73-48fe-cbe6-23300467a055"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "<function <lambda> at 0x7f4f7300da60>\n",
            "<function <lambda> at 0x7f4f7300dc10>\n",
            "tensor([ 22,  21,  18, 119, 103, 114, 110,  93, 102,  23, 116,  86,  44,   1,\n",
            "         10,  16,  72, 124,  19,  41,  36, 123,  36, 106, 101,   8,  11,  25,\n",
            "        104,  48, 123,  35,  15,  31,  66,  27,  14,  61,  47, 114,  58,  68,\n",
            "        113, 114, 123,  15, 105,  17,  32,  15,  77,  88,  97, 100, 108,  33,\n",
            "         29,  98,  67,  95,  89,  88,  80,  32,  69,  60, 114, 123,  32,  15,\n",
            "         46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,  53,  75,\n",
            "         70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,  12, 110,\n",
            "         43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,  14,  73,\n",
            "        110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,  50,  84,\n",
            "          4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,  71, 111,\n",
            "         42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,  64, 118,\n",
            "        114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,  30,  32,\n",
            "         79,   7,  76,  68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "block_size = 64\n",
        "batch_size = 512\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "ix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Y9VdIx5h5k",
        "outputId": "d53a2f02-01be-49d7-9cb3-fef17ad4998f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 30,  71,  72,  51,  71,  33,  61,  38,  89,  69,  48,  25,   5,  74,\n",
              "         68,  39,  19,  54,  63,  77,  48,  25,  37,  25,  56,  77,  31,   3,\n",
              "          6,  43,  47,   5,  85,   6,  27, 106,  76,  71,  31,  69,  97,  17,\n",
              "         52,  67, 105,  30,  80,  23,  57,  17,   6,  20,  81,  93,  13,  24,\n",
              "         19,  37,  28,  88,  36,  14,  89,  54,  82,  43,   2,  38, 102,   2,\n",
              "        102,  33,  76,  62,  55,  92,  49,  67,  21, 102,  72,  49,  20,  25,\n",
              "         16,  68,   3, 106,  54,  16,  30,  57,  47, 103,  37,  92,   5,  34,\n",
              "         62,  81,  97,  47,  68,  20,  92,  66,  10,  30,  22,  27,  48,   1,\n",
              "         19,  25,  84,  51,  75,  70,  70,  10,  97,  17,  15,  85,  70,  21,\n",
              "        101,  78,  93,  66,  82,   8,  49,  41,  11,  66, 105,  70,  96,  54,\n",
              "         46,  62,  27, 105,  26,  90,  67,  85, 100,  24,  48,  95, 107, 102,\n",
              "         52,  25,  53,  53,  94,  30,  20,  27,  81,  30,  38,  28,  12,  10,\n",
              "         24,  43,  44,  36,  68,  42,  92,  48,   1,  39,  39,  98,  98,   2,\n",
              "         79,  75,  70,  47,  16,  22,  85,  85,  64,  57,  20, 103,  98,  80,\n",
              "         41,  85,  37,  15,  31,  42,  53,  98,  57,  96,  27,  44, 100,  92,\n",
              "         82,  79,   9,   8,  24,  20,  33,  47,   7,  98,   9,  93,  97,  19,\n",
              "         13,  46,  65,  15,  18,  32,  58,   9, 104,  38,   6,  74,  38,  70,\n",
              "         50,  85,  17,   9,  64,  25,  54,  99,  84,  37, 106,  90,  91,  69,\n",
              "        105,  34,  68,   0,   5,  65,  26,  25,  85,  47,  65,  39,  85,  50,\n",
              "         43,  59, 103,  76,  71, 101,   3,  94,   8, 103,  85,  40,  48,  56,\n",
              "         82,  22,   0,  34, 101,  17,  31,  20,  36,  99,  77,  32,  14,   6,\n",
              "         77,  35,  70,  10,  86,  62,  48,  90,   6,  33,  72,   6,  76,   0,\n",
              "         66,  48,  67,  57,  63, 101,  39,  33,  10,  25,  28,  30,  75,  71,\n",
              "         59, 102,  64,  63,  73,  37, 105,  36,  88,  62,  58,  91,  69,  32,\n",
              "         75,  40,  68,   2,  58,  91,  53,  43,  79,  34,  30,  73,  66,  40,\n",
              "         72,  30,  35,  44,  88,  66, 106,   5,  51,  42,  33,  20,  73,  53,\n",
              "         40,  86,  62,  12,  69, 100,   4,  47,  71,  76,  69,  60,  51,  55,\n",
              "         93,  56,  99,  23,  16, 101,  80,  82,   1,  90,  85,  96,  32,  34,\n",
              "         82,  99,  32,  67,  78,  85,  36,  16,  85,  61,  51,  27,  41,  53,\n",
              "         40,  79,  83,  89,   8,  55,  86,  38,  59,  33,  49,  43,  90,  44,\n",
              "         63,  34,  39,  22,  27,  39,  84,  67,  30,  83,  25,  33,  98,  74,\n",
              "         80,  17,  27,  99,  40,  96,  59,   0,  43,  29,  34,  58, 104,  89,\n",
              "          8,  61,  85,  13,  92,  17,  11,  51,  93,  48,  36,   2,  80,  84,\n",
              "         63,  62,  46,  37,  74,  81,  83,  49,  32,  98,  99,   4,  24,   6,\n",
              "         67,  24,  51,  56,  20,  27,  45,  25,  16,  18,   9, 105,  95, 102,\n",
              "         95,   0,  40,  69,  93,  53,  87,  61,  98,  16,  21,  45,  31,  60,\n",
              "         39,  72,  32,  44,  88,  38,  84,  62])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "\n",
        "d_k = 2\n",
        "\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "token_emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSS2glS2Pbv9",
        "outputId": "0754502c-0ba9-4ad4-f805-d852d72b1e71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(125, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "x.shape\n",
        "# d = x.float()\n",
        "# d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9X-oqv75k4U",
        "outputId": "5d0b48cc-cd41-434e-84ec-59fe04e08873"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeds = token_emb(x)\n",
        "input_embeds.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx-6ks-EPs9E",
        "outputId": "295da84e-d9e7-4a77-a483-1d506e9c26c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# p = x.transpose(-2, 1)\n",
        "# p.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1y83mbKHM5c",
        "outputId": "3b1a157d-4941-4aad-e11f-e5c19c82a9f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "import torch\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "def scaled_dot_product(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)\n",
        "  \n"
      ],
      "metadata": {
        "id": "MHWCNqg0egmK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = input_embeds\n",
        "query  = input_embeds\n",
        "value = input_embeds\n",
        "\n",
        "sdp = scaled_dot_product(query, key, value)\n",
        "sdp.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXYuihMcVqQL",
        "outputId": "90127c1b-d450-42e3-f91c-eef8d790db02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# chars = sorted(list(set(words)))\n",
        "# # print(chars)\n",
        "# size = len(chars) # sequence length\n",
        "# # print(size)\n",
        "\n",
        "# torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "\n",
        "# key , value, query = input_embeds\n",
        "\n",
        "# # key = torch.randn(batch_size, size, dim_k)\n",
        "# # value = torch.randn(batch_size, size, dim_k) \n",
        "# # query = torch.randn(batch_size, size, dim_k)\n",
        "\n",
        "\n",
        "\n",
        "# sdp = scaled_dot_product_attention(query, key, value)\n",
        "# sdp.size()"
      ],
      "metadata": {
        "id": "D1zKRv0NiPph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## A single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "    "
      ],
      "metadata": {
        "id": "ba0pIBp_wn10"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "pI0uLzziwwIj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attention = MultiHeadAttention(512, 8)\n",
        "# multihead_attention\n",
        "\n",
        "output =  multihead_attention(input_embeds)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "GTfXMZx4xGdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        d_k = K.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k).float())\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        weights = nn.Softmax(dim=-1)(scores)\n",
        "        output = torch.matmul(weights, V)\n",
        "        return output\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \n",
        "        \n",
        "        # Apply linear transformation to Q, K, and V\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "        \n",
        "        # Split into multiple heads\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k)\n",
        "        \n",
        "        # Transpose to prepare for matrix multiplication\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Compute attention scores and weights\n",
        "        output = self.scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "        \n",
        "        # Concatenate the outputs of the multiple heads\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        output = self.W_o(output)\n",
        "        \n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "AVQqRkLydBfB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "\n",
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "seq_len = len(chars) # sequence length\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, d_model)\n",
        "K = torch.randn(batch_size, seq_len, d_model)\n",
        "V = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Create an instance of the multi-head attention model\n",
        "multihead_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# Pass the input tensors through the model\n",
        "output = multihead_attn(Q, K, V)\n",
        "\n",
        "# The output tensor has shape (batch_size, seq_len, d_model)\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CaB5Zw0a3Ej",
        "outputId": "2eebc6cc-fe7d-4103-b596-f0867b1af97b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 125, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    dim_k = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, dim_k) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "SSzxWEbpNJak"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attention = MultiHeadAttention(embedded_dim = 512, num_heads = 4)\n",
        "attention_output = multihead_attention(input_embeds)\n",
        "attention_output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "A7BcFZk2ToJp",
        "outputId": "fee385fe-1e62-4cb8-ad31-094a8736c83f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-74948ccd740d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmultihead_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultihead_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-07d64d448a19>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-07d64d448a19>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-15139e761195>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32768x2 and 512x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim, bias = False)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim, bias = False)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim, bias = False)\n",
        "\n",
        "  def forward(self, p):\n",
        "    attention_outputs = scaled_dot_product_attention(self.q(p), self.k(p), self.v(p))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "    "
      ],
      "metadata": {
        "id": "vxajBtrOglFC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, p):\n",
        "    out = torch.cat([h(p) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "9YHPWifthH2K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attention = MultiHeadAttention(embedded_dim = 512, num_heads = 8)\n",
        "multihead_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzkvfL57heSZ",
        "outputId": "406c8719-c4c1-4281-a701-0eeb5fa9ea05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (heads): ModuleList(\n",
              "    (0): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (1): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (2): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (3): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (4): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (5): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (6): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (7): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Single headed attention\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SingleAttention(nn.Module):\n",
        "    def __init__(self, embedded_dim):\n",
        "        super(SingleAttention, self).__init__()\n",
        "        self.embedded_dim = embedded_dim\n",
        "        \n",
        "        self.W_q = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.W_k = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.W_v = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.W_o = nn.Linear(embedded_dim, embedded_dim)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \n",
        "        \n",
        "        # Apply linear transformation to Q, K, and V\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "        \n",
        "        # Compute attention scores and weights\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.embedded_dim).float())\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        weights = nn.Softmax(dim=-1)(scores)\n",
        "        \n",
        "        # Apply attention weights to values\n",
        "        output = torch.matmul(weights, V)\n",
        "        output = self.W_o(output)\n",
        "        \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "-bfLrHoQnFnL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "seq_len = len(chars) # sequence length\n",
        "embedded_dim = 512\n",
        "\n",
        "\n",
        "# Create some dummy input tensors\n",
        "Q = torch.randn(batch_size, seq_len, embedded_dim)\n",
        "\n",
        "K = torch.randn(batch_size, seq_len, embedded_dim)\n",
        "V = torch.randn(batch_size, seq_len, embedded_dim)\n",
        "\n",
        "# Create an instance of the single attention model\n",
        "single_attn = SingleAttention(embedded_dim)\n",
        "\n",
        "# Pass the input tensors through the model\n",
        "output = single_attn(Q, K, V)\n",
        "output.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIxYtzE8nWhI",
        "outputId": "a5a94f49-51fc-4150-fdf7-f5b4a7db2bad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 125, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "554oVPXfwP_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}