{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3iL7zQWCmdW",
        "outputId": "f07ab82b-c5ad-4f38-e71c-e0a443a4a798"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So',\n",
              " 'Ray',\n",
              " 'J',\n",
              " 'went',\n",
              " 'straight',\n",
              " 'to',\n",
              " 'the',\n",
              " 'radio',\n",
              " 'station',\n",
              " 'The',\n",
              " 'very',\n",
              " 'next',\n",
              " 'day,',\n",
              " '\"Hey',\n",
              " 'Fab,',\n",
              " \"I'ma\",\n",
              " 'kill',\n",
              " 'you!\"',\n",
              " 'Lyrics',\n",
              " \"comin'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from karpathy\n",
        "# create a bigram by sliding across 2 words at a time\n",
        "data = {}\n",
        "for w in words:\n",
        "  chars = list(w)\n",
        "  for ch1, ch2 in zip(chars, chars[2:]):\n",
        "    bigram = (ch1, ch2)\n",
        "    data[bigram] = data.get(bigram, 0) + 1\n",
        "\n"
      ],
      "metadata": {
        "id": "K5iG857vD4RG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(data.items(), key = lambda kv: -kv[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyqmGYeKEguO",
        "outputId": "67ba7ef9-a3d8-4739-8a5c-e339e8f32e05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'e'), 11),\n",
              " (('a', 'i'), 9),\n",
              " (('i', 'g'), 9),\n",
              " (('t', 'a'), 7),\n",
              " (('i', \"'\"), 7),\n",
              " (('h', 't'), 7),\n",
              " (('a', 'e'), 7),\n",
              " (('t', 'n'), 7),\n",
              " (('y', 'u'), 6),\n",
              " (('u', 'e'), 6),\n",
              " (('a', 'd'), 6),\n",
              " (('e', 'e'), 6),\n",
              " (('g', 't'), 5),\n",
              " (('I', 'm'), 5),\n",
              " (('m', 'n'), 5),\n",
              " (('v', 't'), 5),\n",
              " (('e', 'l'), 5),\n",
              " (('h', 'y'), 5),\n",
              " (('v', 'r'), 4),\n",
              " (('u', 'i'), 4),\n",
              " (('n', 't'), 4),\n",
              " (('o', 'e'), 4),\n",
              " (('t', 's'), 4),\n",
              " (('s', 'r'), 3),\n",
              " (('i', 'h'), 3),\n",
              " (('t', 't'), 3),\n",
              " ((\"'\", 'a'), 3),\n",
              " (('r', 'c'), 3),\n",
              " (('o', 'i'), 3),\n",
              " (('e', 's'), 3),\n",
              " (('u', 'm'), 3),\n",
              " (('m', 'a'), 3),\n",
              " (('m', ','), 3),\n",
              " (('a', 's'), 3),\n",
              " (('t', 'r'), 3),\n",
              " (('r', 'u'), 3),\n",
              " (('i', 'e'), 3),\n",
              " (('h', 'n'), 3),\n",
              " (('e', 'i'), 3),\n",
              " (('o', 'f'), 3),\n",
              " (('e', 'a'), 3),\n",
              " (('l', 'v'), 3),\n",
              " (('m', 'k'), 3),\n",
              " (('e', 't'), 2),\n",
              " (('d', 'o'), 2),\n",
              " (('s', 'a'), 2),\n",
              " (('t', 'o'), 2),\n",
              " (('i', 'n'), 2),\n",
              " (('d', 'y'), 2),\n",
              " (('a', ','), 2),\n",
              " (('i', 's'), 2),\n",
              " (('s', 'p'), 2),\n",
              " (('p', 'r'), 2),\n",
              " (('s', 'm'), 2),\n",
              " (('m', '-'), 2),\n",
              " (('a', 'l'), 2),\n",
              " (('-', 'u'), 2),\n",
              " (('l', 'm'), 2),\n",
              " (('o', 'a'), 2),\n",
              " (('h', 'm'), 2),\n",
              " (('u', 'a'), 2),\n",
              " (('h', 'o'), 2),\n",
              " (('t', 'v'), 2),\n",
              " (('Y', 'u'), 2),\n",
              " (('s', 'y'), 2),\n",
              " (('o', 'h'), 2),\n",
              " ((\"'\", 'l'), 2),\n",
              " (('a', 't'), 2),\n",
              " (('n', ','), 2),\n",
              " (('o', 's'), 2),\n",
              " (('r', 't'), 2),\n",
              " (('m', 't'), 2),\n",
              " (('e', 'f'), 2),\n",
              " (('i', 'a'), 2),\n",
              " (('k', 'o'), 2),\n",
              " (('n', 'w'), 2),\n",
              " (('c', 'n'), 2),\n",
              " (('w', 'y'), 2),\n",
              " ((\"'\", 'm'), 2),\n",
              " (('m', 's'), 2),\n",
              " (('s', 'c'), 2),\n",
              " (('a', 'o'), 2),\n",
              " (('f', 's'), 2),\n",
              " (('h', 'p'), 2),\n",
              " (('o', ','), 2),\n",
              " (('o', 'k'), 2),\n",
              " (('R', 'y'), 1),\n",
              " (('w', 'n'), 1),\n",
              " (('r', 'i'), 1),\n",
              " (('a', 'g'), 1),\n",
              " (('r', 'd'), 1),\n",
              " (('T', 'e'), 1),\n",
              " (('e', 'y'), 1),\n",
              " (('n', 'x'), 1),\n",
              " (('\"', 'e'), 1),\n",
              " (('H', 'y'), 1),\n",
              " (('F', 'b'), 1),\n",
              " (('k', 'l'), 1),\n",
              " (('i', 'l'), 1),\n",
              " (('o', '!'), 1),\n",
              " (('u', '\"'), 1),\n",
              " (('L', 'r'), 1),\n",
              " (('y', 'i'), 1),\n",
              " (('c', 'm'), 1),\n",
              " (('r', 'o'), 1),\n",
              " (('s', 'n'), 1),\n",
              " (('n', 'c'), 1),\n",
              " (('s', 'e'), 1),\n",
              " (('p', 'e'), 1),\n",
              " (('e', 'd'), 1),\n",
              " (('(', '.'), 1),\n",
              " (('J', 'J'), 1),\n",
              " (('.', '.'), 1),\n",
              " (('F', 'd'), 1),\n",
              " (('a', ')'), 1),\n",
              " (('U', ','), 1),\n",
              " (('o', 'm'), 1),\n",
              " (('s', 'u'), 1),\n",
              " (('W', 'a'), 1),\n",
              " (('o', 't'), 1),\n",
              " (('o', 'g'), 1),\n",
              " (('u', 'h'), 1),\n",
              " (('e', 'h'), 1),\n",
              " (('a', '?'), 1),\n",
              " (('I', 'n'), 1),\n",
              " (('n', 'o'), 1),\n",
              " (('n', 'v'), 1),\n",
              " (('m', 'd'), 1),\n",
              " (('r', 'b'), 1),\n",
              " (('u', 'b'), 1),\n",
              " (('b', 'e'), 1),\n",
              " (('b', 'r'), 1),\n",
              " (('a', 'y'), 1),\n",
              " (('y', 'h'), 1),\n",
              " (('t', 'i'), 1),\n",
              " (('i', 'o'), 1),\n",
              " (('c', 'c'), 1),\n",
              " (('c', 'e'), 1),\n",
              " (('t', 'l'), 1),\n",
              " (('g', 'u'), 1),\n",
              " (('l', 'e'), 1),\n",
              " (('d', 'v'), 1),\n",
              " (('v', 's'), 1),\n",
              " (('m', 'r'), 1),\n",
              " (('d', 'm'), 1),\n",
              " (('e', 'o'), 1),\n",
              " (('H', 'w'), 1),\n",
              " (('g', 'v'), 1),\n",
              " (('h', 'r'), 1),\n",
              " (('f', 'c'), 1),\n",
              " (('u', 'k'), 1),\n",
              " (('c', 'i'), 1),\n",
              " (('k', 'n'), 1),\n",
              " (('d', 'e'), 1),\n",
              " (('e', 'c'), 1),\n",
              " (('n', 'e'), 1),\n",
              " (('f', 'e'), 1),\n",
              " (('l', 'n'), 1),\n",
              " (('l', 'k'), 1),\n",
              " (('N', 'v'), 1),\n",
              " (('f', 'd'), 1),\n",
              " (('d', 'n'), 1),\n",
              " (('f', 'r'), 1),\n",
              " (('r', 'v'), 1),\n",
              " (('w', 'i'), 1),\n",
              " (('i', 'i'), 1),\n",
              " (('F', 'r'), 1),\n",
              " (('f', 'l'), 1),\n",
              " (('f', ','), 1),\n",
              " (('e', \"'\"), 1),\n",
              " (('y', 'l'), 1),\n",
              " (('c', 'l'), 1),\n",
              " (('l', 'b'), 1),\n",
              " (('e', 'r'), 1),\n",
              " (('b', 'a'), 1),\n",
              " (('C', 'u'), 1),\n",
              " (('t', 'd'), 1),\n",
              " (('i', ','), 1),\n",
              " (('\"', 'h'), 1),\n",
              " (('O', ','), 1),\n",
              " (('h', \"'\"), 1),\n",
              " (('m', 'i'), 1),\n",
              " (('a', 'n'), 1),\n",
              " (('r', 'a'), 1),\n",
              " (('e', 'm'), 1),\n",
              " (('a', '\"'), 1),\n",
              " (('W', 'l'), 1),\n",
              " (('l', ','), 1),\n",
              " (('a', \"'\"), 1),\n",
              " (('w', 'a'), 1),\n",
              " (('w', 'e'), 1),\n",
              " (('j', 'a'), 1),\n",
              " (('l', 'u'), 1),\n",
              " (('u', ','), 1),\n",
              " (('n', 'u'), 1),\n",
              " (('\"', 't'), 1),\n",
              " (('I', \"'\"), 1),\n",
              " (('i', '-'), 1),\n",
              " (('p', 'h'), 1),\n",
              " (('-', 'o'), 1),\n",
              " (('p', 'p'), 1),\n",
              " (('c', 'u'), 1),\n",
              " (('f', 'u'), 1),\n",
              " (('o', 'n'), 1),\n",
              " (('u', 'd'), 1),\n",
              " (('h', 'l'), 1),\n",
              " (('l', 'a'), 1),\n",
              " (('W', 't'), 1),\n",
              " (('c', ','), 1),\n",
              " (('s', 'o'), 1),\n",
              " (('h', 'c'), 1),\n",
              " (('r', 'p'), 1),\n",
              " (('w', 't'), 1),\n",
              " (('D', 'c'), 1),\n",
              " (('T', 'r'), 1),\n",
              " (('r', 'w'), 1),\n",
              " (('\"', 'o'), 1),\n",
              " (('L', 's'), 1),\n",
              " (('o', 'r'), 1),\n",
              " (('u', 's'), 1),\n",
              " (('r', 'e'), 1),\n",
              " (('s', 'l'), 1),\n",
              " (('l', '\"'), 1),\n",
              " (('l', 's'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char = sorted(list(set(words)))"
      ],
      "metadata": {
        "id": "sWqoBOBxH0hl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the training data\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEcBOKFlHKLR",
        "outputId": "01a3c577-5c30-489a-caa4-bf404b0073c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "<function <lambda> at 0x7f9c097c53a0>\n",
            "<function <lambda> at 0x7f9c097c5d30>\n",
            "tensor([ 22,  21,  18, 119, 103, 114, 110,  93, 102,  23, 116,  86,  44,   1,\n",
            "         10,  16,  72, 124,  19,  41,  36, 123,  36, 106, 101,   8,  11,  25,\n",
            "        104,  48, 123,  35,  15,  31,  66,  27,  14,  61,  47, 114,  58,  68,\n",
            "        113, 114, 123,  15, 105,  17,  32,  15,  77,  88,  97, 100, 108,  33,\n",
            "         29,  98,  67,  95,  89,  88,  80,  32,  69,  60, 114, 123,  32,  15,\n",
            "         46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,  53,  75,\n",
            "         70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,  12, 110,\n",
            "         43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,  14,  73,\n",
            "        110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,  50,  84,\n",
            "          4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,  71, 111,\n",
            "         42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,  64, 118,\n",
            "        114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,  30,  32,\n",
            "         79,   7,  76,  68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "n = int(0.4 * len(words))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(val_data)\n",
        "\n",
        "p = decode(val_data[:18].tolist())\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "WMPkvDxvMP3N",
        "outputId": "d38956e2-ef47-401c-d497-db562170b9f3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 32,  15,  46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,\n",
            "         53,  75,  70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,\n",
            "         12, 110,  43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,\n",
            "         14,  73, 110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,\n",
            "         50,  84,   4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,\n",
            "         71, 111,  42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,\n",
            "         64, 118, 114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,\n",
            "         30,  32,  79,   7,  76,  68])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"andI'mdevastating,morethaneverdemonstratingHowtogiveamotherfuckin'audienceafeelinglikeit'slevitating\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string2integer = {ch: i for i, ch in enumerate(char)}\n",
        "print(string2integer)\n",
        "\n",
        "res = dict(enumerate(words))\n",
        "print(res)\n",
        "keyList = [key for key in res]\n",
        "print(keyList)\n",
        "l = len(keyList)\n",
        "l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nS-JjGF9o9x",
        "outputId": "f0215c38-ee98-4ff2-e7a9-cd6e100c1b6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "{0: 'So', 1: 'Ray', 2: 'J', 3: 'went', 4: 'straight', 5: 'to', 6: 'the', 7: 'radio', 8: 'station', 9: 'The', 10: 'very', 11: 'next', 12: 'day,', 13: '\"Hey', 14: 'Fab,', 15: \"I'ma\", 16: 'kill', 17: 'you!\"', 18: 'Lyrics', 19: \"comin'\", 20: 'at', 21: 'you', 22: 'at', 23: 'supersonic', 24: 'speed', 25: '(J.J.', 26: 'Fad)', 27: 'Uh,', 28: 'summa-lumma,', 29: 'dooma-lumma,', 30: 'you', 31: \"assumin'\", 32: \"I'm\", 33: 'a', 34: 'human', 35: 'What', 36: 'I', 37: 'gotta', 38: 'do', 39: 'to', 40: 'get', 41: 'it', 42: 'through', 43: 'to', 44: 'you', 45: \"I'm\", 46: 'superhuman?', 47: 'Innovative', 48: 'and', 49: \"I'm\", 50: 'made', 51: 'of', 52: 'rubber', 53: 'so', 54: 'that', 55: 'anything', 56: 'You', 57: 'say', 58: 'is', 59: \"ricochetin'\", 60: 'off', 61: 'of', 62: 'me,', 63: 'and', 64: \"it'll\", 65: 'glue', 66: 'to', 67: 'you', 68: 'and', 69: \"I'm\", 70: 'devastating,', 71: 'more', 72: 'than', 73: 'ever', 74: 'demonstrating', 75: 'How', 76: 'to', 77: 'give', 78: 'a', 79: \"motherfuckin'\", 80: 'audience', 81: 'a', 82: 'feeling', 83: 'like', 84: \"it's\", 85: 'levitating', 86: 'Never', 87: 'fading,', 88: 'and', 89: 'I', 90: 'know', 91: 'the', 92: 'haters', 93: 'are', 94: 'forever', 95: 'waiting', 96: 'For', 97: 'the', 98: 'day', 99: 'that', 100: 'they', 101: 'can', 102: 'say', 103: 'I', 104: 'fell', 105: 'off,', 106: \"they'll\", 107: 'be', 108: 'celebrating', 109: \"'Cause\", 110: 'I', 111: 'know', 112: 'the', 113: 'way', 114: 'to', 115: 'get', 116: \"'em\", 117: 'motivated', 118: 'I', 119: 'make', 120: 'elevating', 121: 'music,', 122: 'you', 123: 'make', 124: 'elevator', 125: 'music', 126: '\"Oh,', 127: \"he's\", 128: 'too', 129: 'mainstream\"', 130: 'Well,', 131: \"that's\", 132: 'what', 133: 'they', 134: 'do', 135: 'when', 136: 'they', 137: 'get', 138: 'jealous,', 139: 'they', 140: 'confuse', 141: 'it', 142: '\"It\\'s', 143: 'not', 144: 'hip-hop,', 145: \"it's\", 146: 'pop,', 147: '\"', 148: \"'cause\", 149: 'I', 150: 'found', 151: 'a', 152: 'hella', 153: 'way', 154: 'to', 155: 'fuse', 156: 'it', 157: 'With', 158: 'rock,', 159: 'shock', 160: 'rap', 161: 'with', 162: 'Doc', 163: 'Throw', 164: 'on', 165: '\"Lose', 166: 'Yourself\"', 167: 'and', 168: 'make', 169: \"'em\", 170: 'lose', 171: 'it'}\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled dot product attention\n",
        "# 3 matrices, Q -> query matrix\n",
        "# K -> Key matrix\n",
        "# V -> value matrix\n",
        "# T is the length of the sequence\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "import torch\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ],
      "metadata": {
        "id": "aFiuKJk29oTJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dk and Dv\n",
        " are the hidden dimensionality for queries/keys and values respectively"
      ],
      "metadata": {
        "id": "py-vXvCwGjry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "size = len(chars) # sequence length\n",
        "# print(size)\n",
        "\n",
        "\n",
        "d_k = 2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "q = torch.randn(size, d_k)\n",
        "\n",
        "#q = torch.randn(size, d_k).detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "k = torch.randn(size, d_k)\n",
        "v = torch.randn(size, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "# print(\"Q\\n\", q)\n",
        "# print(\"K\\n\", k)\n",
        "# print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values[:5].reshape(1, -1))\n",
        "print(\"Attention\\n\", attention[:5].reshape(1, -1))\n",
        "# p = torch.Tensor(values)\n",
        "# p\n",
        "\n",
        "values.tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKxcaZJ1FWri",
        "outputId": "306d02bb-df4d-4b9d-f0a1-b2f719aeb309"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values\n",
            " tensor([[-0.0024,  0.0350, -0.0502,  0.0396,  0.0462,  0.0850,  0.0854,  0.1279,\n",
            "          0.0942,  0.0176]])\n",
            "Attention\n",
            " tensor([[0.0082, 0.0072, 0.0058, 0.0073, 0.0079, 0.0100, 0.0100, 0.0078, 0.0066,\n",
            "         0.0067, 0.0076, 0.0076, 0.0064, 0.0107, 0.0067, 0.0085, 0.0071, 0.0081,\n",
            "         0.0073, 0.0090, 0.0070, 0.0065, 0.0071, 0.0062, 0.0083, 0.0093, 0.0084,\n",
            "         0.0106, 0.0094, 0.0092, 0.0097, 0.0066, 0.0079, 0.0071, 0.0103, 0.0086,\n",
            "         0.0114, 0.0071, 0.0067, 0.0075, 0.0074, 0.0101, 0.0089, 0.0077, 0.0071,\n",
            "         0.0078, 0.0079, 0.0074, 0.0084, 0.0079, 0.0067, 0.0083, 0.0083, 0.0068,\n",
            "         0.0084, 0.0078, 0.0060, 0.0077, 0.0079, 0.0081, 0.0072, 0.0071, 0.0091,\n",
            "         0.0077, 0.0090, 0.0085, 0.0091, 0.0080, 0.0077, 0.0074, 0.0088, 0.0093,\n",
            "         0.0098, 0.0063, 0.0087, 0.0078, 0.0071, 0.0080, 0.0076, 0.0093, 0.0088,\n",
            "         0.0073, 0.0080, 0.0063, 0.0082, 0.0078, 0.0072, 0.0103, 0.0080, 0.0086,\n",
            "         0.0073, 0.0081, 0.0085, 0.0071, 0.0075, 0.0063, 0.0082, 0.0066, 0.0073,\n",
            "         0.0088, 0.0082, 0.0085, 0.0069, 0.0080, 0.0072, 0.0072, 0.0082, 0.0087,\n",
            "         0.0091, 0.0068, 0.0091, 0.0095, 0.0080, 0.0081, 0.0097, 0.0071, 0.0092,\n",
            "         0.0108, 0.0065, 0.0077, 0.0061, 0.0077, 0.0081, 0.0072, 0.0083, 0.0006,\n",
            "         0.0022, 0.0031, 0.0037, 0.0079, 0.0037, 0.0046, 0.0194, 0.0042, 0.0142,\n",
            "         0.0233, 0.0032, 0.0034, 0.0057, 0.0051, 0.0103, 0.0059, 0.0082, 0.0032,\n",
            "         0.0163, 0.0052, 0.0029, 0.0105, 0.0038, 0.0032, 0.0024, 0.0037, 0.0040,\n",
            "         0.0045, 0.0044, 0.0021, 0.0088, 0.0168, 0.0117, 0.0066, 0.0262, 0.0109,\n",
            "         0.0033, 0.0033, 0.0112, 0.0046, 0.0169, 0.0054, 0.0024, 0.0020, 0.0077,\n",
            "         0.0186, 0.0031, 0.0101, 0.0056, 0.0092, 0.0229, 0.0073, 0.0061, 0.0036,\n",
            "         0.0041, 0.0461, 0.0045, 0.0030, 0.0061, 0.0060, 0.0080, 0.0034, 0.0108,\n",
            "         0.0140, 0.0055, 0.0124, 0.0057, 0.0085, 0.0017, 0.0203, 0.0078, 0.0082,\n",
            "         0.0043, 0.0081, 0.0027, 0.0058, 0.0060, 0.0035, 0.0143, 0.0022, 0.0013,\n",
            "         0.0042, 0.0039, 0.0093, 0.0092, 0.0133, 0.0033, 0.0148, 0.0057, 0.0032,\n",
            "         0.0073, 0.0051, 0.0062, 0.0057, 0.0069, 0.0052, 0.0079, 0.0071, 0.0160,\n",
            "         0.0160, 0.0126, 0.0064, 0.0064, 0.0124, 0.0047, 0.0092, 0.0064, 0.0116,\n",
            "         0.0023, 0.0069, 0.0069, 0.0016, 0.0043, 0.0077, 0.0056, 0.0079, 0.0055,\n",
            "         0.0100, 0.0073, 0.0054, 0.0285, 0.0106, 0.0095, 0.0162, 0.0147, 0.0073,\n",
            "         0.0035, 0.0066, 0.0069, 0.0166, 0.0159, 0.0052, 0.0047, 0.0037, 0.0046,\n",
            "         0.0075, 0.0045, 0.0181, 0.0048, 0.0080, 0.0053, 0.0072, 0.0069, 0.0083,\n",
            "         0.0054, 0.0049, 0.0046, 0.0041, 0.0100, 0.0148, 0.0099, 0.0193, 0.0131,\n",
            "         0.0124, 0.0174, 0.0038, 0.0056, 0.0045, 0.0158, 0.0064, 0.0186, 0.0061,\n",
            "         0.0053, 0.0054, 0.0065, 0.0115, 0.0107, 0.0085, 0.0072, 0.0067, 0.0054,\n",
            "         0.0072, 0.0077, 0.0074, 0.0041, 0.0060, 0.0079, 0.0047, 0.0099, 0.0077,\n",
            "         0.0020, 0.0074, 0.0088, 0.0077, 0.0054, 0.0049, 0.0128, 0.0059, 0.0086,\n",
            "         0.0092, 0.0091, 0.0076, 0.0063, 0.0083, 0.0074, 0.0111, 0.0127, 0.0041,\n",
            "         0.0091, 0.0086, 0.0054, 0.0076, 0.0074, 0.0096, 0.0130, 0.0087, 0.0083,\n",
            "         0.0041, 0.0074, 0.0064, 0.0045, 0.0185, 0.0060, 0.0094, 0.0068, 0.0076,\n",
            "         0.0094, 0.0053, 0.0065, 0.0036, 0.0085, 0.0041, 0.0056, 0.0078, 0.0063,\n",
            "         0.0076, 0.0047, 0.0074, 0.0046, 0.0058, 0.0074, 0.0097, 0.0093, 0.0061,\n",
            "         0.0106, 0.0123, 0.0106, 0.0085, 0.0127, 0.0055, 0.0107, 0.0186, 0.0036,\n",
            "         0.0065, 0.0035, 0.0045, 0.0068, 0.0050, 0.0066, 0.0198, 0.0066, 0.0022,\n",
            "         0.0056, 0.0059, 0.0226, 0.0209, 0.0037, 0.0033, 0.0022, 0.0031, 0.0069,\n",
            "         0.0032, 0.0255, 0.0034, 0.0073, 0.0040, 0.0062, 0.0060, 0.0077, 0.0041,\n",
            "         0.0036, 0.0031, 0.0027, 0.0105, 0.0192, 0.0103, 0.0284, 0.0156, 0.0145,\n",
            "         0.0246, 0.0024, 0.0042, 0.0030, 0.0207, 0.0051, 0.0260, 0.0051, 0.0040,\n",
            "         0.0041, 0.0054, 0.0125, 0.0115, 0.0084, 0.0065, 0.0056, 0.0040, 0.0065,\n",
            "         0.0069, 0.0066, 0.0026, 0.0046, 0.0073, 0.0034, 0.0103, 0.0071, 0.0008,\n",
            "         0.0066, 0.0088, 0.0070, 0.0041, 0.0035, 0.0152, 0.0046, 0.0080, 0.0092,\n",
            "         0.0088, 0.0069, 0.0051, 0.0081, 0.0064, 0.0120, 0.0148, 0.0027, 0.0090,\n",
            "         0.0084, 0.0041, 0.0069, 0.0067, 0.0094, 0.0158, 0.0087, 0.0079, 0.0027,\n",
            "         0.0065, 0.0052, 0.0030, 0.0267, 0.0046, 0.0095, 0.0059, 0.0068, 0.0095,\n",
            "         0.0040, 0.0054, 0.0022, 0.0082, 0.0027, 0.0043, 0.0069, 0.0051, 0.0067,\n",
            "         0.0033, 0.0066, 0.0032, 0.0046, 0.0065, 0.0099, 0.0091, 0.0050, 0.0112,\n",
            "         0.0141, 0.0118, 0.0082, 0.0148, 0.0042, 0.0115, 0.0266, 0.0022, 0.0054,\n",
            "         0.0021, 0.0030, 0.0058, 0.0036, 0.0054, 0.1001, 0.0209, 0.0148, 0.0107,\n",
            "         0.0040, 0.0091, 0.0071, 0.0013, 0.0097, 0.0021, 0.0011, 0.0128, 0.0126,\n",
            "         0.0053, 0.0075, 0.0028, 0.0061, 0.0038, 0.0128, 0.0016, 0.0071, 0.0152,\n",
            "         0.0030, 0.0111, 0.0119, 0.0166, 0.0102, 0.0081, 0.0074, 0.0078, 0.0186,\n",
            "         0.0038, 0.0016, 0.0026, 0.0045, 0.0009, 0.0023, 0.0126, 0.0127, 0.0027,\n",
            "         0.0081, 0.0014, 0.0061, 0.0177, 0.0237, 0.0042, 0.0014, 0.0132, 0.0029,\n",
            "         0.0062, 0.0036, 0.0011, 0.0044, 0.0059, 0.0103, 0.0093, 0.0005, 0.0083,\n",
            "         0.0135, 0.0055, 0.0059, 0.0042, 0.0108, 0.0028, 0.0019, 0.0062, 0.0022,\n",
            "         0.0060, 0.0037, 0.0290, 0.0012, 0.0038, 0.0035, 0.0096, 0.0037, 0.0157,\n",
            "         0.0062, 0.0056, 0.0114, 0.0018, 0.0183, 0.0407, 0.0089, 0.0107, 0.0032,\n",
            "         0.0034, 0.0022, 0.0107, 0.0019, 0.0058, 0.0127, 0.0044, 0.0067, 0.0057,\n",
            "         0.0061, 0.0053, 0.0066, 0.0043, 0.0048, 0.0016, 0.0017, 0.0022, 0.0056,\n",
            "         0.0052, 0.0024, 0.0081, 0.0033, 0.0050, 0.0023, 0.0202, 0.0045, 0.0043,\n",
            "         0.0302, 0.0086, 0.0037, 0.0065, 0.0037, 0.0054, 0.0033, 0.0045, 0.0074,\n",
            "         0.0008, 0.0028, 0.0034, 0.0016]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.0024424297735095024, 0.03498305007815361],\n",
              " [-0.0501624196767807, 0.03959700092673302],\n",
              " [0.04617688059806824, 0.08497251570224762],\n",
              " [0.08537837117910385, 0.12790809571743011],\n",
              " [0.09422030299901962, 0.017580706626176834],\n",
              " [-0.1264338195323944, 0.023994464427232742],\n",
              " [0.0025613093748688698, 0.04516937583684921],\n",
              " [-0.06211200729012489, -0.14241114258766174],\n",
              " [0.11488746851682663, 0.1844596415758133],\n",
              " [0.05125867575407028, 0.019085992127656937],\n",
              " [-0.08051830530166626, -0.12789437174797058],\n",
              " [0.20601417124271393, 0.18496569991111755],\n",
              " [0.01212971843779087, 0.07835806906223297],\n",
              " [-0.10039400309324265, -0.010585237294435501],\n",
              " [0.02608320116996765, 0.07921285182237625],\n",
              " [1.2068867683410645, -0.21861863136291504],\n",
              " [-0.06788432598114014, -0.01689816452562809],\n",
              " [0.09440574795007706, 0.13482502102851868],\n",
              " [0.04743099957704544, -0.001893987413495779],\n",
              " [-0.09803761541843414, -0.015557566657662392],\n",
              " [-0.06463038921356201, 0.03987928479909897],\n",
              " [0.044606633484363556, 0.10082019120454788],\n",
              " [-0.06786196678876877, -0.11546669155359268],\n",
              " [-0.07085177302360535, 0.02141532674431801],\n",
              " [0.08408582955598831, 0.23249703645706177],\n",
              " [0.10790508985519409, 0.1856371909379959],\n",
              " [-0.07390289753675461, -0.24841341376304626],\n",
              " [-0.204962357878685, 0.4506817162036896],\n",
              " [0.0425923652946949, 0.21631477773189545],\n",
              " [0.06323853880167007, 0.11288471519947052],\n",
              " [0.32137084007263184, 0.0138127775862813],\n",
              " [0.11352662742137909, 0.22290918231010437],\n",
              " [-0.06172105297446251, -0.08920097351074219],\n",
              " [-0.023547453805804253, 0.012024279683828354],\n",
              " [-0.049802232533693314, -0.05151800066232681],\n",
              " [-0.0387587696313858, 0.05011781305074692],\n",
              " [0.1044725701212883, 0.2801820635795593],\n",
              " [0.020276693627238274, 0.02053844928741455],\n",
              " [-0.06469479203224182, -0.16202229261398315],\n",
              " [-0.0033133672550320625, 0.02325192466378212],\n",
              " [-0.040326669812202454, 0.37206676602363586],\n",
              " [0.1288796067237854, 0.12441892921924591],\n",
              " [-0.22545887529850006, 0.17358411848545074],\n",
              " [0.049806147813797, 0.09604141861200333],\n",
              " [0.013444921933114529, 0.05880897864699364],\n",
              " [0.028337912634015083, 0.08114787936210632],\n",
              " [-0.05397948622703552, 0.00599332433193922],\n",
              " [-0.09354490786790848, -0.00452154316008091],\n",
              " [-0.247136652469635, 0.19789570569992065],\n",
              " [-0.014638174325227737, -0.032824479043483734],\n",
              " [-0.014189068228006363, 0.04506535828113556],\n",
              " [-0.04191680625081062, -0.012674781493842602],\n",
              " [-0.03402138501405716, 0.004024013876914978],\n",
              " [0.12008803337812424, -0.0020702348556369543],\n",
              " [0.48477116227149963, -0.12070552259683609],\n",
              " [-0.07899396121501923, 0.12319973111152649],\n",
              " [-0.04412655532360077, -0.014746597036719322],\n",
              " [0.060042545199394226, 0.14346745610237122],\n",
              " [0.003791394643485546, 0.020065966993570328],\n",
              " [-0.07045283913612366, -0.14242567121982574],\n",
              " [0.18573209643363953, 0.2339632213115692],\n",
              " [0.011732539162039757, 0.0632440596818924],\n",
              " [-0.05842911824584007, -0.05754157900810242],\n",
              " [-0.06896857172250748, 0.08207385241985321],\n",
              " [-0.050417233258485794, -0.3103606402873993],\n",
              " [0.09675605595111847, 0.17993196845054626],\n",
              " [0.615382969379425, -0.0033014966174960136],\n",
              " [-0.011275796219706535, 0.07992802560329437],\n",
              " [-0.02069387212395668, 0.009522494859993458],\n",
              " [0.15226347744464874, 0.12404480576515198],\n",
              " [0.012251729145646095, 0.06535175442695618],\n",
              " [0.12833236157894135, 0.07487228512763977],\n",
              " [0.4187730848789215, 0.23370786011219025],\n",
              " [-0.1234004944562912, -0.0005882065743207932],\n",
              " [-0.029366828501224518, 0.01695026084780693],\n",
              " [-0.0506465770304203, -0.09251296520233154],\n",
              " [-0.07232385128736496, -0.06657937169075012],\n",
              " [0.10032971203327179, 0.16767138242721558],\n",
              " [-0.10917498916387558, 0.027502689510583878],\n",
              " [0.09207552671432495, 0.22307342290878296],\n",
              " [-0.07177484780550003, -0.052996501326560974],\n",
              " [0.29386579990386963, -0.09685372561216354],\n",
              " [-0.11338923126459122, -0.09776075929403305],\n",
              " [-0.07646654546260834, -0.021449344232678413],\n",
              " [0.2097140997648239, -0.0699368417263031],\n",
              " [-0.03277796879410744, 0.009404689073562622],\n",
              " [-0.09490090608596802, -0.04519902169704437],\n",
              " [-0.2841004729270935, 0.22845274209976196],\n",
              " [-0.04705136641860008, 0.055574171245098114],\n",
              " [-0.004122947342693806, 0.024830251932144165],\n",
              " [0.13418430089950562, 0.07413949072360992],\n",
              " [-0.05963941290974617, -0.22598084807395935],\n",
              " [0.21346107125282288, 0.08454982936382294],\n",
              " [0.04626943916082382, 0.10521285235881805],\n",
              " [0.007309351582080126, -0.12868118286132812],\n",
              " [0.1402263045310974, 0.02868991531431675],\n",
              " [-0.0016942033544182777, 0.04364467412233353],\n",
              " [0.569010853767395, 0.0898367241024971],\n",
              " [0.07748383283615112, 0.150150328874588],\n",
              " [-0.0468885637819767, -0.05225071683526039],\n",
              " [0.0040985047817230225, 0.07913917303085327],\n",
              " [0.14085090160369873, 0.13989678025245667],\n",
              " [-0.14683037996292114, 0.08329899609088898],\n",
              " [-0.025278249755501747, 0.002229011617600918],\n",
              " [0.0112387053668499, 0.06409496068954468],\n",
              " [-0.045776739716529846, 0.01202912162989378],\n",
              " [0.07596416771411896, 0.07522649317979813],\n",
              " [0.19462814927101135, 0.14411456882953644],\n",
              " [0.07975327223539352, 0.048079714179039],\n",
              " [-0.04268196225166321, 0.030908886343240738],\n",
              " [-0.054754436016082764, -0.054453350603580475],\n",
              " [-0.06346316635608673, -0.0017432011663913727],\n",
              " [-0.062472742050886154, -0.016133101657032967],\n",
              " [-0.018293533474206924, 0.026685478165745735],\n",
              " [-0.005379578098654747, -0.012982969172298908],\n",
              " [-0.019156137481331825, 0.03110877051949501],\n",
              " [0.057240553200244904, 0.12793909013271332],\n",
              " [-0.06263590604066849, -0.09591933339834213],\n",
              " [0.01872917450964451, 0.0662773847579956],\n",
              " [-0.0931088849902153, 0.04280717670917511],\n",
              " [0.03052111528813839, 0.0861184149980545],\n",
              " [-0.145289808511734, 0.10917168110609055],\n",
              " [-0.02502337656915188, -0.09166721999645233],\n",
              " [-0.12027861177921295, 0.000664866529405117],\n",
              " [-0.098392054438591, -0.07573743164539337]]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExIVbPmpMFTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b = sorted(list(set(values[:n])))\n",
        "\n",
        "# d  = torch.stack(b)\n",
        "# d\n",
        "# g = sorted(list(set(attention[:5])))\n"
      ],
      "metadata": {
        "id": "LuFscL68but-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Tensor ops to be continued"
      ],
      "metadata": {
        "id": "dR9-u68lEr07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.zeros((40, 40), dtype=torch.int32)\n"
      ],
      "metadata": {
        "id": "EtKi2zNbrpTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demystifying queries Keys and Values"
      ],
      "metadata": {
        "id": "2jl3BYTNJE1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "words[:20]"
      ],
      "metadata": {
        "id": "Tv1vYyDjKCXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbpZFK24I_J5",
        "outputId": "7a5fe353-efae-43c0-90ef-8c94562981cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "<function <lambda> at 0x7f9ca5cc4d30>\n",
            "<function <lambda> at 0x7f9ca5cc4af0>\n",
            "tensor([ 22,  21,  18, 119, 103, 114, 110,  93, 102,  23, 116,  86,  44,   1,\n",
            "         10,  16,  72, 124,  19,  41,  36, 123,  36, 106, 101,   8,  11,  25,\n",
            "        104,  48, 123,  35,  15,  31,  66,  27,  14,  61,  47, 114,  58,  68,\n",
            "        113, 114, 123,  15, 105,  17,  32,  15,  77,  88,  97, 100, 108,  33,\n",
            "         29,  98,  67,  95,  89,  88,  80,  32,  69,  60, 114, 123,  32,  15,\n",
            "         46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,  53,  75,\n",
            "         70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,  12, 110,\n",
            "         43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,  14,  73,\n",
            "        110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,  50,  84,\n",
            "          4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,  71, 111,\n",
            "         42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,  64, 118,\n",
            "        114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,  30,  32,\n",
            "         79,   7,  76,  68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "\n",
        "d_k = 2\n",
        "\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "token_emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uftVqQICKX8Q",
        "outputId": "63c33c1a-f125-4cd7-b7f0-78bf4c23e182"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(125, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_emb(data)\n",
        "input_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUs90cgBNb_w",
        "outputId": "8095966c-db26-44c2-8d96-10dad7a213d1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([172, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now, create the query, key and value vectors and calculate the attention scores based on the dot product as the similarity function"
      ],
      "metadata": {
        "id": "DwARBpr-N4sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import sqrt \n",
        "\n",
        "query = key = value = input_embeddings\n",
        "dim_k = key.size(-1)\n",
        "print(dim_k)\n",
        "scores = torch.matmul(query, key.transpose(1,-2))/sqrt(dim_k)\n",
        "print(scores)\n",
        "scores.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTgZhEpjNpnH",
        "outputId": "1eb7e1b7-c5c9-4b60-cbe0-9d2b714c31e4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensor([[ 1.0342,  0.2054, -1.5626,  ...,  0.1081,  0.5004,  1.1613],\n",
            "        [ 0.2054,  0.7806,  0.4476,  ...,  0.7237, -0.3282,  0.4966],\n",
            "        [-1.5626,  0.4476,  3.1375,  ...,  0.5561, -1.1942, -1.4822],\n",
            "        ...,\n",
            "        [ 0.1081,  0.7237,  0.5561,  ...,  0.6778, -0.3536,  0.3737],\n",
            "        [ 0.5004, -0.3282, -1.1942,  ..., -0.3536,  0.4893,  0.4082],\n",
            "        [ 1.1613,  0.4966, -1.4822,  ...,  0.3737,  0.4082,  1.3996]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([172, 172])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Now, lets apply the softmax function"
      ],
      "metadata": {
        "id": "h0Z5L7FGQp4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "weights = F.softmax(scores, dim = -1)\n",
        "weights.sum(dim = -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MwGAFO9Qxg6",
        "outputId": "b6155f76-01ac-4e7c-9373-7ad7e70dba7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_logits = torch.matmul(weights, value)\n",
        "attention_logits.shape\n",
        "### Now look at the scaled dot product function above"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWhq9Rb0RRPP",
        "outputId": "a9bb5be0-a68a-45d9-a727-95a5cfc2ac0d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([172, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#m = decode(scores[:3].tolist())\n",
        "m = scores[:5].tolist()\n",
        "m[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AD1mjs8SLHA",
        "outputId": "9027dca4-ba82-40be-a54f-9cc6f96852ef"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0342422723770142,\n",
              "  0.20540887117385864,\n",
              "  -1.5626333951950073,\n",
              "  -0.05896643549203873,\n",
              "  -0.9931272268295288,\n",
              "  -0.47809404134750366,\n",
              "  -0.18479514122009277,\n",
              "  0.801186203956604,\n",
              "  -1.481999397277832,\n",
              "  -0.6528627872467041,\n",
              "  0.6440685391426086,\n",
              "  -0.6117128729820251,\n",
              "  2.2193775177001953,\n",
              "  -0.6889744400978088,\n",
              "  0.1810396909713745,\n",
              "  1.1753352880477905,\n",
              "  -1.5324331521987915,\n",
              "  0.027971414849162102,\n",
              "  0.9061397314071655,\n",
              "  0.7818080186843872,\n",
              "  -0.3954094350337982,\n",
              "  0.8335515260696411,\n",
              "  -0.3954094350337982,\n",
              "  0.8242469429969788,\n",
              "  -0.7864075303077698,\n",
              "  0.5478734970092773,\n",
              "  -0.36804521083831787,\n",
              "  -1.5315810441970825,\n",
              "  -0.6273103356361389,\n",
              "  -0.7414959073066711,\n",
              "  0.8335515260696411,\n",
              "  -0.43255582451820374,\n",
              "  -0.5283082723617554,\n",
              "  0.18301750719547272,\n",
              "  -1.1334327459335327,\n",
              "  -0.6214486956596375,\n",
              "  0.7942524552345276,\n",
              "  0.14834068715572357,\n",
              "  0.054729580879211426,\n",
              "  -0.47809404134750366,\n",
              "  0.17520080506801605,\n",
              "  1.161338448524475,\n",
              "  1.0053986310958862,\n",
              "  -0.47809404134750366,\n",
              "  0.8335515260696411,\n",
              "  -0.5283082723617554,\n",
              "  -1.5276403427124023,\n",
              "  1.5446666479110718,\n",
              "  0.2779058516025543,\n",
              "  -0.5283082723617554,\n",
              "  0.4996093809604645,\n",
              "  -0.041296809911727905,\n",
              "  -0.934061586856842,\n",
              "  -0.058247555047273636,\n",
              "  0.8709485530853271,\n",
              "  -1.2062342166900635,\n",
              "  0.9629801511764526,\n",
              "  0.6889790892601013,\n",
              "  -1.0761098861694336,\n",
              "  0.8568218350410461,\n",
              "  0.31196123361587524,\n",
              "  -0.041296809911727905,\n",
              "  0.2088858187198639,\n",
              "  0.2779058516025543,\n",
              "  -0.8211394548416138,\n",
              "  0.8386353254318237,\n",
              "  -0.47809404134750366,\n",
              "  0.8335515260696411,\n",
              "  0.2779058516025543,\n",
              "  -0.5283082723617554,\n",
              "  0.4100796580314636,\n",
              "  -0.48054197430610657,\n",
              "  0.24279476702213287,\n",
              "  -0.7260441184043884,\n",
              "  1.1273918151855469,\n",
              "  0.7382808923721313,\n",
              "  -0.47809404134750366,\n",
              "  0.42732077836990356,\n",
              "  0.18301750719547272,\n",
              "  1.1157114505767822,\n",
              "  0.05677425116300583,\n",
              "  0.18301750719547272,\n",
              "  -1.3867977857589722,\n",
              "  0.5625859498977661,\n",
              "  0.5916496515274048,\n",
              "  0.9940657019615173,\n",
              "  0.3648740351200104,\n",
              "  0.7261645793914795,\n",
              "  0.2779058516025543,\n",
              "  0.7942524552345276,\n",
              "  -0.20958064496517181,\n",
              "  -0.18479514122009277,\n",
              "  -1.1716748476028442,\n",
              "  -0.8950702548027039,\n",
              "  -0.5513252019882202,\n",
              "  -0.9261310696601868,\n",
              "  0.06246376037597656,\n",
              "  -0.18479514122009277,\n",
              "  0.8296648859977722,\n",
              "  0.8709485530853271,\n",
              "  -1.7152314186096191,\n",
              "  0.8350431323051453,\n",
              "  0.6889790892601013,\n",
              "  0.7942524552345276,\n",
              "  0.8101135492324829,\n",
              "  -0.2930276095867157,\n",
              "  1.244353175163269,\n",
              "  0.2824312746524811,\n",
              "  1.6593986749649048,\n",
              "  -0.7501859068870544,\n",
              "  0.7942524552345276,\n",
              "  -0.20958064496517181,\n",
              "  -0.18479514122009277,\n",
              "  -0.6368962526321411,\n",
              "  -0.47809404134750366,\n",
              "  0.17520080506801605,\n",
              "  0.10806342214345932,\n",
              "  0.401845246553421,\n",
              "  0.7942524552345276,\n",
              "  0.14858287572860718,\n",
              "  0.9020659923553467,\n",
              "  0.3010682165622711,\n",
              "  0.8335515260696411,\n",
              "  0.14858287572860718,\n",
              "  0.017056144773960114,\n",
              "  1.4461787939071655,\n",
              "  0.5141102075576782,\n",
              "  0.6158698797225952,\n",
              "  0.5053097605705261,\n",
              "  0.4866545796394348,\n",
              "  -0.43190303444862366,\n",
              "  0.21687428653240204,\n",
              "  0.17293845117092133,\n",
              "  -1.7152314186096191,\n",
              "  0.054729580879211426,\n",
              "  -0.3897246718406677,\n",
              "  -1.7152314186096191,\n",
              "  0.17520080506801605,\n",
              "  0.3267030417919159,\n",
              "  -1.7152314186096191,\n",
              "  0.36751988530158997,\n",
              "  1.161338448524475,\n",
              "  0.33013924956321716,\n",
              "  -0.39739203453063965,\n",
              "  -1.0366700887680054,\n",
              "  0.5916496515274048,\n",
              "  -0.871671199798584,\n",
              "  0.4768771827220917,\n",
              "  0.24210096895694733,\n",
              "  0.7942524552345276,\n",
              "  -0.49893394112586975,\n",
              "  0.18301750719547272,\n",
              "  0.580617368221283,\n",
              "  -0.6368962526321411,\n",
              "  -0.47809404134750366,\n",
              "  1.191164255142212,\n",
              "  1.161338448524475,\n",
              "  -0.17042887210845947,\n",
              "  -0.3716675341129303,\n",
              "  -0.49478816986083984,\n",
              "  -0.27585887908935547,\n",
              "  0.11709406226873398,\n",
              "  -0.7443332672119141,\n",
              "  -1.3449928760528564,\n",
              "  -0.9909066557884216,\n",
              "  0.00331847439520061,\n",
              "  0.16263647377490997,\n",
              "  0.2779058516025543,\n",
              "  0.14858287572860718,\n",
              "  0.10806342214345932,\n",
              "  0.5004359483718872,\n",
              "  1.161338448524475]]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}