{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3iL7zQWCmdW",
        "outputId": "98a2930f-34eb-4ee7-a545-431a1f724fba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So',\n",
              " 'Ray',\n",
              " 'J',\n",
              " 'went',\n",
              " 'straight',\n",
              " 'to',\n",
              " 'the',\n",
              " 'radio',\n",
              " 'station',\n",
              " 'The',\n",
              " 'very',\n",
              " 'next',\n",
              " 'day,',\n",
              " '\"Hey',\n",
              " 'Fab,',\n",
              " \"I'ma\",\n",
              " 'kill',\n",
              " 'you!\"',\n",
              " 'Lyrics',\n",
              " \"comin'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from karpathy\n",
        "# create a bigram by sliding across 2 words at a time\n",
        "data = {}\n",
        "for w in words:\n",
        "  chars = list(w)\n",
        "  for ch1, ch2 in zip(chars, chars[2:]):\n",
        "    bigram = (ch1, ch2)\n",
        "    data[bigram] = data.get(bigram, 0) + 1\n",
        "\n"
      ],
      "metadata": {
        "id": "K5iG857vD4RG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(data.items(), key = lambda kv: -kv[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyqmGYeKEguO",
        "outputId": "0d865407-7039-4693-ccae-92b0f806a717"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'e'), 11),\n",
              " (('a', 'i'), 9),\n",
              " (('i', 'g'), 9),\n",
              " (('t', 'a'), 7),\n",
              " (('i', \"'\"), 7),\n",
              " (('h', 't'), 7),\n",
              " (('a', 'e'), 7),\n",
              " (('t', 'n'), 7),\n",
              " (('y', 'u'), 6),\n",
              " (('u', 'e'), 6),\n",
              " (('a', 'd'), 6),\n",
              " (('e', 'e'), 6),\n",
              " (('g', 't'), 5),\n",
              " (('I', 'm'), 5),\n",
              " (('m', 'n'), 5),\n",
              " (('v', 't'), 5),\n",
              " (('e', 'l'), 5),\n",
              " (('h', 'y'), 5),\n",
              " (('v', 'r'), 4),\n",
              " (('u', 'i'), 4),\n",
              " (('n', 't'), 4),\n",
              " (('o', 'e'), 4),\n",
              " (('t', 's'), 4),\n",
              " (('s', 'r'), 3),\n",
              " (('i', 'h'), 3),\n",
              " (('t', 't'), 3),\n",
              " ((\"'\", 'a'), 3),\n",
              " (('r', 'c'), 3),\n",
              " (('o', 'i'), 3),\n",
              " (('e', 's'), 3),\n",
              " (('u', 'm'), 3),\n",
              " (('m', 'a'), 3),\n",
              " (('m', ','), 3),\n",
              " (('a', 's'), 3),\n",
              " (('t', 'r'), 3),\n",
              " (('r', 'u'), 3),\n",
              " (('i', 'e'), 3),\n",
              " (('h', 'n'), 3),\n",
              " (('e', 'i'), 3),\n",
              " (('o', 'f'), 3),\n",
              " (('e', 'a'), 3),\n",
              " (('l', 'v'), 3),\n",
              " (('m', 'k'), 3),\n",
              " (('e', 't'), 2),\n",
              " (('d', 'o'), 2),\n",
              " (('s', 'a'), 2),\n",
              " (('t', 'o'), 2),\n",
              " (('i', 'n'), 2),\n",
              " (('d', 'y'), 2),\n",
              " (('a', ','), 2),\n",
              " (('i', 's'), 2),\n",
              " (('s', 'p'), 2),\n",
              " (('p', 'r'), 2),\n",
              " (('s', 'm'), 2),\n",
              " (('m', '-'), 2),\n",
              " (('a', 'l'), 2),\n",
              " (('-', 'u'), 2),\n",
              " (('l', 'm'), 2),\n",
              " (('o', 'a'), 2),\n",
              " (('h', 'm'), 2),\n",
              " (('u', 'a'), 2),\n",
              " (('h', 'o'), 2),\n",
              " (('t', 'v'), 2),\n",
              " (('Y', 'u'), 2),\n",
              " (('s', 'y'), 2),\n",
              " (('o', 'h'), 2),\n",
              " ((\"'\", 'l'), 2),\n",
              " (('a', 't'), 2),\n",
              " (('n', ','), 2),\n",
              " (('o', 's'), 2),\n",
              " (('r', 't'), 2),\n",
              " (('m', 't'), 2),\n",
              " (('e', 'f'), 2),\n",
              " (('i', 'a'), 2),\n",
              " (('k', 'o'), 2),\n",
              " (('n', 'w'), 2),\n",
              " (('c', 'n'), 2),\n",
              " (('w', 'y'), 2),\n",
              " ((\"'\", 'm'), 2),\n",
              " (('m', 's'), 2),\n",
              " (('s', 'c'), 2),\n",
              " (('a', 'o'), 2),\n",
              " (('f', 's'), 2),\n",
              " (('h', 'p'), 2),\n",
              " (('o', ','), 2),\n",
              " (('o', 'k'), 2),\n",
              " (('R', 'y'), 1),\n",
              " (('w', 'n'), 1),\n",
              " (('r', 'i'), 1),\n",
              " (('a', 'g'), 1),\n",
              " (('r', 'd'), 1),\n",
              " (('T', 'e'), 1),\n",
              " (('e', 'y'), 1),\n",
              " (('n', 'x'), 1),\n",
              " (('\"', 'e'), 1),\n",
              " (('H', 'y'), 1),\n",
              " (('F', 'b'), 1),\n",
              " (('k', 'l'), 1),\n",
              " (('i', 'l'), 1),\n",
              " (('o', '!'), 1),\n",
              " (('u', '\"'), 1),\n",
              " (('L', 'r'), 1),\n",
              " (('y', 'i'), 1),\n",
              " (('c', 'm'), 1),\n",
              " (('r', 'o'), 1),\n",
              " (('s', 'n'), 1),\n",
              " (('n', 'c'), 1),\n",
              " (('s', 'e'), 1),\n",
              " (('p', 'e'), 1),\n",
              " (('e', 'd'), 1),\n",
              " (('(', '.'), 1),\n",
              " (('J', 'J'), 1),\n",
              " (('.', '.'), 1),\n",
              " (('F', 'd'), 1),\n",
              " (('a', ')'), 1),\n",
              " (('U', ','), 1),\n",
              " (('o', 'm'), 1),\n",
              " (('s', 'u'), 1),\n",
              " (('W', 'a'), 1),\n",
              " (('o', 't'), 1),\n",
              " (('o', 'g'), 1),\n",
              " (('u', 'h'), 1),\n",
              " (('e', 'h'), 1),\n",
              " (('a', '?'), 1),\n",
              " (('I', 'n'), 1),\n",
              " (('n', 'o'), 1),\n",
              " (('n', 'v'), 1),\n",
              " (('m', 'd'), 1),\n",
              " (('r', 'b'), 1),\n",
              " (('u', 'b'), 1),\n",
              " (('b', 'e'), 1),\n",
              " (('b', 'r'), 1),\n",
              " (('a', 'y'), 1),\n",
              " (('y', 'h'), 1),\n",
              " (('t', 'i'), 1),\n",
              " (('i', 'o'), 1),\n",
              " (('c', 'c'), 1),\n",
              " (('c', 'e'), 1),\n",
              " (('t', 'l'), 1),\n",
              " (('g', 'u'), 1),\n",
              " (('l', 'e'), 1),\n",
              " (('d', 'v'), 1),\n",
              " (('v', 's'), 1),\n",
              " (('m', 'r'), 1),\n",
              " (('d', 'm'), 1),\n",
              " (('e', 'o'), 1),\n",
              " (('H', 'w'), 1),\n",
              " (('g', 'v'), 1),\n",
              " (('h', 'r'), 1),\n",
              " (('f', 'c'), 1),\n",
              " (('u', 'k'), 1),\n",
              " (('c', 'i'), 1),\n",
              " (('k', 'n'), 1),\n",
              " (('d', 'e'), 1),\n",
              " (('e', 'c'), 1),\n",
              " (('n', 'e'), 1),\n",
              " (('f', 'e'), 1),\n",
              " (('l', 'n'), 1),\n",
              " (('l', 'k'), 1),\n",
              " (('N', 'v'), 1),\n",
              " (('f', 'd'), 1),\n",
              " (('d', 'n'), 1),\n",
              " (('f', 'r'), 1),\n",
              " (('r', 'v'), 1),\n",
              " (('w', 'i'), 1),\n",
              " (('i', 'i'), 1),\n",
              " (('F', 'r'), 1),\n",
              " (('f', 'l'), 1),\n",
              " (('f', ','), 1),\n",
              " (('e', \"'\"), 1),\n",
              " (('y', 'l'), 1),\n",
              " (('c', 'l'), 1),\n",
              " (('l', 'b'), 1),\n",
              " (('e', 'r'), 1),\n",
              " (('b', 'a'), 1),\n",
              " (('C', 'u'), 1),\n",
              " (('t', 'd'), 1),\n",
              " (('i', ','), 1),\n",
              " (('\"', 'h'), 1),\n",
              " (('O', ','), 1),\n",
              " (('h', \"'\"), 1),\n",
              " (('m', 'i'), 1),\n",
              " (('a', 'n'), 1),\n",
              " (('r', 'a'), 1),\n",
              " (('e', 'm'), 1),\n",
              " (('a', '\"'), 1),\n",
              " (('W', 'l'), 1),\n",
              " (('l', ','), 1),\n",
              " (('a', \"'\"), 1),\n",
              " (('w', 'a'), 1),\n",
              " (('w', 'e'), 1),\n",
              " (('j', 'a'), 1),\n",
              " (('l', 'u'), 1),\n",
              " (('u', ','), 1),\n",
              " (('n', 'u'), 1),\n",
              " (('\"', 't'), 1),\n",
              " (('I', \"'\"), 1),\n",
              " (('i', '-'), 1),\n",
              " (('p', 'h'), 1),\n",
              " (('-', 'o'), 1),\n",
              " (('p', 'p'), 1),\n",
              " (('c', 'u'), 1),\n",
              " (('f', 'u'), 1),\n",
              " (('o', 'n'), 1),\n",
              " (('u', 'd'), 1),\n",
              " (('h', 'l'), 1),\n",
              " (('l', 'a'), 1),\n",
              " (('W', 't'), 1),\n",
              " (('c', ','), 1),\n",
              " (('s', 'o'), 1),\n",
              " (('h', 'c'), 1),\n",
              " (('r', 'p'), 1),\n",
              " (('w', 't'), 1),\n",
              " (('D', 'c'), 1),\n",
              " (('T', 'r'), 1),\n",
              " (('r', 'w'), 1),\n",
              " (('\"', 'o'), 1),\n",
              " (('L', 's'), 1),\n",
              " (('o', 'r'), 1),\n",
              " (('u', 's'), 1),\n",
              " (('r', 'e'), 1),\n",
              " (('s', 'l'), 1),\n",
              " (('l', '\"'), 1),\n",
              " (('l', 's'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(words)))"
      ],
      "metadata": {
        "id": "sWqoBOBxH0hl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# split the training data\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEcBOKFlHKLR",
        "outputId": "71480341-658d-45b1-e8f5-61646fae6773"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "<function <lambda> at 0x7f24bb0acd30>\n",
            "<function <lambda> at 0x7f24bb0acee0>\n",
            "tensor([ 22,  21,  18, 119, 103, 114, 110,  93, 102,  23, 116,  86,  44,   1,\n",
            "         10,  16,  72, 124,  19,  41,  36, 123,  36, 106, 101,   8,  11,  25,\n",
            "        104,  48, 123,  35,  15,  31,  66,  27,  14,  61,  47, 114,  58,  68,\n",
            "        113, 114, 123,  15, 105,  17,  32,  15,  77,  88,  97, 100, 108,  33,\n",
            "         29,  98,  67,  95,  89,  88,  80,  32,  69,  60, 114, 123,  32,  15,\n",
            "         46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,  53,  75,\n",
            "         70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,  12, 110,\n",
            "         43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,  14,  73,\n",
            "        110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,  50,  84,\n",
            "          4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,  71, 111,\n",
            "         42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,  64, 118,\n",
            "        114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,  30,  32,\n",
            "         79,   7,  76,  68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "n = int(0.4 * len(words))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(val_data)\n",
        "\n",
        "p = decode(val_data[:18].tolist())\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "WMPkvDxvMP3N",
        "outputId": "460237e0-94c5-481d-9490-ab59fe2d7f69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 32,  15,  46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,\n",
            "         53,  75,  70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,\n",
            "         12, 110,  43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,\n",
            "         14,  73, 110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,\n",
            "         50,  84,   4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,\n",
            "         71, 111,  42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,\n",
            "         64, 118, 114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,\n",
            "         30,  32,  79,   7,  76,  68])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"andI'mdevastating,morethaneverdemonstratingHowtogiveamotherfuckin'audienceafeelinglikeit'slevitating\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ix = torch.randint(len(data) - 5, (12,))\n",
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAlzg-rQW5JT",
        "outputId": "32d635b3-61d0-41da-b0ce-9116facb2efb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([52,  4, 37, 20, 53, 56,  9, 40, 47,  4, 44, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "res = dict(enumerate(words))\n",
        "print(res)\n",
        "keyList = [key for key in res]\n",
        "print(keyList)\n",
        "l = len(keyList)\n",
        "l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nS-JjGF9o9x",
        "outputId": "2e71dcbf-2474-4c0c-c636-30a8ba7601c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "{0: 'So', 1: 'Ray', 2: 'J', 3: 'went', 4: 'straight', 5: 'to', 6: 'the', 7: 'radio', 8: 'station', 9: 'The', 10: 'very', 11: 'next', 12: 'day,', 13: '\"Hey', 14: 'Fab,', 15: \"I'ma\", 16: 'kill', 17: 'you!\"', 18: 'Lyrics', 19: \"comin'\", 20: 'at', 21: 'you', 22: 'at', 23: 'supersonic', 24: 'speed', 25: '(J.J.', 26: 'Fad)', 27: 'Uh,', 28: 'summa-lumma,', 29: 'dooma-lumma,', 30: 'you', 31: \"assumin'\", 32: \"I'm\", 33: 'a', 34: 'human', 35: 'What', 36: 'I', 37: 'gotta', 38: 'do', 39: 'to', 40: 'get', 41: 'it', 42: 'through', 43: 'to', 44: 'you', 45: \"I'm\", 46: 'superhuman?', 47: 'Innovative', 48: 'and', 49: \"I'm\", 50: 'made', 51: 'of', 52: 'rubber', 53: 'so', 54: 'that', 55: 'anything', 56: 'You', 57: 'say', 58: 'is', 59: \"ricochetin'\", 60: 'off', 61: 'of', 62: 'me,', 63: 'and', 64: \"it'll\", 65: 'glue', 66: 'to', 67: 'you', 68: 'and', 69: \"I'm\", 70: 'devastating,', 71: 'more', 72: 'than', 73: 'ever', 74: 'demonstrating', 75: 'How', 76: 'to', 77: 'give', 78: 'a', 79: \"motherfuckin'\", 80: 'audience', 81: 'a', 82: 'feeling', 83: 'like', 84: \"it's\", 85: 'levitating', 86: 'Never', 87: 'fading,', 88: 'and', 89: 'I', 90: 'know', 91: 'the', 92: 'haters', 93: 'are', 94: 'forever', 95: 'waiting', 96: 'For', 97: 'the', 98: 'day', 99: 'that', 100: 'they', 101: 'can', 102: 'say', 103: 'I', 104: 'fell', 105: 'off,', 106: \"they'll\", 107: 'be', 108: 'celebrating', 109: \"'Cause\", 110: 'I', 111: 'know', 112: 'the', 113: 'way', 114: 'to', 115: 'get', 116: \"'em\", 117: 'motivated', 118: 'I', 119: 'make', 120: 'elevating', 121: 'music,', 122: 'you', 123: 'make', 124: 'elevator', 125: 'music', 126: '\"Oh,', 127: \"he's\", 128: 'too', 129: 'mainstream\"', 130: 'Well,', 131: \"that's\", 132: 'what', 133: 'they', 134: 'do', 135: 'when', 136: 'they', 137: 'get', 138: 'jealous,', 139: 'they', 140: 'confuse', 141: 'it', 142: '\"It\\'s', 143: 'not', 144: 'hip-hop,', 145: \"it's\", 146: 'pop,', 147: '\"', 148: \"'cause\", 149: 'I', 150: 'found', 151: 'a', 152: 'hella', 153: 'way', 154: 'to', 155: 'fuse', 156: 'it', 157: 'With', 158: 'rock,', 159: 'shock', 160: 'rap', 161: 'with', 162: 'Doc', 163: 'Throw', 164: 'on', 165: '\"Lose', 166: 'Yourself\"', 167: 'and', 168: 'make', 169: \"'em\", 170: 'lose', 171: 'it'}\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scaled dot product attention\n",
        "# 3 matrices, Q -> query matrix\n",
        "# K -> Key matrix\n",
        "# V -> value matrix\n",
        "# T is the length of the sequence\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "import torch\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ],
      "metadata": {
        "id": "aFiuKJk29oTJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dk and Dv\n",
        " are the hidden dimensionality for queries/keys and values respectively"
      ],
      "metadata": {
        "id": "py-vXvCwGjry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "size = len(chars) # sequence length\n",
        "# print(size)\n",
        "\n",
        "\n",
        "d_k = 2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "q = torch.randn(size, d_k)\n",
        "\n",
        "#q = torch.randn(size, d_k).detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "k = torch.randn(size, d_k)\n",
        "v = torch.randn(size, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "# print(\"Q\\n\", q)\n",
        "# print(\"K\\n\", k)\n",
        "# print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values[:5].reshape(1, -1))\n",
        "print(\"Attention\\n\", attention[:5].reshape(1, -1))\n",
        "# p = torch.Tensor(values)\n",
        "# p\n",
        "\n",
        "values.tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKxcaZJ1FWri",
        "outputId": "2fbdd996-4a7e-4613-8309-b1af5e6b94b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values\n",
            " tensor([[-0.0024,  0.0350, -0.0502,  0.0396,  0.0462,  0.0850,  0.0854,  0.1279,\n",
            "          0.0942,  0.0176]])\n",
            "Attention\n",
            " tensor([[0.0082, 0.0072, 0.0058, 0.0073, 0.0079, 0.0100, 0.0100, 0.0078, 0.0066,\n",
            "         0.0067, 0.0076, 0.0076, 0.0064, 0.0107, 0.0067, 0.0085, 0.0071, 0.0081,\n",
            "         0.0073, 0.0090, 0.0070, 0.0065, 0.0071, 0.0062, 0.0083, 0.0093, 0.0084,\n",
            "         0.0106, 0.0094, 0.0092, 0.0097, 0.0066, 0.0079, 0.0071, 0.0103, 0.0086,\n",
            "         0.0114, 0.0071, 0.0067, 0.0075, 0.0074, 0.0101, 0.0089, 0.0077, 0.0071,\n",
            "         0.0078, 0.0079, 0.0074, 0.0084, 0.0079, 0.0067, 0.0083, 0.0083, 0.0068,\n",
            "         0.0084, 0.0078, 0.0060, 0.0077, 0.0079, 0.0081, 0.0072, 0.0071, 0.0091,\n",
            "         0.0077, 0.0090, 0.0085, 0.0091, 0.0080, 0.0077, 0.0074, 0.0088, 0.0093,\n",
            "         0.0098, 0.0063, 0.0087, 0.0078, 0.0071, 0.0080, 0.0076, 0.0093, 0.0088,\n",
            "         0.0073, 0.0080, 0.0063, 0.0082, 0.0078, 0.0072, 0.0103, 0.0080, 0.0086,\n",
            "         0.0073, 0.0081, 0.0085, 0.0071, 0.0075, 0.0063, 0.0082, 0.0066, 0.0073,\n",
            "         0.0088, 0.0082, 0.0085, 0.0069, 0.0080, 0.0072, 0.0072, 0.0082, 0.0087,\n",
            "         0.0091, 0.0068, 0.0091, 0.0095, 0.0080, 0.0081, 0.0097, 0.0071, 0.0092,\n",
            "         0.0108, 0.0065, 0.0077, 0.0061, 0.0077, 0.0081, 0.0072, 0.0083, 0.0006,\n",
            "         0.0022, 0.0031, 0.0037, 0.0079, 0.0037, 0.0046, 0.0194, 0.0042, 0.0142,\n",
            "         0.0233, 0.0032, 0.0034, 0.0057, 0.0051, 0.0103, 0.0059, 0.0082, 0.0032,\n",
            "         0.0163, 0.0052, 0.0029, 0.0105, 0.0038, 0.0032, 0.0024, 0.0037, 0.0040,\n",
            "         0.0045, 0.0044, 0.0021, 0.0088, 0.0168, 0.0117, 0.0066, 0.0262, 0.0109,\n",
            "         0.0033, 0.0033, 0.0112, 0.0046, 0.0169, 0.0054, 0.0024, 0.0020, 0.0077,\n",
            "         0.0186, 0.0031, 0.0101, 0.0056, 0.0092, 0.0229, 0.0073, 0.0061, 0.0036,\n",
            "         0.0041, 0.0461, 0.0045, 0.0030, 0.0061, 0.0060, 0.0080, 0.0034, 0.0108,\n",
            "         0.0140, 0.0055, 0.0124, 0.0057, 0.0085, 0.0017, 0.0203, 0.0078, 0.0082,\n",
            "         0.0043, 0.0081, 0.0027, 0.0058, 0.0060, 0.0035, 0.0143, 0.0022, 0.0013,\n",
            "         0.0042, 0.0039, 0.0093, 0.0092, 0.0133, 0.0033, 0.0148, 0.0057, 0.0032,\n",
            "         0.0073, 0.0051, 0.0062, 0.0057, 0.0069, 0.0052, 0.0079, 0.0071, 0.0160,\n",
            "         0.0160, 0.0126, 0.0064, 0.0064, 0.0124, 0.0047, 0.0092, 0.0064, 0.0116,\n",
            "         0.0023, 0.0069, 0.0069, 0.0016, 0.0043, 0.0077, 0.0056, 0.0079, 0.0055,\n",
            "         0.0100, 0.0073, 0.0054, 0.0285, 0.0106, 0.0095, 0.0162, 0.0147, 0.0073,\n",
            "         0.0035, 0.0066, 0.0069, 0.0166, 0.0159, 0.0052, 0.0047, 0.0037, 0.0046,\n",
            "         0.0075, 0.0045, 0.0181, 0.0048, 0.0080, 0.0053, 0.0072, 0.0069, 0.0083,\n",
            "         0.0054, 0.0049, 0.0046, 0.0041, 0.0100, 0.0148, 0.0099, 0.0193, 0.0131,\n",
            "         0.0124, 0.0174, 0.0038, 0.0056, 0.0045, 0.0158, 0.0064, 0.0186, 0.0061,\n",
            "         0.0053, 0.0054, 0.0065, 0.0115, 0.0107, 0.0085, 0.0072, 0.0067, 0.0054,\n",
            "         0.0072, 0.0077, 0.0074, 0.0041, 0.0060, 0.0079, 0.0047, 0.0099, 0.0077,\n",
            "         0.0020, 0.0074, 0.0088, 0.0077, 0.0054, 0.0049, 0.0128, 0.0059, 0.0086,\n",
            "         0.0092, 0.0091, 0.0076, 0.0063, 0.0083, 0.0074, 0.0111, 0.0127, 0.0041,\n",
            "         0.0091, 0.0086, 0.0054, 0.0076, 0.0074, 0.0096, 0.0130, 0.0087, 0.0083,\n",
            "         0.0041, 0.0074, 0.0064, 0.0045, 0.0185, 0.0060, 0.0094, 0.0068, 0.0076,\n",
            "         0.0094, 0.0053, 0.0065, 0.0036, 0.0085, 0.0041, 0.0056, 0.0078, 0.0063,\n",
            "         0.0076, 0.0047, 0.0074, 0.0046, 0.0058, 0.0074, 0.0097, 0.0093, 0.0061,\n",
            "         0.0106, 0.0123, 0.0106, 0.0085, 0.0127, 0.0055, 0.0107, 0.0186, 0.0036,\n",
            "         0.0065, 0.0035, 0.0045, 0.0068, 0.0050, 0.0066, 0.0198, 0.0066, 0.0022,\n",
            "         0.0056, 0.0059, 0.0226, 0.0209, 0.0037, 0.0033, 0.0022, 0.0031, 0.0069,\n",
            "         0.0032, 0.0255, 0.0034, 0.0073, 0.0040, 0.0062, 0.0060, 0.0077, 0.0041,\n",
            "         0.0036, 0.0031, 0.0027, 0.0105, 0.0192, 0.0103, 0.0284, 0.0156, 0.0145,\n",
            "         0.0246, 0.0024, 0.0042, 0.0030, 0.0207, 0.0051, 0.0260, 0.0051, 0.0040,\n",
            "         0.0041, 0.0054, 0.0125, 0.0115, 0.0084, 0.0065, 0.0056, 0.0040, 0.0065,\n",
            "         0.0069, 0.0066, 0.0026, 0.0046, 0.0073, 0.0034, 0.0103, 0.0071, 0.0008,\n",
            "         0.0066, 0.0088, 0.0070, 0.0041, 0.0035, 0.0152, 0.0046, 0.0080, 0.0092,\n",
            "         0.0088, 0.0069, 0.0051, 0.0081, 0.0064, 0.0120, 0.0148, 0.0027, 0.0090,\n",
            "         0.0084, 0.0041, 0.0069, 0.0067, 0.0094, 0.0158, 0.0087, 0.0079, 0.0027,\n",
            "         0.0065, 0.0052, 0.0030, 0.0267, 0.0046, 0.0095, 0.0059, 0.0068, 0.0095,\n",
            "         0.0040, 0.0054, 0.0022, 0.0082, 0.0027, 0.0043, 0.0069, 0.0051, 0.0067,\n",
            "         0.0033, 0.0066, 0.0032, 0.0046, 0.0065, 0.0099, 0.0091, 0.0050, 0.0112,\n",
            "         0.0141, 0.0118, 0.0082, 0.0148, 0.0042, 0.0115, 0.0266, 0.0022, 0.0054,\n",
            "         0.0021, 0.0030, 0.0058, 0.0036, 0.0054, 0.1001, 0.0209, 0.0148, 0.0107,\n",
            "         0.0040, 0.0091, 0.0071, 0.0013, 0.0097, 0.0021, 0.0011, 0.0128, 0.0126,\n",
            "         0.0053, 0.0075, 0.0028, 0.0061, 0.0038, 0.0128, 0.0016, 0.0071, 0.0152,\n",
            "         0.0030, 0.0111, 0.0119, 0.0166, 0.0102, 0.0081, 0.0074, 0.0078, 0.0186,\n",
            "         0.0038, 0.0016, 0.0026, 0.0045, 0.0009, 0.0023, 0.0126, 0.0127, 0.0027,\n",
            "         0.0081, 0.0014, 0.0061, 0.0177, 0.0237, 0.0042, 0.0014, 0.0132, 0.0029,\n",
            "         0.0062, 0.0036, 0.0011, 0.0044, 0.0059, 0.0103, 0.0093, 0.0005, 0.0083,\n",
            "         0.0135, 0.0055, 0.0059, 0.0042, 0.0108, 0.0028, 0.0019, 0.0062, 0.0022,\n",
            "         0.0060, 0.0037, 0.0290, 0.0012, 0.0038, 0.0035, 0.0096, 0.0037, 0.0157,\n",
            "         0.0062, 0.0056, 0.0114, 0.0018, 0.0183, 0.0407, 0.0089, 0.0107, 0.0032,\n",
            "         0.0034, 0.0022, 0.0107, 0.0019, 0.0058, 0.0127, 0.0044, 0.0067, 0.0057,\n",
            "         0.0061, 0.0053, 0.0066, 0.0043, 0.0048, 0.0016, 0.0017, 0.0022, 0.0056,\n",
            "         0.0052, 0.0024, 0.0081, 0.0033, 0.0050, 0.0023, 0.0202, 0.0045, 0.0043,\n",
            "         0.0302, 0.0086, 0.0037, 0.0065, 0.0037, 0.0054, 0.0033, 0.0045, 0.0074,\n",
            "         0.0008, 0.0028, 0.0034, 0.0016]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.0024424439761787653, 0.034983038902282715],\n",
              " [-0.050162434577941895, 0.03959700092673302],\n",
              " [0.04617689922451973, 0.0849725529551506],\n",
              " [0.08537839353084564, 0.12790806591510773],\n",
              " [0.09422025084495544, 0.017580723389983177],\n",
              " [-0.1264338195323944, 0.02399449422955513],\n",
              " [0.0025612979661673307, 0.04516935721039772],\n",
              " [-0.06211200729012489, -0.14241111278533936],\n",
              " [0.11488749086856842, 0.18445967137813568],\n",
              " [0.051258690655231476, 0.019085988402366638],\n",
              " [-0.08051832020282745, -0.1278943568468094],\n",
              " [0.20601433515548706, 0.18496564030647278],\n",
              " [0.012129737064242363, 0.07835805416107178],\n",
              " [-0.10039397329092026, -0.010585248470306396],\n",
              " [0.0260832067579031, 0.07921289652585983],\n",
              " [1.2068865299224854, -0.21861864626407623],\n",
              " [-0.06788432598114014, -0.01689816266298294],\n",
              " [0.09440572559833527, 0.13482505083084106],\n",
              " [0.047430962324142456, -0.0018940325826406479],\n",
              " [-0.09803760796785355, -0.015557516366243362],\n",
              " [-0.06463036686182022, 0.039879269897937775],\n",
              " [0.04460662603378296, 0.10082023590803146],\n",
              " [-0.06786197423934937, -0.11546669155359268],\n",
              " [-0.07085174322128296, 0.021415341645479202],\n",
              " [0.08408580720424652, 0.23249703645706177],\n",
              " [0.10790508985519409, 0.1856372207403183],\n",
              " [-0.07390296459197998, -0.24841350317001343],\n",
              " [-0.20496270060539246, 0.45068177580833435],\n",
              " [0.04259226843714714, 0.21631474792957306],\n",
              " [0.06323853135108948, 0.11288473755121231],\n",
              " [0.32137107849121094, 0.01381275337189436],\n",
              " [0.11352662742137909, 0.22290925681591034],\n",
              " [-0.061721060425043106, -0.08920096606016159],\n",
              " [-0.023547464981675148, 0.012024267576634884],\n",
              " [-0.049802232533693314, -0.05151798948645592],\n",
              " [-0.0387587733566761, 0.05011780932545662],\n",
              " [0.1044725850224495, 0.2801821231842041],\n",
              " [0.020276764407753944, 0.020538443699479103],\n",
              " [-0.06469479203224182, -0.16202233731746674],\n",
              " [-0.0033133956603705883, 0.023251930251717567],\n",
              " [-0.040326714515686035, 0.37206679582595825],\n",
              " [0.128879576921463, 0.12441898137331009],\n",
              " [-0.22545892000198364, 0.17358402907848358],\n",
              " [0.049806129187345505, 0.09604144096374512],\n",
              " [0.013444907031953335, 0.05880894511938095],\n",
              " [0.028337931260466576, 0.08114787936210632],\n",
              " [-0.05397949367761612, 0.005993328057229519],\n",
              " [-0.09354491531848907, -0.004521561320871115],\n",
              " [-0.24713677167892456, 0.19789570569992065],\n",
              " [-0.014638207852840424, -0.032824479043483734],\n",
              " [-0.014189084060490131, 0.04506533592939377],\n",
              " [-0.041916802525520325, -0.012674777768552303],\n",
              " [-0.03402138501405716, 0.004023995716124773],\n",
              " [0.12008793652057648, -0.0020702544134110212],\n",
              " [0.4847713112831116, -0.12070554494857788],\n",
              " [-0.07899395376443863, 0.12319979071617126],\n",
              " [-0.04412655904889107, -0.014746583998203278],\n",
              " [0.06004246324300766, 0.1434674710035324],\n",
              " [0.0037914151325821877, 0.020065993070602417],\n",
              " [-0.07045281678438187, -0.14242567121982574],\n",
              " [0.18573205173015594, 0.2339632213115692],\n",
              " [0.011732527986168861, 0.0632440522313118],\n",
              " [-0.058429088443517685, -0.05754159390926361],\n",
              " [-0.06896853446960449, 0.08207383751869202],\n",
              " [-0.050417300313711166, -0.3103606402873993],\n",
              " [0.09675606340169907, 0.1799319088459015],\n",
              " [0.6153830885887146, -0.0033013855572789907],\n",
              " [-0.011275829747319221, 0.07992804050445557],\n",
              " [-0.020693862810730934, 0.009522517211735249],\n",
              " [0.15226352214813232, 0.12404477596282959],\n",
              " [0.012251758947968483, 0.06535176932811737],\n",
              " [0.12833239138126373, 0.07487227767705917],\n",
              " [0.4187729060649872, 0.23370787501335144],\n",
              " [-0.12340056151151657, -0.0005881812539882958],\n",
              " [-0.029366834089159966, 0.016950281336903572],\n",
              " [-0.05064656585454941, -0.09251292794942856],\n",
              " [-0.07232383638620377, -0.06657938659191132],\n",
              " [0.10032973438501358, 0.16767136752605438],\n",
              " [-0.10917497426271439, 0.02750268206000328],\n",
              " [0.09207548946142197, 0.22307348251342773],\n",
              " [-0.07177485525608063, -0.05299648642539978],\n",
              " [0.2938661277294159, -0.09685366600751877],\n",
              " [-0.11338920891284943, -0.09776081144809723],\n",
              " [-0.07646653056144714, -0.021449347957968712],\n",
              " [0.20971408486366272, -0.06993675976991653],\n",
              " [-0.03277796879410744, 0.00940472912043333],\n",
              " [-0.09490091353654861, -0.04519900307059288],\n",
              " [-0.28410041332244873, 0.22845271229743958],\n",
              " [-0.04705136641860008, 0.055574122816324234],\n",
              " [-0.0041229319758713245, 0.02483026310801506],\n",
              " [0.13418443500995636, 0.07413949817419052],\n",
              " [-0.059639424085617065, -0.22598078846931458],\n",
              " [0.21346110105514526, 0.08454982936382294],\n",
              " [0.04626942425966263, 0.10521287471055984],\n",
              " [0.007309459149837494, -0.12868116796016693],\n",
              " [0.140226349234581, 0.028689930215477943],\n",
              " [-0.0016941835638135672, 0.04364469274878502],\n",
              " [0.5690109729766846, 0.08983670175075531],\n",
              " [0.07748385518789291, 0.1501503586769104],\n",
              " [-0.0468885563313961, -0.052250705659389496],\n",
              " [0.00409851735457778, 0.07913918793201447],\n",
              " [0.14085090160369873, 0.13989679515361786],\n",
              " [-0.14683027565479279, 0.08329900354146957],\n",
              " [-0.025278247892856598, 0.002228998811915517],\n",
              " [0.011238706298172474, 0.06409496068954468],\n",
              " [-0.04577672854065895, 0.012029117904603481],\n",
              " [0.07596416771411896, 0.07522647827863693],\n",
              " [0.19462814927101135, 0.14411456882953644],\n",
              " [0.07975324243307114, 0.048079706728458405],\n",
              " [-0.04268199950456619, 0.03090888075530529],\n",
              " [-0.05475441738963127, -0.054453372955322266],\n",
              " [-0.06346315145492554, -0.001743187429383397],\n",
              " [-0.06247274577617645, -0.016133101657032967],\n",
              " [-0.018293526023626328, 0.026685470715165138],\n",
              " [-0.005379571579396725, -0.01298296358436346],\n",
              " [-0.01915614679455757, 0.0311087928712368],\n",
              " [0.05724046006798744, 0.12793909013271332],\n",
              " [-0.06263592094182968, -0.09591933339834213],\n",
              " [0.018729163333773613, 0.06627736240625381],\n",
              " [-0.09310890734195709, 0.04280717670917511],\n",
              " [0.030521100386977196, 0.0861184298992157],\n",
              " [-0.1452898234128952, 0.10917162150144577],\n",
              " [-0.025023339316248894, -0.0916673019528389],\n",
              " [-0.12027856707572937, 0.0006648367270827293],\n",
              " [-0.09839203953742981, -0.07573743909597397]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExIVbPmpMFTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b = sorted(list(set(values[:n])))\n",
        "\n",
        "# d  = torch.stack(b)\n",
        "# d\n",
        "# g = sorted(list(set(attention[:5])))\n"
      ],
      "metadata": {
        "id": "LuFscL68but-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Tensor ops to be continued"
      ],
      "metadata": {
        "id": "dR9-u68lEr07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.zeros((40, 40), dtype=torch.int32)\n"
      ],
      "metadata": {
        "id": "EtKi2zNbrpTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demystifying queries Keys and Values"
      ],
      "metadata": {
        "id": "2jl3BYTNJE1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "words[:20]"
      ],
      "metadata": {
        "id": "Tv1vYyDjKCXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a4df37-13b1-4780-adb9-07ffa84844ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So',\n",
              " 'Ray',\n",
              " 'J',\n",
              " 'went',\n",
              " 'straight',\n",
              " 'to',\n",
              " 'the',\n",
              " 'radio',\n",
              " 'station',\n",
              " 'The',\n",
              " 'very',\n",
              " 'next',\n",
              " 'day,',\n",
              " '\"Hey',\n",
              " 'Fab,',\n",
              " \"I'ma\",\n",
              " 'kill',\n",
              " 'you!\"',\n",
              " 'Lyrics',\n",
              " \"comin'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbpZFK24I_J5",
        "outputId": "bd19cd86-dd10-4004-b0b8-f6baa73bd715"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "<function <lambda> at 0x7f24bb08b670>\n",
            "<function <lambda> at 0x7f24bb08b700>\n",
            "tensor([ 22,  21,  18, 119, 103, 114, 110,  93, 102,  23, 116,  86,  44,   1,\n",
            "         10,  16,  72, 124,  19,  41,  36, 123,  36, 106, 101,   8,  11,  25,\n",
            "        104,  48, 123,  35,  15,  31,  66,  27,  14,  61,  47, 114,  58,  68,\n",
            "        113, 114, 123,  15, 105,  17,  32,  15,  77,  88,  97, 100, 108,  33,\n",
            "         29,  98,  67,  95,  89,  88,  80,  32,  69,  60, 114, 123,  32,  15,\n",
            "         46,  81, 107,  51,  45,  13, 114,  59,  31,  82,  37,  31,  53,  75,\n",
            "         70,  74,  20,  52,  32,  14,  73, 110,  62,  34,  55, 117,  12, 110,\n",
            "         43, 108, 111,  39,  98,  14,  54,  90, 112,  38,  40,   5,  14,  73,\n",
            "        110, 118, 114,  58,   7,  83,  14,  79,  49,  85, 123,  79,  50,  84,\n",
            "          4,  63, 115,  78,  26, 109, 120, 111,  47, 121, 111,  58,  71, 111,\n",
            "         42,  68,   2,  87,  65,  70,  92,   0,   6,  14,  56,  31,  64, 118,\n",
            "        114,  57,  68,  28,  96,  99,  94, 122,   9,  24,  91,   3,  30,  32,\n",
            "         79,   7,  76,  68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "\n",
        "d_k = 2\n",
        "\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "token_emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uftVqQICKX8Q",
        "outputId": "b88b8f2f-33c3-4ec8-c941-4012e87d24d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(125, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_emb(data)\n",
        "input_embeddings.size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUs90cgBNb_w",
        "outputId": "1c5aab0d-91c9-4b9c-b24c-ab0c30070893"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([172, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now, create the query, key and value vectors and calculate the attention scores based on the dot product as the similarity function"
      ],
      "metadata": {
        "id": "DwARBpr-N4sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import sqrt \n",
        "\n",
        "query = key = value = input_embeddings\n",
        "dim_k = key.size(-1)\n",
        "print(dim_k)\n",
        "scores = torch.matmul(query, key.transpose(1,-2))/sqrt(dim_k)\n",
        "print(scores)\n",
        "scores.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTgZhEpjNpnH",
        "outputId": "eaafa7db-1f1b-4a7a-e012-2ebff0a07c9e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensor([[ 0.8805,  1.6122,  1.3415,  ...,  0.1712, -0.1440, -1.1985],\n",
            "        [ 1.6122,  3.6220,  1.5442,  ...,  1.5937, -0.0444, -2.1457],\n",
            "        [ 1.3415,  1.5442,  3.2844,  ..., -1.4811, -0.5176, -1.8920],\n",
            "        ...,\n",
            "        [ 0.1712,  1.5937, -1.4811,  ...,  2.4789,  0.3908, -0.1401],\n",
            "        [-0.1440, -0.0444, -0.5176,  ...,  0.3908,  0.0952,  0.2119],\n",
            "        [-1.1985, -2.1457, -1.8920,  ..., -0.1401,  0.2119,  1.6348]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([172, 172])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Now, lets apply the softmax function"
      ],
      "metadata": {
        "id": "h0Z5L7FGQp4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "weights = F.softmax(scores, dim = -1)\n",
        "weights.sum(dim = -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MwGAFO9Qxg6",
        "outputId": "1c380b9d-ca76-45d5-9582-57a4f23b35eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "        1.0000], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_logits = torch.matmul(weights, value)\n",
        "attention_logits.shape\n",
        "### Now look at the scaled dot product function above"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWhq9Rb0RRPP",
        "outputId": "67bb545f-1c64-4f40-c05a-477eb67dcd94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([172, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#m = decode(scores[:3].tolist())\n",
        "m = scores[:5].tolist()\n",
        "m[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AD1mjs8SLHA",
        "outputId": "857363e9-9188-452f-9934-4f6d804546cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.880527675151825,\n",
              "  1.6121827363967896,\n",
              "  1.3414628505706787,\n",
              "  -0.6853272914886475,\n",
              "  -0.5371031165122986,\n",
              "  -0.07534497231245041,\n",
              "  -0.4273715317249298,\n",
              "  -1.0355830192565918,\n",
              "  0.38905414938926697,\n",
              "  0.1615649312734604,\n",
              "  -0.08832787722349167,\n",
              "  0.454852432012558,\n",
              "  0.42466068267822266,\n",
              "  0.7379029989242554,\n",
              "  1.2833964824676514,\n",
              "  -0.2417803704738617,\n",
              "  0.4343053996562958,\n",
              "  -0.3744153380393982,\n",
              "  1.0852693319320679,\n",
              "  1.1492124795913696,\n",
              "  0.8317149877548218,\n",
              "  0.5078718066215515,\n",
              "  0.8317149877548218,\n",
              "  0.7213720679283142,\n",
              "  -1.6162853240966797,\n",
              "  1.2266061305999756,\n",
              "  0.4563087522983551,\n",
              "  -1.5484733581542969,\n",
              "  0.6256605982780457,\n",
              "  1.0806660652160645,\n",
              "  0.5078718066215515,\n",
              "  0.9754514098167419,\n",
              "  -0.575219452381134,\n",
              "  -0.04656592011451721,\n",
              "  -0.46124306321144104,\n",
              "  0.482714980840683,\n",
              "  -0.007621599826961756,\n",
              "  -0.12223619222640991,\n",
              "  -2.1652355194091797,\n",
              "  -0.07534497231245041,\n",
              "  0.18879733979701996,\n",
              "  -1.1984755992889404,\n",
              "  0.17231516540050507,\n",
              "  -0.07534497231245041,\n",
              "  0.5078718066215515,\n",
              "  -0.575219452381134,\n",
              "  0.6082704663276672,\n",
              "  0.8177943825721741,\n",
              "  -1.6046204566955566,\n",
              "  -0.575219452381134,\n",
              "  0.2031204104423523,\n",
              "  1.337758183479309,\n",
              "  0.24277980625629425,\n",
              "  -0.8531370162963867,\n",
              "  -0.7194668650627136,\n",
              "  0.37265169620513916,\n",
              "  -0.45060107111930847,\n",
              "  0.6188591122627258,\n",
              "  0.73471599817276,\n",
              "  -0.25870028138160706,\n",
              "  0.10439661890268326,\n",
              "  1.337758183479309,\n",
              "  0.4403753876686096,\n",
              "  -1.6046204566955566,\n",
              "  1.063830852508545,\n",
              "  -1.1150509119033813,\n",
              "  -0.07534497231245041,\n",
              "  0.5078718066215515,\n",
              "  -1.6046204566955566,\n",
              "  -0.575219452381134,\n",
              "  -0.016621854156255722,\n",
              "  -0.4998096227645874,\n",
              "  -0.24650809168815613,\n",
              "  0.2945634722709656,\n",
              "  -0.7343708276748657,\n",
              "  0.11289795488119125,\n",
              "  -0.07534497231245041,\n",
              "  1.1259641647338867,\n",
              "  -0.04656592011451721,\n",
              "  1.4210783243179321,\n",
              "  -0.34934544563293457,\n",
              "  -0.04656592011451721,\n",
              "  -0.476396381855011,\n",
              "  0.09128706157207489,\n",
              "  -0.1007452979683876,\n",
              "  -0.43312808871269226,\n",
              "  -1.0641546249389648,\n",
              "  0.8574514389038086,\n",
              "  -1.6046204566955566,\n",
              "  -0.007621599826961756,\n",
              "  -0.5410043597221375,\n",
              "  -0.4273715317249298,\n",
              "  0.10409607738256454,\n",
              "  0.8706990480422974,\n",
              "  -0.25361382961273193,\n",
              "  -0.527722954750061,\n",
              "  1.1883955001831055,\n",
              "  -0.4273715317249298,\n",
              "  -0.26747313141822815,\n",
              "  -0.7194668650627136,\n",
              "  0.6510216593742371,\n",
              "  0.5555424094200134,\n",
              "  0.6188591122627258,\n",
              "  -0.007621599826961756,\n",
              "  -0.7414505481719971,\n",
              "  1.0191147327423096,\n",
              "  0.4924728274345398,\n",
              "  -0.03305666521191597,\n",
              "  -0.6333608031272888,\n",
              "  -1.0397727489471436,\n",
              "  -0.007621599826961756,\n",
              "  -0.5410043597221375,\n",
              "  -0.4273715317249298,\n",
              "  0.9946380853652954,\n",
              "  -0.07534497231245041,\n",
              "  0.18879733979701996,\n",
              "  0.17118175327777863,\n",
              "  0.3782617449760437,\n",
              "  -0.007621599826961756,\n",
              "  1.5267328023910522,\n",
              "  -0.9619168043136597,\n",
              "  0.3994270861148834,\n",
              "  0.5078718066215515,\n",
              "  1.5267328023910522,\n",
              "  0.07398238778114319,\n",
              "  0.13311617076396942,\n",
              "  -0.14284183084964752,\n",
              "  0.6726320385932922,\n",
              "  0.11716482788324356,\n",
              "  0.018099181354045868,\n",
              "  -0.9622510671615601,\n",
              "  0.10909111052751541,\n",
              "  0.27915892004966736,\n",
              "  0.6510216593742371,\n",
              "  -2.1652355194091797,\n",
              "  -0.3414449095726013,\n",
              "  0.6510216593742371,\n",
              "  0.18879733979701996,\n",
              "  0.9370202422142029,\n",
              "  0.6510216593742371,\n",
              "  -1.380117654800415,\n",
              "  -1.1984755992889404,\n",
              "  0.31355687975883484,\n",
              "  0.9514071941375732,\n",
              "  0.5708409547805786,\n",
              "  -0.1007452979683876,\n",
              "  -0.9567033648490906,\n",
              "  0.7298786640167236,\n",
              "  0.3852558732032776,\n",
              "  -0.007621599826961756,\n",
              "  -0.44785717129707336,\n",
              "  -0.04656592011451721,\n",
              "  -0.1929018795490265,\n",
              "  0.9946380853652954,\n",
              "  -0.07534497231245041,\n",
              "  -0.5069014430046082,\n",
              "  -1.1984755992889404,\n",
              "  -0.21208173036575317,\n",
              "  -0.09246286749839783,\n",
              "  1.4699969291687012,\n",
              "  0.34621551632881165,\n",
              "  0.754248857498169,\n",
              "  1.585134744644165,\n",
              "  0.05780409276485443,\n",
              "  -0.6966914534568787,\n",
              "  -0.815542459487915,\n",
              "  -1.0658657550811768,\n",
              "  -1.6046204566955566,\n",
              "  1.5267328023910522,\n",
              "  0.17118175327777863,\n",
              "  -0.14396218955516815,\n",
              "  -1.1984755992889404]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi headed attention\n",
        "\n",
        "Having many heads allows the model to focus on different parts of the sentences. The softmax on one head tends to focus on one aspect of similarity. For example subject verb interaction.\n",
        "\n",
        "### Clean the rest of the Notebook\n",
        "\n"
      ],
      "metadata": {
        "id": "HCXzrQVX7VtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(len(data) - 5, (100,))\n",
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2ufRboyXbZy",
        "outputId": "ef366d92-e3cb-43f9-f777-46e23905b440"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([34, 47, 22, 40, 61, 44, 22, 62,  4,  9,  0, 51,  4, 37, 26, 60, 61, 10,\n",
              "        13, 40, 29,  8, 53, 58, 44, 20, 15, 38, 15, 46,  7, 56,  0, 57, 30, 35,\n",
              "        51, 22, 53, 31, 28, 53, 10, 55, 26,  3, 39, 33, 48, 27, 31,  0, 48,  4,\n",
              "        49, 47,  5, 17, 49, 48, 43, 37, 26, 17,  4, 28, 11, 34, 57, 24, 60,  5,\n",
              "        42, 28, 30, 29, 29, 51, 31, 38, 37,  0, 16, 54, 56, 11, 30, 11, 57, 58,\n",
              "        10, 55, 55, 40, 54, 13, 37, 27, 45, 42])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.stack([data[i:i+5] for i in ix])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krWRaGDVXnGr",
        "outputId": "e3523707-1bbe-459a-c06f-4df2922e7296"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 66,  27,  14,  61,  47],\n",
              "        [ 17,  32,  15,  77,  88],\n",
              "        [ 36, 106, 101,   8,  11],\n",
              "        [ 58,  68, 113, 114, 123],\n",
              "        [ 88,  80,  32,  69,  60],\n",
              "        [123,  15, 105,  17,  32],\n",
              "        [ 36, 106, 101,   8,  11],\n",
              "        [ 80,  32,  69,  60, 114],\n",
              "        [103, 114, 110,  93, 102],\n",
              "        [ 23, 116,  86,  44,   1],\n",
              "        [ 22,  21,  18, 119, 103],\n",
              "        [ 88,  97, 100, 108,  33],\n",
              "        [103, 114, 110,  93, 102],\n",
              "        [ 61,  47, 114,  58,  68],\n",
              "        [ 11,  25, 104,  48, 123],\n",
              "        [ 89,  88,  80,  32,  69],\n",
              "        [ 88,  80,  32,  69,  60],\n",
              "        [116,  86,  44,   1,  10],\n",
              "        [  1,  10,  16,  72, 124],\n",
              "        [ 58,  68, 113, 114, 123],\n",
              "        [ 48, 123,  35,  15,  31],\n",
              "        [102,  23, 116,  86,  44],\n",
              "        [100, 108,  33,  29,  98],\n",
              "        [ 67,  95,  89,  88,  80],\n",
              "        [123,  15, 105,  17,  32],\n",
              "        [ 36, 123,  36, 106, 101],\n",
              "        [ 16,  72, 124,  19,  41],\n",
              "        [ 47, 114,  58,  68, 113],\n",
              "        [ 16,  72, 124,  19,  41],\n",
              "        [105,  17,  32,  15,  77],\n",
              "        [ 93, 102,  23, 116,  86],\n",
              "        [ 29,  98,  67,  95,  89],\n",
              "        [ 22,  21,  18, 119, 103],\n",
              "        [ 98,  67,  95,  89,  88],\n",
              "        [123,  35,  15,  31,  66],\n",
              "        [ 27,  14,  61,  47, 114],\n",
              "        [ 88,  97, 100, 108,  33],\n",
              "        [ 36, 106, 101,   8,  11],\n",
              "        [100, 108,  33,  29,  98],\n",
              "        [ 35,  15,  31,  66,  27],\n",
              "        [104,  48, 123,  35,  15],\n",
              "        [100, 108,  33,  29,  98],\n",
              "        [116,  86,  44,   1,  10],\n",
              "        [ 33,  29,  98,  67,  95],\n",
              "        [ 11,  25, 104,  48, 123],\n",
              "        [119, 103, 114, 110,  93],\n",
              "        [114,  58,  68, 113, 114],\n",
              "        [ 31,  66,  27,  14,  61],\n",
              "        [ 32,  15,  77,  88,  97],\n",
              "        [ 25, 104,  48, 123,  35],\n",
              "        [ 35,  15,  31,  66,  27],\n",
              "        [ 22,  21,  18, 119, 103],\n",
              "        [ 32,  15,  77,  88,  97],\n",
              "        [103, 114, 110,  93, 102],\n",
              "        [ 15,  77,  88,  97, 100],\n",
              "        [ 17,  32,  15,  77,  88],\n",
              "        [114, 110,  93, 102,  23],\n",
              "        [124,  19,  41,  36, 123],\n",
              "        [ 15,  77,  88,  97, 100],\n",
              "        [ 32,  15,  77,  88,  97],\n",
              "        [114, 123,  15, 105,  17],\n",
              "        [ 61,  47, 114,  58,  68],\n",
              "        [ 11,  25, 104,  48, 123],\n",
              "        [124,  19,  41,  36, 123],\n",
              "        [103, 114, 110,  93, 102],\n",
              "        [104,  48, 123,  35,  15],\n",
              "        [ 86,  44,   1,  10,  16],\n",
              "        [ 66,  27,  14,  61,  47],\n",
              "        [ 98,  67,  95,  89,  88],\n",
              "        [101,   8,  11,  25, 104],\n",
              "        [ 89,  88,  80,  32,  69],\n",
              "        [114, 110,  93, 102,  23],\n",
              "        [113, 114, 123,  15, 105],\n",
              "        [104,  48, 123,  35,  15],\n",
              "        [123,  35,  15,  31,  66],\n",
              "        [ 48, 123,  35,  15,  31],\n",
              "        [ 48, 123,  35,  15,  31],\n",
              "        [ 88,  97, 100, 108,  33],\n",
              "        [ 35,  15,  31,  66,  27],\n",
              "        [ 47, 114,  58,  68, 113],\n",
              "        [ 61,  47, 114,  58,  68],\n",
              "        [ 22,  21,  18, 119, 103],\n",
              "        [ 72, 124,  19,  41,  36],\n",
              "        [108,  33,  29,  98,  67],\n",
              "        [ 29,  98,  67,  95,  89],\n",
              "        [ 86,  44,   1,  10,  16],\n",
              "        [123,  35,  15,  31,  66],\n",
              "        [ 86,  44,   1,  10,  16],\n",
              "        [ 98,  67,  95,  89,  88],\n",
              "        [ 67,  95,  89,  88,  80],\n",
              "        [116,  86,  44,   1,  10],\n",
              "        [ 33,  29,  98,  67,  95],\n",
              "        [ 33,  29,  98,  67,  95],\n",
              "        [ 58,  68, 113, 114, 123],\n",
              "        [108,  33,  29,  98,  67],\n",
              "        [  1,  10,  16,  72, 124],\n",
              "        [ 61,  47, 114,  58,  68],\n",
              "        [ 25, 104,  48, 123,  35],\n",
              "        [ 15, 105,  17,  32,  15],\n",
              "        [113, 114, 123,  15, 105]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_emb(x)\n",
        "input_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIllk8Taa_dT",
        "outputId": "2bb27ce9-86b4-4f74-d146-e4785646d4f6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 5, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## A single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "    \n"
      ],
      "metadata": {
        "id": "9LpJFdpo7Da9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "eOQeVVaX-WKd"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attention = MultiHeadAttention(2,2)\n",
        "# multihead_attention\n",
        "\n",
        "output =  multihead_attention(input_embeddings)\n",
        "output"
      ],
      "metadata": {
        "id": "cU528DEaBnYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "size = len(chars) # sequence length\n",
        "print(size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOQPD7BBfPnr",
        "outputId": "e5d567ef-0df6-42ff-8680-20b475872ad0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, input_dim, key_dim, value_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.value_dim = value_dim\n",
        "\n",
        "        self.query_projection = nn.Linear(input_dim, key_dim * num_heads)\n",
        "        self.key_projection = nn.Linear(input_dim, key_dim * num_heads)\n",
        "        self.value_projection = nn.Linear(input_dim, value_dim * num_heads)\n",
        "\n",
        "        self.output_projection = nn.Linear(value_dim * num_heads, input_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Project the query, key and value using linear projections\n",
        "        # and split the resulting tensor into multiple heads\n",
        "        query = self.query_projection(query).view(batch_size, self.num_heads, self.key_dim)\n",
        "        key = self.key_projection(key).view(batch_size, self.num_heads, self.key_dim)\n",
        "        value = self.value_projection(value).view(batch_size, self.num_heads, self.value_dim)\n",
        "\n",
        "        # Compute the scaled dot-product attention\n",
        "        # For efficiency, we compute all the dot products in parallel using matrix multiplication\n",
        "        scores = torch.bmm(query, key.transpose(1, 2)) / (self.key_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Mask out the padded positions with a large negative value\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply the softmax function to get the attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout to the attention weights\n",
        "        attention_weights = nn.Dropout(p=0.1)(attention_weights)\n",
        "\n",
        "        # Multiply the attention weights with the value tensor\n",
        "        context = torch.bmm(attention_weights, value)\n",
        "\n",
        "        # Concatenate the heads and project the resulting tensor\n",
        "        context = context.view(batch_size, -1)\n",
        "        output = self.output_projection(context)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "VKkyf-ghfAE5"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the multi-head attention model with 4 heads, input dimension of 256, key dimension of 64, and value dimension of 64\n",
        "multi_head_attention = MultiHeadAttention(num_heads=4, input_dim=256, key_dim=64, value_dim=64)\n",
        "\n",
        "# Create some input tensors\n",
        "query = torch.randn(size, 256)\n",
        "key = torch.randn(size, 256)\n",
        "value = torch.randn(size, 256)\n",
        "\n",
        "# Apply the multi-head attention\n",
        "output = multi_head_attention(query, key, value)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcEGrb1QfCN_",
        "outputId": "4add9481-6fd4-4b66-dbe0-3ca31cf7c037"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([125, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    }
  ]
}