{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "babygpt with rope\n",
        "\n",
        "cot - step by step thinking"
      ],
      "metadata": {
        "id": "AOBG_ItAO-Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# --- 1. Chain-of-Thought Data Generation ---\n",
        "# This generates examples like: \"12+39=2+9=11,1+3+1=5,ans:51\"\n",
        "def create_cot_addition(a, b):\n",
        "    res = a + b\n",
        "    # Pads to 2 digits for consistency\n",
        "    a_s, b_s = f\"{a:02d}\", f\"{b:02d}\"\n",
        "    a1, a0 = int(a_s[0]), int(a_s[1])\n",
        "    b1, b0 = int(b_s[0]), int(b_s[1])\n",
        "\n",
        "    # Step 1: Ones place\n",
        "    s0 = a0 + b0\n",
        "    carry = s0 // 10\n",
        "    # Step 2: Tens place (including carry)\n",
        "    s1 = a1 + b1 + carry\n",
        "\n",
        "    # Final String\n",
        "    return f\"{a}+{b}={a0}+{b0}={s0},{a1}+{b1}+{carry}={s1},ans:{res}\"\n",
        "\n",
        "# Create a dataset of 5000 examples\n",
        "raw_data = [create_cot_addition(torch.randint(0,100,(1,)).item(),\n",
        "                                torch.randint(0,100,(1,)).item()) for _ in range(5000)]\n",
        "all_text = \"\\n\".join(raw_data)\n",
        "\n",
        "# Tokenizer (Character level)\n",
        "chars = sorted(list(set(all_text)))\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Convert to tensor\n",
        "data = torch.tensor(encode(all_text), dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(chars)\n",
        "block_size = 32 # Long enough for the CoT string\n",
        "batch_size = 32\n",
        "embedded_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 2. RoPE Helper Functions (from your code) ---\n",
        "def precompute_freqs_cis(dim, end, theta=10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(x, freqs_cis):\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs_cis = freqs_cis.view(1, x.shape[1], 1, -1)\n",
        "    x_out = torch.view_as_real(x_complex * freqs_cis).flatten(3)\n",
        "    return x_out.type_as(x)\n",
        "\n",
        "# --- 3. Model Components ---\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wqkv = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "        self.proj = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).view(1,1,block_size,block_size))\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.wqkv(x).split(embedded_dim, dim=2)\n",
        "        q = q.view(B, T, num_heads, C // num_heads)\n",
        "        k = k.view(B, T, num_heads, C // num_heads)\n",
        "        v = v.view(B, T, num_heads, C // num_heads)\n",
        "\n",
        "        q, k = apply_rotary_emb(q, freqs_cis), apply_rotary_emb(k, freqs_cis)\n",
        "\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        y = F.softmax(att, dim=-1) @ v\n",
        "        return self.proj(y.transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = Attention()\n",
        "        self.ffwd = nn.Sequential(nn.Linear(embedded_dim, 4*embedded_dim), nn.GELU(), nn.Linear(4*embedded_dim, embedded_dim))\n",
        "        self.ln1, self.ln2 = nn.LayerNorm(embedded_dim), nn.LayerNorm(embedded_dim)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.sa(self.ln1(x), freqs_cis)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class CoTBabyGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "        self.layers = nn.ModuleList([Block() for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embedded_dim)\n",
        "        self.head = nn.Linear(embedded_dim, vocab_size)\n",
        "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(embedded_dim // num_heads, block_size))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        T = idx.size(1)\n",
        "        x = self.token(idx)\n",
        "        freqs = self.freqs_cis[:T]\n",
        "        for layer in self.layers: x = layer(x, freqs)\n",
        "        logits = self.head(self.ln_f(x))\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1)) if targets is not None else None\n",
        "        return logits, loss\n",
        "\n",
        "# --- 4. Training ---\n",
        "model = CoTBabyGPT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(2000):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "\n",
        "    logits, loss = model(x, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 500 == 0: print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- 5. Evaluation (Generate CoT) ---\n",
        "model.eval()\n",
        "input_str = \"45+27=\"\n",
        "context = torch.tensor([stoi[c] for c in input_str], dtype=torch.long, device=device).unsqueeze(0)\n",
        "for _ in range(25):\n",
        "    logits, _ = model(context[:, -block_size:])\n",
        "    next_token = torch.argmax(F.softmax(logits[:, -1, :], dim=-1), dim=-1, keepdim=True)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "    if itos[next_token.item()] == '\\n': break\n",
        "\n",
        "print(f\"\\nProblem: {input_str}\")\n",
        "print(f\"Model Thought & Result: {decode(context[0].tolist())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-6N7C37M3uX",
        "outputId": "de48d08f-1d54-4e4a-c487-89eafc81e73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 3.1222\n",
            "Step 500, Loss: 0.6015\n",
            "Step 1000, Loss: 0.5488\n",
            "Step 1500, Loss: 0.5653\n",
            "\n",
            "Problem: 45+27=\n",
            "Model Thought & Result: 45+27=5+7=12,4+2+1=7,ans:72\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple babygpt, addition"
      ],
      "metadata": {
        "id": "G72ucuHYO5We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# --- 1. Chain-of-Thought Data Logic ---\n",
        "def create_cot_example(a, b):\n",
        "    # Generates a string like: \"12+39=2+9=11,1+3+1=5,ans:51\"\n",
        "    res = a + b\n",
        "    a_s, b_s = f\"{a:02d}\", f\"{b:02d}\"\n",
        "    a1, a0 = int(a_s[0]), int(a_s[1])\n",
        "    b1, b0 = int(b_s[0]), int(b_s[1])\n",
        "\n",
        "    s0 = a0 + b0\n",
        "    carry = s0 // 10\n",
        "    s1 = a1 + b1 + carry\n",
        "\n",
        "    return f\"{a:02d}+{b:02d}={a0}+{b0}={s0:02d},{a1}+{b1}+{carry}={s1:02d},ans:{res:03d}\\n\"\n",
        "\n",
        "# Create a dataset of 5000 addition strings\n",
        "dataset_text = \"\".join([create_cot_example(torch.randint(0, 100, (1,)).item(),\n",
        "                                          torch.randint(0, 100, (1,)).item()) for _ in range(5000)])\n",
        "\n",
        "# Character-level Tokenizer\n",
        "chars = sorted(list(set(dataset_text)))\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "data = torch.tensor(encode(dataset_text), dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(chars)\n",
        "block_size = 48 # Enough room for the CoT string\n",
        "batch_size = 32\n",
        "embedded_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 2. Model Architecture (Directly from babygpt.py) ---\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embedded_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "        self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.num_heads = num_heads\n",
        "        self.embedded_dim = embedded_dim\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.projection(y)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedded_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedded_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = Attention(embedded_dim, num_heads)\n",
        "        self.feed_forward = FeedForward(embedded_dim)\n",
        "        self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.layer_norm_1(x))\n",
        "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "        return x\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "        self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "        self.layers = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embedded_dim)\n",
        "        self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        tok_emb = self.token(idx)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device)) # Absolute Positional Embedding\n",
        "        x = tok_emb + pos_emb\n",
        "        for layer in self.layers: x = layer(x)\n",
        "        logits = self.ln_head(self.ln_f(x))\n",
        "\n",
        "        if targets is None:\n",
        "            return logits[:, -1, :] # For generation, return last token\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "# --- 3. Training & Evaluation ---\n",
        "model = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for step in range(3000):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "\n",
        "    logits, loss = model(x, y)\n",
        "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "    if step % 500 == 0: print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Generate CoT\n",
        "model.eval()\n",
        "test_input = \"56+29=\"\n",
        "context = torch.tensor(encode(test_input), dtype=torch.long, device=device).unsqueeze(0)\n",
        "for _ in range(35):\n",
        "    logits = model(context[:, -block_size:])\n",
        "    next_token = torch.argmax(F.softmax(logits, dim=-1), dim=-1, keepdim=True)\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "    if itos[next_token.item()] == '\\n': break\n",
        "\n",
        "print(f\"\\nFinal Result:\\n{decode(context[0].tolist())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlI6OtfeNYst",
        "outputId": "b732f7ba-96d9-4d75-ad63-9e19531570d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 2.9826\n",
            "Step 500, Loss: 0.6066\n",
            "Step 1000, Loss: 0.4954\n",
            "Step 1500, Loss: 0.4643\n",
            "Step 2000, Loss: 0.4561\n",
            "Step 2500, Loss: 0.4655\n",
            "\n",
            "Final Result:\n",
            "56+29=6+9=15,5+2+1=08,ans:085\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLOPs per reasoning step without RoPe."
      ],
      "metadata": {
        "id": "r903yae7UVqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. FLOP Calculation Function ---\n",
        "def calculate_flops_per_token(T, d, n_layers, vocab_size):\n",
        "    \"\"\"\n",
        "    Theoretical FLOPs for one forward pass with input length T.\n",
        "    Formula: 2 * Parameters * T + Attention Matmuls (4 * T^2 * d)\n",
        "    Simplified for BabyGPT:\n",
        "    \"\"\"\n",
        "    # Projections per layer: Q,K,V (6d^2*T) + O (2d^2*T) + FFN (16d^2*T) = 24d^2*T\n",
        "    proj_flops = n_layers * (24 * (d**2) * T)\n",
        "    # Attention dot products: 2 * T * T * d (QK^T) + 2 * T * T * d (Softmax*V) = 4 * T^2 * d\n",
        "    attn_flops = n_layers * (4 * (T**2) * d)\n",
        "    # Head: 2 * d * vocab_size\n",
        "    head_flops = 2 * d * vocab_size\n",
        "\n",
        "    return proj_flops + attn_flops + head_flops\n",
        "\n",
        "\n",
        "\n",
        "# --- 4. Reasoning with FLOP Tracking ---\n",
        "model.eval()\n",
        "test_input = \"47+38=\"\n",
        "context = torch.tensor(encode(test_input), dtype=torch.long, device=device).unsqueeze(0)\n",
        "total_step_flops = 0\n",
        "\n",
        "print(f\"\\nProblem: {test_input}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for _ in range(40):\n",
        "    curr_T = context.size(1)\n",
        "\n",
        "    # 1. FLOPs for this specific token\n",
        "    token_flops = calculate_flops_per_token(curr_T, embedded_dim, num_layers, vocab_size)\n",
        "    total_step_flops += token_flops\n",
        "\n",
        "    # 2. Predict next token\n",
        "    logits = model(context[:, -block_size:])\n",
        "    next_token = torch.argmax(F.softmax(logits, dim=-1), dim=-1, keepdim=True)\n",
        "    char = itos[next_token.item()]\n",
        "\n",
        "    print(f\"Token: '{char.strip() if char != ' ' else 'SPACE'}' | FLOPs: {token_flops:,}\")\n",
        "\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "    if char == '\\n': break\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Full CoT: {decode(context[0].tolist()).strip()}\")\n",
        "print(f\"Total FLOPs for reasoning step PFLOPs: {total_step_flops/1e15:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c6GO0rDRuPf",
        "outputId": "563de8b1-3674-4a97-a4e3-85922a0f62e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Problem: 47+38=\n",
            "------------------------------\n",
            "Token: '7' | FLOPs: 2,398,464\n",
            "Token: '+' | FLOPs: 2,804,992\n",
            "Token: '8' | FLOPs: 3,213,568\n",
            "Token: '=' | FLOPs: 3,624,192\n",
            "Token: '1' | FLOPs: 4,036,864\n",
            "Token: '5' | FLOPs: 4,451,584\n",
            "Token: ',' | FLOPs: 4,868,352\n",
            "Token: '4' | FLOPs: 5,287,168\n",
            "Token: '+' | FLOPs: 5,708,032\n",
            "Token: '3' | FLOPs: 6,130,944\n",
            "Token: '+' | FLOPs: 6,555,904\n",
            "Token: '1' | FLOPs: 6,982,912\n",
            "Token: '=' | FLOPs: 7,411,968\n",
            "Token: '0' | FLOPs: 7,843,072\n",
            "Token: '8' | FLOPs: 8,276,224\n",
            "Token: ',' | FLOPs: 8,711,424\n",
            "Token: 'a' | FLOPs: 9,148,672\n",
            "Token: 'n' | FLOPs: 9,587,968\n",
            "Token: 's' | FLOPs: 10,029,312\n",
            "Token: ':' | FLOPs: 10,472,704\n",
            "Token: '0' | FLOPs: 10,918,144\n",
            "Token: '8' | FLOPs: 11,365,632\n",
            "Token: '5' | FLOPs: 11,815,168\n",
            "Token: '' | FLOPs: 12,266,752\n",
            "------------------------------\n",
            "Full CoT: 47+38=7+8=15,4+3+1=08,ans:085\n",
            "Total FLOPs for reasoning step PFLOPs: 1.73910016e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLOPs per reasoning step with RoPE"
      ],
      "metadata": {
        "id": "3swbuqWwUQN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# --- 1. CoT Data Logic (Addition with Scratchpad) ---\n",
        "def create_cot_example(a, b):\n",
        "    res = a + b\n",
        "    a_s, b_s = f\"{a:02d}\", f\"{b:02d}\"\n",
        "    a1, a0, b1, b0 = int(a_s[0]), int(a_s[1]), int(b_s[0]), int(b_s[1])\n",
        "    s0 = a0 + b0\n",
        "    carry = s0 // 10\n",
        "    s1 = a1 + b1 + carry\n",
        "    # Sequence: \"45+27=5+7=12,4+2+1=07,ans:072\\n\"\n",
        "    return f\"{a:02d}+{b:02d}={a0}+{b0}={s0:02d},{a1}+{b1}+{carry}={s1:02d},ans:{res:03d}\\n\"\n",
        "\n",
        "# Generate Dataset\n",
        "dataset_text = \"\".join([create_cot_example(torch.randint(0,100,(1,)).item(),\n",
        "                                          torch.randint(0,100,(1,)).item()) for _ in range(3000)])\n",
        "chars = sorted(list(set(dataset_text)))\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "data = torch.tensor(encode(dataset_text), dtype=torch.long)\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 64\n",
        "embedded_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- 2. RoPE Helpers ---\n",
        "def precompute_freqs_cis(dim, end):\n",
        "    freqs = 1.0 / (10000.0 ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end)\n",
        "    freqs = torch.outer(t, freqs).float()\n",
        "    return torch.polar(torch.ones_like(freqs), freqs)\n",
        "\n",
        "def apply_rotary_emb(x, freqs_cis):\n",
        "    # RoPE transformation adds approx 10-12 FLOPs per element (complex mul + reshapes)\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs_cis = freqs_cis.view(1, x.shape[1], 1, -1)\n",
        "    x_out = torch.view_as_real(x_complex * freqs_cis).flatten(3)\n",
        "    return x_out.type_as(x)\n",
        "\n",
        "# --- 3. FLOP Estimation Function ---\n",
        "def estimate_rope_flops(T, d, n_layers, vocab_size):\n",
        "    \"\"\"\n",
        "    T: current sequence length\n",
        "    d: embedding dimension\n",
        "    n_layers: number of transformer blocks\n",
        "    \"\"\"\n",
        "    # 1. Linear Projections: Q,K,V (6d^2*T) + Out (2d^2*T) + FFN (16d^2*T)\n",
        "    # Total Linear per layer: 24 * d^2 * T\n",
        "    linear_flops = n_layers * (24 * (d**2) * T)\n",
        "\n",
        "    # 2. Attention Matrix: QK^T (2*T^2*d) + Softmax*V (2*T^2*d)\n",
        "    attn_flops = n_layers * (4 * (T**2) * d)\n",
        "\n",
        "    # 3. RoPE: complex multiply for Q and K\n",
        "    # Approx 12 FLOPs per dimension element for rotation\n",
        "    rope_flops = n_layers * (2 * (12 * d * T))\n",
        "\n",
        "    # 4. Final Head\n",
        "    head_flops = 2 * d * vocab_size * T\n",
        "\n",
        "    return linear_flops + attn_flops + rope_flops + head_flops\n",
        "\n",
        "# --- 4. Model (RoPE Implementation) ---\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.wqkv = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "        self.proj = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.wqkv(x).split(embedded_dim, dim=2)\n",
        "        q = q.view(B, T, num_heads, C // num_heads)\n",
        "        k = k.view(B, T, num_heads, C // num_heads)\n",
        "        v = v.view(B, T, num_heads, C // num_heads)\n",
        "\n",
        "        q, k = apply_rotary_emb(q, freqs_cis), apply_rotary_emb(k, freqs_cis)\n",
        "\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        mask = torch.tril(torch.ones(T, T, device=device))\n",
        "        att = att.masked_fill(mask == 0, float('-inf'))\n",
        "        y = F.softmax(att, dim=-1) @ v\n",
        "        return self.proj(y.transpose(1, 2).contiguous().view(B, T, C))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = Attention()\n",
        "        self.ffwd = nn.Sequential(nn.Linear(embedded_dim, 4*embedded_dim), nn.GELU(), nn.Linear(4*embedded_dim, embedded_dim))\n",
        "        self.ln1, self.ln2 = nn.LayerNorm(embedded_dim), nn.LayerNorm(embedded_dim)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        x = x + self.sa(self.ln1(x), freqs_cis)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class RoPEBabyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "        self.layers = nn.ModuleList([Block() for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embedded_dim)\n",
        "        self.head = nn.Linear(embedded_dim, vocab_size)\n",
        "        self.freqs_cis = precompute_freqs_cis(embedded_dim // num_heads, block_size).to(device)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        T = idx.size(1)\n",
        "        x = self.token(idx)\n",
        "        freqs = self.freqs_cis[:T]\n",
        "        for layer in self.layers: x = layer(x, freqs)\n",
        "        logits = self.head(self.ln_f(x))\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1)) if targets is not None else None\n",
        "        return logits, loss\n",
        "\n",
        "# --- 5. Training & Inference with FLOP Tracking ---\n",
        "model = RoPEBabyGPT(len(chars)).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# (Simplified Training)\n",
        "for i in range(1000):\n",
        "    ix = torch.randint(len(data)-block_size, (32,))\n",
        "    xb = torch.stack([data[j:j+block_size] for j in ix]).to(device)\n",
        "    yb = torch.stack([data[j+1:j+block_size+1] for j in ix]).to(device)\n",
        "    _, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "# --- Evaluation with Reasoning FLOPs ---\n",
        "model.eval()\n",
        "input_str = \"47+38=\"\n",
        "context = torch.tensor(encode(input_str), dtype=torch.long, device=device).unsqueeze(0)\n",
        "total_reasoning_step_flops = 0\n",
        "\n",
        "print(f\"Problem: {input_str}\\n\")\n",
        "for _ in range(40):\n",
        "    T = context.size(1)\n",
        "\n",
        "    # Calculate FLOPs for current token generation\n",
        "    token_flops = estimate_rope_flops(T, embedded_dim, num_layers, len(chars))\n",
        "    total_reasoning_step_flops += token_flops\n",
        "\n",
        "    logits, _ = model(context)\n",
        "    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "    char = itos[next_token.item()]\n",
        "\n",
        "    print(f\"Generating '{char.strip() or 'SP'}' | Token FLOPs: {token_flops:,}\")\n",
        "    context = torch.cat((context, next_token), dim=1)\n",
        "    if char == '\\n': break\n",
        "\n",
        "print(f\"\\nFull reasoning chain: {decode(context[0].tolist()).strip()}\")\n",
        "print(f\"TOTAL FLOPs for this reasoning step PFLOPs: {total_reasoning_step_flops/1e15:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dalJE7BPSywO",
        "outputId": "190d3c7f-cfad-4d11-e3ca-5760a4bf7871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem: 47+38=\n",
            "\n",
            "Generating '7' | Token FLOPs: 2,446,848\n",
            "Generating '+' | Token FLOPs: 2,861,824\n",
            "Generating '8' | Token FLOPs: 3,278,848\n",
            "Generating '=' | Token FLOPs: 3,697,920\n",
            "Generating '1' | Token FLOPs: 4,119,040\n",
            "Generating '5' | Token FLOPs: 4,542,208\n",
            "Generating ',' | Token FLOPs: 4,967,424\n",
            "Generating '4' | Token FLOPs: 5,394,688\n",
            "Generating '+' | Token FLOPs: 5,824,000\n",
            "Generating '3' | Token FLOPs: 6,255,360\n",
            "Generating '+' | Token FLOPs: 6,688,768\n",
            "Generating '1' | Token FLOPs: 7,124,224\n",
            "Generating '=' | Token FLOPs: 7,561,728\n",
            "Generating '0' | Token FLOPs: 8,001,280\n",
            "Generating '8' | Token FLOPs: 8,442,880\n",
            "Generating ',' | Token FLOPs: 8,886,528\n",
            "Generating 'a' | Token FLOPs: 9,332,224\n",
            "Generating 'n' | Token FLOPs: 9,779,968\n",
            "Generating 's' | Token FLOPs: 10,229,760\n",
            "Generating ':' | Token FLOPs: 10,681,600\n",
            "Generating '0' | Token FLOPs: 11,135,488\n",
            "Generating '8' | Token FLOPs: 11,591,424\n",
            "Generating '5' | Token FLOPs: 12,049,408\n",
            "Generating 'SP' | Token FLOPs: 12,509,440\n",
            "\n",
            "Full reasoning chain: 47+38=7+8=15,4+3+1=08,ans:085\n",
            "TOTAL FLOPs for this reasoning step PFLOPs: 1.7740288e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Without RoPE:- 1.73910016e-07.\n",
        " With RoPE:- 1.7740288e-07.\n",
        "\n",
        " Around 2 % increase using RopE."
      ],
      "metadata": {
        "id": "GOzCPXXsgSqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal BabyGPT trainer"
      ],
      "metadata": {
        "id": "RXXJx_m75nhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # these are default GPT-2 hyperparameters\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    bias :bool = False\n",
        "\n",
        "\n",
        "### other hyperparametres\n",
        "batch_size = 64\n",
        "max_iters = 1000\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "words = open(r\"/content/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "\n",
        "\n",
        "## train and split the data\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+ config.block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+ config.block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "### from pytorch GPT tutorial\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "\n",
        "    self.atten = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "    self.projection = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "    self.attention_weights = None # Added to store attention weights\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.n_embd, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    self.attention_weights = att # Stored attention weights\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y\n",
        "\n",
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),\n",
        "    nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "### A simple Transformer Block\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BabyGPTmodel, self).__init__()\n",
        "\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "\n",
        "        self.config = config\n",
        "        self.token = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Transformer(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps = 1e-12) # final layer norm\n",
        "        self.lnum_heads = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        ## init all weights\n",
        "        ## from karpathy\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "          if pn.endswith('projection.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token(idx)\n",
        "        position_ids = torch.arange(0, T, dtype = torch.long, device = device).unsqueeze(0)\n",
        "        pos_emb =  self.positional_embeddings(position_ids)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "          x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lnum_heads(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    ## from karpathy's youtube videos.\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -config.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "config = GPTConfig(\n",
        "    block_size = 20, # Increased block_size to accommodate 'The quick brown fox' (19 chars)\n",
        "    vocab_size = len(chars),\n",
        "    n_head = 4,\n",
        "    n_layer = 4,\n",
        "    n_embd = 16)\n",
        "\n",
        "model = BabyGPTmodel(config)\n",
        "\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NZ5EzgR9T8S",
        "outputId": "d754c456-4c4d-4ae2-c5f5-55b2def6a841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 15668\n",
            "step 0: train loss 4.4338, val loss 4.4336\n",
            "step 500: train loss 2.9850, val loss 3.0020\n",
            "step 999: train loss 2.5268, val loss 2.5342\n",
            "\n",
            "   is t sas s mong.\n",
            "  Tes fetdc milaW  mo tot mr  hudF?\n",
            "Q`ir NLO BSW. STE - fishg re rnstdar\n",
            "   (ufo6D!\n",
            "ITh? ISARETFL, OJRAd TA fetop mprh st. Arase n dand.\n",
            " ea-\n",
            " sotils   oud._thsiyhamanrr s cosad tlreond\n",
            "   h res  oud inabu! th,r,  tBohowis  au  I a, 7\n",
            "  pee ;Alv pin[Enb>,  Hdseey E. Lurl  hecomidla ixt edornb t bill livmerou, k, Agpoohemealo th tk asane y ndanal2\n",
            "y   RLRR LTTA, SCaterrgelnlzef<<NRW8 8 P AMjo6LOOTT.\n",
            " > I nrar   The      ngoshroavir etheA n3role we,\n",
            ",   A-rm. 2      N    I ithe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention inspection\n",
        "\n",
        "Attention heads are the primary way information moves between tokens. A classic mechanistic analysis is to look for specific types of heads, like Induction Heads.\n",
        "\n",
        "Hypothesis: Some heads are responsible for copying past information. If the sequence is A B ... A, the head should attend to the B after the first A to predict the next token is B.\n",
        "\n",
        "Action: Extract the attention weights (self.attention_weights) from Attention class and plot them."
      ],
      "metadata": {
        "id": "LQOtGnGY7vq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_attention(model, text, tokenizer_encode, tokenizer_decode):\n",
        "    model.eval()\n",
        "    # Move the input tensor to the same device as the model\n",
        "    idx = torch.tensor(tokenizer_encode(text), dtype=torch.long, device=next(model.parameters()).device).unsqueeze(0)\n",
        "\n",
        "    # Forward pass to populate attention_weights in the layers\n",
        "    with torch.no_grad():\n",
        "        model(idx)\n",
        "\n",
        "    # Visualize Head 0 of Layer 0 (Adjust indices as needed)\n",
        "    # We assume you stored weights in self.layers1[i].attention.attention_weights\n",
        "    # You might need to modify the Transformer class to expose the Attention module\n",
        "\n",
        "    # Accessing the first layer's attention weights\n",
        "    # Note: In your code, layers are in a ModuleList called layers1\n",
        "    layer_idx = 0\n",
        "    att_matrix = model.blocks[layer_idx].attention.attention_weights[0, 0].cpu().numpy() # Batch 0, Head 0\n",
        "\n",
        "    tokens = [tokenizer_decode([i]) for i in idx[0].tolist()]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(att_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "    plt.title(f\"Layer {layer_idx} Head 0 Attention Pattern\")\n",
        "    plt.xlabel(\"Key (Source)\")\n",
        "    plt.ylabel(\"Query (Destination)\")\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "visualize_attention(model, \"The quick\", encode, decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "JpJBioUC74Oq",
        "outputId": "5550d917-d98e-4ba9-e571-6e335b55c533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logit Lens\n",
        "\n",
        "The \"Residual Stream\" is the main highway of information. In a Transformer, layers add to this stream: $x_{l+1} = x_l + Attention(x_l) + MLP(x_l)$.The Logit Lens technique applies the final classification layer (ln_f and ln_head) to the output of intermediate layers. This allows to see what the model \"believes\" the next token is at layer 1, layer 2, etc.\n",
        "\n",
        "Goal: See if the model \"figures out\" the answer early or only at the very end.\n",
        "\n",
        "Action: Hook the residual stream after every block."
      ],
      "metadata": {
        "id": "UzZFfkyS-qFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit_lens(model, idx):\n",
        "    # This requires modifying the forward pass or using hooks to capture 'x' after each layer\n",
        "    hiddens = []\n",
        "\n",
        "    # Manual forward pass to capture intermediates\n",
        "    tok_emb = model.token(idx)\n",
        "    pos_emb = model.positional_embeddings(torch.arange(idx.size(1), device=idx.device))\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    for layer in model.blocks:\n",
        "        x = layer(x)\n",
        "        hiddens.append(x) # Capture state after this layer\n",
        "\n",
        "    # Apply the final decoding to each hidden state\n",
        "    for i, h in enumerate(hiddens):\n",
        "        # Apply final LayerNorm and Head\n",
        "        logits = model.lnum_heads(model.ln_f(h))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get top prediction for the last token\n",
        "        top_val, top_idx = torch.topk(probs[0, -1], k=3)\n",
        "        print(f\"Layer {i} thinks next token is: {[integer2string[t.item()] for t in top_idx]}\")\n",
        "\n",
        "# Usage\n",
        "logit_lens(model, torch.tensor(encode(\"The quick brown\"), dtype=torch.long).unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx7s3kzX-31C",
        "outputId": "444de7a5-5176-4e66-f2a4-5b3357202ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0 thinks next token is: [' ', 'd', 'e']\n",
            "Layer 1 thinks next token is: [' ', 'd', 'e']\n",
            "Layer 2 thinks next token is: ['d', ' ', 'e']\n",
            "Layer 3 thinks next token is: [' ', 'd', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Memory"
      ],
      "metadata": {
        "id": "iQkWm-jasdXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added the Memory Classes: ContextWindowMemory (Short-term) and SimpleVectorStore (Long-term) are now part of the script.\n",
        "\n",
        "Added a generate function: original model didn't have a way to generate text, only train. I added a standard autoregressive generation method to the BabyGPTmodel class.\n",
        "\n",
        "Added a Chat Loop: Instead of just training and stopping, the script now enters an interactive chat mode where it uses the memories to construct prompts."
      ],
      "metadata": {
        "id": "bt7CK4kM5TjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import re\n",
        "\n",
        "# ==========================================\n",
        "# 1. MEMORY COMPONENTS\n",
        "# ==========================================\n",
        "\n",
        "class ContextWindowMemory:\n",
        "    \"\"\"Short-Term Memory: Keeps the last N tokens of conversation.\"\"\"\n",
        "    def __init__(self, max_tokens=200):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.history = []\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.history.append({'role': role, 'content': content})\n",
        "        # Prune oldest messages if we exceed a \"mock\" token limit\n",
        "        # (Using char length/4 as a rough token approximation)\n",
        "        while sum(len(m['content']) for m in self.history)/4 > self.max_tokens and len(self.history) > 1:\n",
        "            self.history.pop(0)\n",
        "\n",
        "    def get_context(self):\n",
        "        return \"\\n\".join([f\"[{msg['role'].upper()}]: {msg['content']}\" for msg in self.history])\n",
        "\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"Long-Term Memory: Retreives relevant past info.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.memory_chunks = []\n",
        "\n",
        "    def store_memory(self, text_chunk):\n",
        "        self.memory_chunks.append(text_chunk)\n",
        "\n",
        "    def retrieve_context(self, query, top_k=2):\n",
        "        if not self.memory_chunks: return []\n",
        "\n",
        "        # MOCK SIMILARITY: Counts shared words between query and memory\n",
        "        # In production, use cosine similarity with real embeddings\n",
        "        query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "        scores = []\n",
        "        for chunk in self.memory_chunks:\n",
        "            chunk_words = set(re.findall(r'\\b\\w+\\b', chunk.lower()))\n",
        "            score = len(query_words.intersection(chunk_words))\n",
        "            scores.append((score, chunk))\n",
        "\n",
        "        scores.sort(key=lambda x: x[0], reverse=True)\n",
        "        return [chunk for score, chunk in scores[:top_k] if score > 0]\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA & TOKENIZATION setup\n",
        "# ==========================================\n",
        "\n",
        "# --- REPLACEMENT FOR EMINEM FILE (For immediate runnability) ---\n",
        "# We use a character-level tokenizer here so the model can output any text.\n",
        "# If you want your word-level tokenizer back, uncomment your original lines.\n",
        "text_data = open(r\"/content/ALL_eminem.txt\", 'r', encoding='utf-8').read().split()\n",
        "chars = sorted(list(set(text_data)))\n",
        "vocab_size = len(chars) + 1 # +1 for a special <UNK> token if needed\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "\n",
        "# Robust encode/decode to handle unknown chars\n",
        "def encode(s):\n",
        "    return [string2integer.get(c, 0) for c in s] # Returns 0 if char not found\n",
        "def decode(l):\n",
        "    return ''.join([integer2string.get(i, '') for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text_data), dtype=torch.long)\n",
        "\n",
        "# Global Config\n",
        "block_size = 16   # Context length for training\n",
        "batch_size = 4\n",
        "embedded_dim = 32\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "dropout = 0.1\n",
        "\n",
        "# ==========================================\n",
        "# 3. MODEL ARCHITECTURE ( Original Code)\n",
        "# ==========================================\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Attention, self).__init__()\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    # Ensure mask handles variable sequence lengths up to block_size\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "\n",
        "    y = att @ v\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "    y = self.projection(y)\n",
        "    return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_layers)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12)\n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    b, t = idx.size()\n",
        "\n",
        "    # Crop context if it exceeds block_size (crucial for generation loop)\n",
        "    if t > block_size:\n",
        "        idx = idx[:, -block_size:]\n",
        "        t = block_size\n",
        "\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype=torch.long, device=idx.device)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    for layer in self.layers1:\n",
        "        x = layer(x)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    # If we are just generating, we usually care about the last token\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  # --- NEW: GENERATION FUNCTION ---\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Idx is (B, T) array of indices in the current context\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        # 1. Get predictions\n",
        "        logits, _ = self(idx)\n",
        "        # 2. Softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # 3. Sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        # 4. Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "# ==========================================\n",
        "# 4. MAIN EXECUTION (Chat Loop)\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Initialize Model\n",
        "    model = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(\"--- Training briefly on Eminem data... ---\")\n",
        "    model.train()\n",
        "    # Very generic training loop just to make the model \"runnable\"\n",
        "    for i in range(5000):\n",
        "        # Random chunk sampling\n",
        "        if len(data) <= block_size: break\n",
        "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "        x_batch = torch.stack([data[i:i+block_size] for i in ix])\n",
        "        y_batch = torch.stack([data[i+block_size] for i in ix]) # Target is next token\n",
        "\n",
        "        logits, loss = model(x_batch, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"--- Training Done. Starting Memory Chat... ---\")\n",
        "\n",
        "    # 2. Initialize Memory Systems\n",
        "    short_term_mem = ContextWindowMemory(max_tokens=100)\n",
        "    long_term_mem = SimpleVectorStore()\n",
        "\n",
        "    # Pre-seed long-term memory with facts\n",
        "    long_term_mem.store_memory(\"BabyGPT is created using PyTorch.\")\n",
        "    long_term_mem.store_memory(\"The user is testing a memory implementation.\")\n",
        "    long_term_mem.store_memory(\"Eminem is a famous rapper.\")\n",
        "\n",
        "    # 3. Interactive Loop\n",
        "    model.eval()\n",
        "    print(\"\\nType 'exit' to quit. Ask about Eminem or BabyGPT!\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in ['exit', 'quit']: break\n",
        "\n",
        "        # A. Retrieval (Long-Term)\n",
        "        retrieved_facts = long_term_mem.retrieve_context(user_input)\n",
        "        context_str = \" \".join(retrieved_facts)\n",
        "\n",
        "        # B. Construct Prompt (System + Retrieved + Short-Term History + Current)\n",
        "        # Note: Since this is a baby model trained on random chars, the OUTPUT\n",
        "        # will likely be gibberish, but the ARCHITECTURE is correct.\n",
        "\n",
        "        # We simulate the prompt construction:\n",
        "        full_prompt_text = f\"{context_str} {short_term_mem.get_context()} [USER]: {user_input}\"\n",
        "\n",
        "        # Encode prompt\n",
        "        input_ids = torch.tensor(encode(user_input), dtype=torch.long).unsqueeze(0) # (1, T)\n",
        "\n",
        "        # C. Generate\n",
        "        # We generate 10 new tokens\n",
        "        generated_ids = model.generate(input_ids, max_new_tokens=10)\n",
        "        response_text = decode(generated_ids[0].tolist())\n",
        "\n",
        "        # Extract just the new part (simple slicing)\n",
        "        new_content = response_text[len(user_input):]\n",
        "\n",
        "        print(f\"BabyGPT (w/ Memory): {new_content}\")\n",
        "\n",
        "        # D. Update Short-Term Memory\n",
        "        short_term_mem.add_message(\"user\", user_input)\n",
        "        short_term_mem.add_message(\"ai\", new_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsBFcd5dq-ec",
        "outputId": "e3b6d693-5948-49ee-e636-3f13ceb5c32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training briefly on Eminem data... ---\n",
            "--- Training Done. Starting Memory Chat... ---\n",
            "\n",
            "Type 'exit' to quit. Ask about Eminem or BabyGPT!\n",
            "\n",
            "You: rap god\n",
            "BabyGPT (w/ Memory): Nope,chartinglevelssumorapperawkwardlinkin'timebailedwomen\n",
            "\n",
            "You: superman\n",
            "BabyGPT (w/ Memory): beautifullyTheyBoughtzoobe-ditchingThinkinmedsagain.ravishchildish-like\n",
            "\n",
            "You: eminem sucks\n",
            "BabyGPT (w/ Memory): missSofuckerreargottaGiveworldspitethegun\n",
            "\n",
            "You: mockingbird\n",
            "BabyGPT (w/ Memory): eachscrewedfingerfuckSetProfitin'SonjustMyclass\n",
            "\n",
            "You: dr dre\n",
            "BabyGPT (w/ Memory): madyoueasy?amWepicturesyearyeah)youSomethin'\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}