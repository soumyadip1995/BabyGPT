{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT7gJGCXJg2Dnx0hVD8bJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/language-models/blob/main/Notebook/BabyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "from math import sqrt\n",
        "import torch.nn as nn \n",
        "\n",
        "\n",
        "\n",
        "words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "# words[:20]\n",
        "\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "# print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "# print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "# print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "# print(data)\n",
        "# data.size()\n",
        "\n",
        "## block_size and batch size has been changed from 64 and 512 to 32 and 128\n",
        "block_size = 16\n",
        "batch_size = 32\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "## hidden dimensionality has been changed from 512 to 128.\n",
        "\n",
        "vocab_size = len(chars)\n",
        "d_k = 32\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "\n",
        "\n",
        "x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "input_embeds = token_emb(x)\n",
        "# input_embeds.size()\n",
        "\n",
        "\n",
        "def scaled_dot_product(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)\n",
        "\n",
        "key = input_embeds\n",
        "query = input_embeds\n",
        "value = input_embeds\n",
        "\n",
        "# sdp = scaled_dot_product(query, key, value)\n",
        "# print(sdp.size())\n",
        "\n",
        "### Multi headed attention\n",
        "\n",
        "\"\"\"Having many heads allows the model to focus on different parts of the sentences. \n",
        "The softmax on one head tends to focus on one aspect of similarity. For example subject verb interaction.\"\"\"\n",
        "## A single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# multihead_attention = MultiHeadAttention(128, 8)\n",
        "# # multihead_attention\n",
        "\n",
        "# attention_outputs =  multihead_attention(input_embeds)\n",
        "# # print(attention_outputs.size())\n",
        "\n",
        "\n",
        "# from karpathy , partially\n",
        "dropout = 0.2\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "### A simple Transformer Block    \n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mOdus5D9uFKq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, d_k):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, d_k)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, d_k)\n",
        "    self.layers1 = nn.ModuleList([TransformerBlock(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('q.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(x)\n",
        "    position_ids = torch.arange(x.size(-1), dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "      x = self.ln_f(x)\n",
        "    logits = self.ln_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "uiePG-vfm8Hx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## number of parameters: 117,187\n",
        "num_layers = 3\n",
        "gpt = BabyGPTmodel(num_layers, vocab_size, block_size, 32, 8, 32)\n",
        "# d = gpt(x)\n",
        "# d.size()"
      ],
      "metadata": {
        "id": "Qk_1wRLcHSP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Attention, self).__init__()\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "W1xqB6Arc73u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(3, 3, 16)\n",
        "\n",
        "# att = Attention(16, 8)\n",
        "# tt = att(x)\n",
        "# tt.size()"
      ],
      "metadata": {
        "id": "8fgGPCd9Yxbl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "X7zzfrdObFZL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed_forward = FeedForward(embedded_dim = 16)\n",
        "# # feed_forward\n",
        "# ff_outputs = feed_forward(tt)\n",
        "# ff_outputs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQlwF5bbYAz",
        "outputId": "976c7577-da62-4c91-b5aa-8ab53c1f3648"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "39qXw2CmbpSO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btt = Transformer(16, 8)\n",
        "# to = btt(x)\n",
        "# print(to.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKLZmauscJiK",
        "outputId": "08fd3c82-8e42-43fc-dba0-313703edda55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits"
      ],
      "metadata": {
        "id": "XuvEumpqcdwU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btt = Transformer(16, 8)\n",
        "# to = btt(x)\n",
        "# print(to.size())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9iJ54aRgZDI",
        "outputId": "31e50bd1-d388-4f84-d031-198559eb9f74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size =  4\n",
        "# block_size = 4\n",
        "# embedded_dim = 16\n",
        "# num_heads = 4\n",
        "# num_layers = 4\n",
        "# gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "# # number of parameters: 13,315"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br5cx40HjsjY",
        "outputId": "27335a6c-d765-4ab6-c701-751caf51eaa9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 13315\n",
            "number of parameters: 13315\n",
            "number of parameters: 13315\n",
            "number of parameters: 13315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq = list(map(int, \"1111011110111101101\"))\n",
        "# seq"
      ],
      "metadata": {
        "id": "J2R76Xsfnv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## from karpathy\n",
        "# # convert the sequence to a tensor holding all the individual examples in that sequence\n",
        "# X, Y = [], []\n",
        "# # iterate over the sequence and grab every consecutive 3 bits\n",
        "# # the correct label for what's next is the next bit at each position\n",
        "# for i in range(len(seq) - block_size):\n",
        "#     X.append(seq[i:i+block_size])\n",
        "#     Y.append(seq[i+block_size])\n",
        "#     print(f\"example {i+1:2d}: {X[-1]} --> {Y[-1]}\")\n",
        "# X = torch.tensor(X,  dtype=torch.long)\n",
        "# Y = torch.tensor(Y,  dtype=torch.long)\n",
        "# # print(X.shape, Y.shape)\n",
        "# # print(X.size(), Y.size())\n",
        "\n",
        "# print(X, Y)"
      ],
      "metadata": {
        "id": "2MHJ-fZ0n5c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "id": "fdEuygOMouVa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(100):\n",
        "#     logits = gpt(X)\n",
        "#     loss = F.cross_entropy(logits, Y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     optimizer.zero_grad()\n",
        "#     print(i, loss.item())"
      ],
      "metadata": {
        "id": "--b_jDhDoykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n"
      ],
      "metadata": {
        "id": "_19368fGf9zZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+block_size] for i in ix])\n",
        "print((x, y))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwhLy0v-l2I7",
        "outputId": "063372ca-7c62-4b93-85ba-be4e3a04ceca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 44, 373,  29, 194],\n",
            "        [253, 228, 321, 387],\n",
            "        [ 98, 193, 475, 118],\n",
            "        [451, 285, 238,  20],\n",
            "        [ 52, 185, 470, 358],\n",
            "        [318, 308, 105,  97],\n",
            "        [281, 101,  37, 418],\n",
            "        [ 29, 467, 244, 312],\n",
            "        [470,   4, 470, 403],\n",
            "        [ 75, 256, 407, 389],\n",
            "        [445, 226,  29, 256],\n",
            "        [339, 136, 436, 295],\n",
            "        [295, 131, 295, 323],\n",
            "        [308, 105,  97,  84],\n",
            "        [106, 382, 288,  91],\n",
            "        [206, 249,   4,  29]]), tensor([234, 110,  29, 470, 241,  84, 133,  76, 244, 122, 470, 459,  91,  65,\n",
            "        393, 399]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "## number of parameters: 28,990"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KO184DtqEFe",
        "outputId": "9bf059d0-bdcb-4e97-ef0f-6211d9a65023"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 28990\n",
            "number of parameters: 28990\n",
            "number of parameters: 28990\n",
            "number of parameters: 28990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]"
      ],
      "metadata": {
        "id": "2GxxoDMvgQ8s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d = torch.tensor(train_data.shape, dtype = torch.long)\n",
        "# p = torch.tensor(val_data.shape, dtype = torch.long)\n",
        "# p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9O0JKqhsmN",
        "outputId": "9caff398-cc88-4b7f-a6eb-75aacfbf8ee6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([135])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # batch_size = 32\n",
        "# input = torch.randn(d.shape, embedded_dim)\n",
        "# target = torch.tensor(p.shape)\n",
        "# print(input.shape, target.shape)"
      ],
      "metadata": {
        "id": "2mOtGcYLgqzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "id": "-XYEFDhqqk2-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    logits = gpt(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(i, loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG2r1I4PjCFj",
        "outputId": "e07f9156-aee2-49fe-daab-115bc58db106"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 6.1647467613220215\n",
            "1 6.0855512619018555\n",
            "2 6.0388503074646\n",
            "3 6.0027546882629395\n",
            "4 5.960081577301025\n",
            "5 5.9288225173950195\n",
            "6 5.896416187286377\n",
            "7 5.863935470581055\n",
            "8 5.842287540435791\n",
            "9 5.814393520355225\n",
            "10 5.78857421875\n",
            "11 5.760965347290039\n",
            "12 5.739848613739014\n",
            "13 5.711612701416016\n",
            "14 5.6829304695129395\n",
            "15 5.663541793823242\n",
            "16 5.634166717529297\n",
            "17 5.610829830169678\n",
            "18 5.582950592041016\n",
            "19 5.55332088470459\n",
            "20 5.525705337524414\n",
            "21 5.5001301765441895\n",
            "22 5.474945545196533\n",
            "23 5.444350719451904\n",
            "24 5.414790153503418\n",
            "25 5.38487434387207\n",
            "26 5.357189655303955\n",
            "27 5.328928470611572\n",
            "28 5.299066066741943\n",
            "29 5.267841815948486\n",
            "30 5.234304428100586\n",
            "31 5.203612327575684\n",
            "32 5.1714630126953125\n",
            "33 5.140346527099609\n",
            "34 5.107305526733398\n",
            "35 5.0787858963012695\n",
            "36 5.044053554534912\n",
            "37 5.011567115783691\n",
            "38 4.977425575256348\n",
            "39 4.942967891693115\n",
            "40 4.910092830657959\n",
            "41 4.8771162033081055\n",
            "42 4.8441596031188965\n",
            "43 4.807589054107666\n",
            "44 4.772795677185059\n",
            "45 4.738954544067383\n",
            "46 4.704535961151123\n",
            "47 4.668580055236816\n",
            "48 4.631651401519775\n",
            "49 4.5983567237854\n",
            "50 4.561915397644043\n",
            "51 4.526247024536133\n",
            "52 4.49081563949585\n",
            "53 4.455860137939453\n",
            "54 4.419033050537109\n",
            "55 4.383411884307861\n",
            "56 4.348185062408447\n",
            "57 4.313320636749268\n",
            "58 4.279051303863525\n",
            "59 4.241971492767334\n",
            "60 4.207936763763428\n",
            "61 4.170825004577637\n",
            "62 4.135182857513428\n",
            "63 4.099650859832764\n",
            "64 4.0649871826171875\n",
            "65 4.0313005447387695\n",
            "66 3.998537063598633\n",
            "67 3.9609365463256836\n",
            "68 3.9263622760772705\n",
            "69 3.893320083618164\n",
            "70 3.85738205909729\n",
            "71 3.823812246322632\n",
            "72 3.7904770374298096\n",
            "73 3.7592153549194336\n",
            "74 3.7255289554595947\n",
            "75 3.691765308380127\n",
            "76 3.6598546504974365\n",
            "77 3.6277151107788086\n",
            "78 3.597505807876587\n",
            "79 3.5648574829101562\n",
            "80 3.533220052719116\n",
            "81 3.5031309127807617\n",
            "82 3.471472978591919\n",
            "83 3.441662549972534\n",
            "84 3.412412405014038\n",
            "85 3.3821640014648438\n",
            "86 3.353949546813965\n",
            "87 3.3253519535064697\n",
            "88 3.297471284866333\n",
            "89 3.269744634628296\n",
            "90 3.241762161254883\n",
            "91 3.2132294178009033\n",
            "92 3.186490774154663\n",
            "93 3.1614291667938232\n",
            "94 3.133168935775757\n",
            "95 3.1073033809661865\n",
            "96 3.080878257751465\n",
            "97 3.0560524463653564\n",
            "98 3.0311453342437744\n",
            "99 3.0068609714508057\n",
            "100 2.982682228088379\n",
            "101 2.9571192264556885\n",
            "102 2.934206485748291\n",
            "103 2.9089648723602295\n",
            "104 2.8859996795654297\n",
            "105 2.8629517555236816\n",
            "106 2.8420193195343018\n",
            "107 2.8170359134674072\n",
            "108 2.7949469089508057\n",
            "109 2.773223400115967\n",
            "110 2.751453161239624\n",
            "111 2.7293589115142822\n",
            "112 2.7076730728149414\n",
            "113 2.6858763694763184\n",
            "114 2.665933609008789\n",
            "115 2.6451961994171143\n",
            "116 2.623549222946167\n",
            "117 2.604180097579956\n",
            "118 2.582406759262085\n",
            "119 2.562926769256592\n",
            "120 2.5435049533843994\n",
            "121 2.522728681564331\n",
            "122 2.5042097568511963\n",
            "123 2.4808690547943115\n",
            "124 2.462116003036499\n",
            "125 2.44124436378479\n",
            "126 2.4214892387390137\n",
            "127 2.405979871749878\n",
            "128 2.3862836360931396\n",
            "129 2.3655829429626465\n",
            "130 2.346677541732788\n",
            "131 2.3282723426818848\n",
            "132 2.308992862701416\n",
            "133 2.2893919944763184\n",
            "134 2.272453546524048\n",
            "135 2.2528915405273438\n",
            "136 2.2356648445129395\n",
            "137 2.214404582977295\n",
            "138 2.19730544090271\n",
            "139 2.1784934997558594\n",
            "140 2.160841941833496\n",
            "141 2.1422572135925293\n",
            "142 2.122877836227417\n",
            "143 2.104003429412842\n",
            "144 2.0883076190948486\n",
            "145 2.0696604251861572\n",
            "146 2.051041603088379\n",
            "147 2.0343708992004395\n",
            "148 2.0159828662872314\n",
            "149 2.000575542449951\n",
            "150 1.9820241928100586\n",
            "151 1.9636034965515137\n",
            "152 1.9455478191375732\n",
            "153 1.930107831954956\n",
            "154 1.9104548692703247\n",
            "155 1.8943448066711426\n",
            "156 1.8768985271453857\n",
            "157 1.860037088394165\n",
            "158 1.8422900438308716\n",
            "159 1.826647400856018\n",
            "160 1.8097656965255737\n",
            "161 1.792459487915039\n",
            "162 1.7766437530517578\n",
            "163 1.7583421468734741\n",
            "164 1.7427924871444702\n",
            "165 1.7249882221221924\n",
            "166 1.709768295288086\n",
            "167 1.6917134523391724\n",
            "168 1.676938533782959\n",
            "169 1.6607671976089478\n",
            "170 1.6436352729797363\n",
            "171 1.628566861152649\n",
            "172 1.6123466491699219\n",
            "173 1.596995234489441\n",
            "174 1.5804277658462524\n",
            "175 1.5651745796203613\n",
            "176 1.5496480464935303\n",
            "177 1.5341050624847412\n",
            "178 1.5183579921722412\n",
            "179 1.5027252435684204\n",
            "180 1.4873006343841553\n",
            "181 1.47365140914917\n",
            "182 1.4581767320632935\n",
            "183 1.4425183534622192\n",
            "184 1.4275139570236206\n",
            "185 1.4130589962005615\n",
            "186 1.3981989622116089\n",
            "187 1.382454514503479\n",
            "188 1.3692891597747803\n",
            "189 1.3548063039779663\n",
            "190 1.3381481170654297\n",
            "191 1.3265002965927124\n",
            "192 1.3108912706375122\n",
            "193 1.2966557741165161\n",
            "194 1.282995581626892\n",
            "195 1.2695460319519043\n",
            "196 1.2546733617782593\n",
            "197 1.2410454750061035\n",
            "198 1.227942705154419\n",
            "199 1.2149126529693604\n",
            "200 1.2015656232833862\n",
            "201 1.1879786252975464\n",
            "202 1.1737056970596313\n",
            "203 1.1616175174713135\n",
            "204 1.1478426456451416\n",
            "205 1.1351091861724854\n",
            "206 1.1222693920135498\n",
            "207 1.1105084419250488\n",
            "208 1.0975770950317383\n",
            "209 1.0839734077453613\n",
            "210 1.072521686553955\n",
            "211 1.0593955516815186\n",
            "212 1.0475823879241943\n",
            "213 1.0367777347564697\n",
            "214 1.0228691101074219\n",
            "215 1.011831283569336\n",
            "216 1.0006952285766602\n",
            "217 0.9890559911727905\n",
            "218 0.976599395275116\n",
            "219 0.9660010933876038\n",
            "220 0.9535384774208069\n",
            "221 0.9426912665367126\n",
            "222 0.9325209259986877\n",
            "223 0.9230251908302307\n",
            "224 0.9113152027130127\n",
            "225 0.9000112414360046\n",
            "226 0.8890950083732605\n",
            "227 0.8783183693885803\n",
            "228 0.8680440187454224\n",
            "229 0.8583460450172424\n",
            "230 0.8484892249107361\n",
            "231 0.8379020094871521\n",
            "232 0.8271515965461731\n",
            "233 0.8181173801422119\n",
            "234 0.8087332248687744\n",
            "235 0.7984774112701416\n",
            "236 0.7887642979621887\n",
            "237 0.7802799940109253\n",
            "238 0.7703004479408264\n",
            "239 0.7607495784759521\n",
            "240 0.7518784403800964\n",
            "241 0.7431155443191528\n",
            "242 0.7346933484077454\n",
            "243 0.7255427837371826\n",
            "244 0.717191755771637\n",
            "245 0.7083696722984314\n",
            "246 0.6998247504234314\n",
            "247 0.6917384266853333\n",
            "248 0.6833145022392273\n",
            "249 0.6753175854682922\n",
            "250 0.6670955419540405\n",
            "251 0.6592917442321777\n",
            "252 0.6516043543815613\n",
            "253 0.6445877552032471\n",
            "254 0.6362856030464172\n",
            "255 0.6289554238319397\n",
            "256 0.6208694577217102\n",
            "257 0.6141438484191895\n",
            "258 0.607028603553772\n",
            "259 0.599499523639679\n",
            "260 0.5921828150749207\n",
            "261 0.5859380960464478\n",
            "262 0.5789040923118591\n",
            "263 0.5721571445465088\n",
            "264 0.565005898475647\n",
            "265 0.5586638450622559\n",
            "266 0.5521203875541687\n",
            "267 0.5462047457695007\n",
            "268 0.5391643643379211\n",
            "269 0.5329163670539856\n",
            "270 0.5274980068206787\n",
            "271 0.5205759406089783\n",
            "272 0.5156844258308411\n",
            "273 0.509189784526825\n",
            "274 0.5031084418296814\n",
            "275 0.49733567237854004\n",
            "276 0.49161311984062195\n",
            "277 0.48567473888397217\n",
            "278 0.4805251657962799\n",
            "279 0.47515353560447693\n",
            "280 0.46990692615509033\n",
            "281 0.46448445320129395\n",
            "282 0.45968949794769287\n",
            "283 0.4539151191711426\n",
            "284 0.44909536838531494\n",
            "285 0.4444376528263092\n",
            "286 0.4392834007740021\n",
            "287 0.43413007259368896\n",
            "288 0.43012964725494385\n",
            "289 0.42487362027168274\n",
            "290 0.4202461838722229\n",
            "291 0.41604915261268616\n",
            "292 0.4112420678138733\n",
            "293 0.40665164589881897\n",
            "294 0.40246331691741943\n",
            "295 0.398245632648468\n",
            "296 0.3938278555870056\n",
            "297 0.3892684876918793\n",
            "298 0.385611355304718\n",
            "299 0.3815312683582306\n",
            "300 0.3771524429321289\n",
            "301 0.3731195330619812\n",
            "302 0.3693042993545532\n",
            "303 0.36556553840637207\n",
            "304 0.3618500232696533\n",
            "305 0.3578522205352783\n",
            "306 0.3544692099094391\n",
            "307 0.3506249189376831\n",
            "308 0.3471224308013916\n",
            "309 0.34344178438186646\n",
            "310 0.33992132544517517\n",
            "311 0.33667895197868347\n",
            "312 0.33317986130714417\n",
            "313 0.3298686146736145\n",
            "314 0.3266248404979706\n",
            "315 0.32340359687805176\n",
            "316 0.32015419006347656\n",
            "317 0.3168588876724243\n",
            "318 0.3139309585094452\n",
            "319 0.3108579218387604\n",
            "320 0.3077447712421417\n",
            "321 0.30480077862739563\n",
            "322 0.3017370402812958\n",
            "323 0.29887524247169495\n",
            "324 0.2962993085384369\n",
            "325 0.2932327389717102\n",
            "326 0.29049113392829895\n",
            "327 0.2876426577568054\n",
            "328 0.2850595712661743\n",
            "329 0.28232017159461975\n",
            "330 0.2797775864601135\n",
            "331 0.2771843671798706\n",
            "332 0.2747255265712738\n",
            "333 0.2721833288669586\n",
            "334 0.26971346139907837\n",
            "335 0.2671669125556946\n",
            "336 0.2647664248943329\n",
            "337 0.2623366117477417\n",
            "338 0.2599780857563019\n",
            "339 0.2577518820762634\n",
            "340 0.25560522079467773\n",
            "341 0.25318580865859985\n",
            "342 0.250887393951416\n",
            "343 0.24886462092399597\n",
            "344 0.24664002656936646\n",
            "345 0.24436339735984802\n",
            "346 0.2424059361219406\n",
            "347 0.2402254343032837\n",
            "348 0.23823969066143036\n",
            "349 0.23614570498466492\n",
            "350 0.23418068885803223\n",
            "351 0.232334166765213\n",
            "352 0.2305261641740799\n",
            "353 0.22841066122055054\n",
            "354 0.22640030086040497\n",
            "355 0.22453761100769043\n",
            "356 0.2227560132741928\n",
            "357 0.2208576202392578\n",
            "358 0.21901756525039673\n",
            "359 0.21722887456417084\n",
            "360 0.2155996710062027\n",
            "361 0.21383491158485413\n",
            "362 0.21214643120765686\n",
            "363 0.21038122475147247\n",
            "364 0.20877523720264435\n",
            "365 0.20706140995025635\n",
            "366 0.2053806632757187\n",
            "367 0.20378832519054413\n",
            "368 0.20222850143909454\n",
            "369 0.20072756707668304\n",
            "370 0.19911642372608185\n",
            "371 0.1974932700395584\n",
            "372 0.19602909684181213\n",
            "373 0.19455461204051971\n",
            "374 0.19304081797599792\n",
            "375 0.19154904782772064\n",
            "376 0.19014377892017365\n",
            "377 0.1887848973274231\n",
            "378 0.18729424476623535\n",
            "379 0.18594174087047577\n",
            "380 0.18453332781791687\n",
            "381 0.18315282464027405\n",
            "382 0.1818121075630188\n",
            "383 0.18052974343299866\n",
            "384 0.17930923402309418\n",
            "385 0.17796115577220917\n",
            "386 0.17663225531578064\n",
            "387 0.17531311511993408\n",
            "388 0.1740785539150238\n",
            "389 0.17284810543060303\n",
            "390 0.17162875831127167\n",
            "391 0.17047327756881714\n",
            "392 0.1692829579114914\n",
            "393 0.16804322600364685\n",
            "394 0.16685782372951508\n",
            "395 0.1657690852880478\n",
            "396 0.16463090479373932\n",
            "397 0.16346235573291779\n",
            "398 0.162352055311203\n",
            "399 0.1613200455904007\n",
            "400 0.16013522446155548\n",
            "401 0.15909188985824585\n",
            "402 0.15798670053482056\n",
            "403 0.15693844854831696\n",
            "404 0.15592417120933533\n",
            "405 0.15485766530036926\n",
            "406 0.15383107960224152\n",
            "407 0.15280818939208984\n",
            "408 0.15185987949371338\n",
            "409 0.15084153413772583\n",
            "410 0.1498631089925766\n",
            "411 0.1488964855670929\n",
            "412 0.1479330062866211\n",
            "413 0.14700792729854584\n",
            "414 0.14602011442184448\n",
            "415 0.14509454369544983\n",
            "416 0.14419904351234436\n",
            "417 0.14329658448696136\n",
            "418 0.14238862693309784\n",
            "419 0.1414937824010849\n",
            "420 0.14061382412910461\n",
            "421 0.1397838443517685\n",
            "422 0.13885435461997986\n",
            "423 0.13803640007972717\n",
            "424 0.13720151782035828\n",
            "425 0.13633112609386444\n",
            "426 0.13551545143127441\n",
            "427 0.134693443775177\n",
            "428 0.13390065729618073\n",
            "429 0.13306277990341187\n",
            "430 0.13229595124721527\n",
            "431 0.13149678707122803\n",
            "432 0.1307412087917328\n",
            "433 0.12994830310344696\n",
            "434 0.1292436569929123\n",
            "435 0.12841735780239105\n",
            "436 0.12769675254821777\n",
            "437 0.1269470602273941\n",
            "438 0.12624591588974\n",
            "439 0.12551183998584747\n",
            "440 0.12477867305278778\n",
            "441 0.1240440085530281\n",
            "442 0.12333330512046814\n",
            "443 0.12264733761548996\n",
            "444 0.12200672179460526\n",
            "445 0.12127786874771118\n",
            "446 0.12060606479644775\n",
            "447 0.1199391633272171\n",
            "448 0.11925610154867172\n",
            "449 0.11858446151018143\n",
            "450 0.11792714148759842\n",
            "451 0.11726173758506775\n",
            "452 0.1166246086359024\n",
            "453 0.116037517786026\n",
            "454 0.11540991067886353\n",
            "455 0.1147584468126297\n",
            "456 0.11412500590085983\n",
            "457 0.11350420862436295\n",
            "458 0.11289956420660019\n",
            "459 0.11232027411460876\n",
            "460 0.11173911392688751\n",
            "461 0.11113963276147842\n",
            "462 0.11053134500980377\n",
            "463 0.10994976758956909\n",
            "464 0.10939700901508331\n",
            "465 0.10883474349975586\n",
            "466 0.10824093222618103\n",
            "467 0.10769583284854889\n",
            "468 0.1071595698595047\n",
            "469 0.10658217966556549\n",
            "470 0.10605186969041824\n",
            "471 0.10550674796104431\n",
            "472 0.10497687011957169\n",
            "473 0.10442239046096802\n",
            "474 0.10391053557395935\n",
            "475 0.10338404029607773\n",
            "476 0.10286867618560791\n",
            "477 0.102369025349617\n",
            "478 0.10186819732189178\n",
            "479 0.10132681578397751\n",
            "480 0.10080961883068085\n",
            "481 0.10032504796981812\n",
            "482 0.09984880685806274\n",
            "483 0.09936270862817764\n",
            "484 0.09888015687465668\n",
            "485 0.09841492772102356\n",
            "486 0.0979088693857193\n",
            "487 0.09745533019304276\n",
            "488 0.09697272628545761\n",
            "489 0.09652677923440933\n",
            "490 0.0960475355386734\n",
            "491 0.09558568149805069\n",
            "492 0.09515952318906784\n",
            "493 0.09468327462673187\n",
            "494 0.09424744546413422\n",
            "495 0.09379683434963226\n",
            "496 0.09337897598743439\n",
            "497 0.09294916689395905\n",
            "498 0.09248056262731552\n",
            "499 0.09206658601760864\n",
            "500 0.09164635092020035\n",
            "501 0.09123946726322174\n",
            "502 0.09079389274120331\n",
            "503 0.09038855880498886\n",
            "504 0.08999612927436829\n",
            "505 0.08959116041660309\n",
            "506 0.08917134255170822\n",
            "507 0.08876778930425644\n",
            "508 0.08835499733686447\n",
            "509 0.0879804939031601\n",
            "510 0.08756953477859497\n",
            "511 0.08717599511146545\n",
            "512 0.0867939367890358\n",
            "513 0.08641137182712555\n",
            "514 0.08604873716831207\n",
            "515 0.08567573875188828\n",
            "516 0.08528048545122147\n",
            "517 0.08492375165224075\n",
            "518 0.08454601466655731\n",
            "519 0.08417932689189911\n",
            "520 0.08382035046815872\n",
            "521 0.08344686776399612\n",
            "522 0.08311019092798233\n",
            "523 0.08275216817855835\n",
            "524 0.08239611238241196\n",
            "525 0.08204005658626556\n",
            "526 0.08170225471258163\n",
            "527 0.08135943114757538\n",
            "528 0.08100511878728867\n",
            "529 0.08066744357347488\n",
            "530 0.08033832162618637\n",
            "531 0.0799977034330368\n",
            "532 0.07966026663780212\n",
            "533 0.0793401300907135\n",
            "534 0.07900270819664001\n",
            "535 0.07867872714996338\n",
            "536 0.07836530357599258\n",
            "537 0.07805599272251129\n",
            "538 0.07771628350019455\n",
            "539 0.07741478085517883\n",
            "540 0.07709596306085587\n",
            "541 0.0767778679728508\n",
            "542 0.07646734267473221\n",
            "543 0.07615494728088379\n",
            "544 0.07585795223712921\n",
            "545 0.07555852085351944\n",
            "546 0.07526324689388275\n",
            "547 0.07495667785406113\n",
            "548 0.07465313374996185\n",
            "549 0.07435239106416702\n",
            "550 0.07406982034444809\n",
            "551 0.07376774400472641\n",
            "552 0.07349180430173874\n",
            "553 0.07320248335599899\n",
            "554 0.07291233539581299\n",
            "555 0.07264278084039688\n",
            "556 0.07234824448823929\n",
            "557 0.07207492738962173\n",
            "558 0.07180323451757431\n",
            "559 0.07151608169078827\n",
            "560 0.0712486281991005\n",
            "561 0.07098811864852905\n",
            "562 0.07070342451334\n",
            "563 0.07044575363397598\n",
            "564 0.07017993181943893\n",
            "565 0.06990484893321991\n",
            "566 0.06964301317930222\n",
            "567 0.06937763094902039\n",
            "568 0.06913132220506668\n",
            "569 0.06886711716651917\n",
            "570 0.06860890984535217\n",
            "571 0.06835883110761642\n",
            "572 0.06811271607875824\n",
            "573 0.06785352528095245\n",
            "574 0.06759602576494217\n",
            "575 0.06737279146909714\n",
            "576 0.06711642444133759\n",
            "577 0.06686500459909439\n",
            "578 0.06662515550851822\n",
            "579 0.06638950854539871\n",
            "580 0.06615052372217178\n",
            "581 0.06590716540813446\n",
            "582 0.06566280871629715\n",
            "583 0.06543457508087158\n",
            "584 0.0651981309056282\n",
            "585 0.06497572362422943\n",
            "586 0.0647444948554039\n",
            "587 0.06451719254255295\n",
            "588 0.06428008526563644\n",
            "589 0.06404802948236465\n",
            "590 0.06382805109024048\n",
            "591 0.06360074877738953\n",
            "592 0.0633908212184906\n",
            "593 0.06316497921943665\n",
            "594 0.06294329464435577\n",
            "595 0.06272212415933609\n",
            "596 0.06251727044582367\n",
            "597 0.06229427829384804\n",
            "598 0.06207780912518501\n",
            "599 0.06186538189649582\n",
            "600 0.061650268733501434\n",
            "601 0.06145394220948219\n",
            "602 0.061236780136823654\n",
            "603 0.06102216616272926\n",
            "604 0.0608232282102108\n",
            "605 0.0606173612177372\n",
            "606 0.06041626259684563\n",
            "607 0.0602082833647728\n",
            "608 0.06002264842391014\n",
            "609 0.059816837310791016\n",
            "610 0.05962016060948372\n",
            "611 0.05941930040717125\n",
            "612 0.05921374261379242\n",
            "613 0.059014350175857544\n",
            "614 0.05883786827325821\n",
            "615 0.05863913893699646\n",
            "616 0.05844854936003685\n",
            "617 0.05826250836253166\n",
            "618 0.05805925279855728\n",
            "619 0.057873308658599854\n",
            "620 0.05769187584519386\n",
            "621 0.057492394000291824\n",
            "622 0.057317979633808136\n",
            "623 0.05712198466062546\n",
            "624 0.05693638697266579\n",
            "625 0.05675797164440155\n",
            "626 0.05658087134361267\n",
            "627 0.056398019194602966\n",
            "628 0.05622052401304245\n",
            "629 0.05603576824069023\n",
            "630 0.0558612160384655\n",
            "631 0.055688533931970596\n",
            "632 0.05551254749298096\n",
            "633 0.05533653497695923\n",
            "634 0.055155012756586075\n",
            "635 0.05499120429158211\n",
            "636 0.05481397360563278\n",
            "637 0.054645050317049026\n",
            "638 0.054475925862789154\n",
            "639 0.05431581288576126\n",
            "640 0.0541439987719059\n",
            "641 0.05398588255047798\n",
            "642 0.05380646884441376\n",
            "643 0.05364542454481125\n",
            "644 0.05347488448023796\n",
            "645 0.053313471376895905\n",
            "646 0.05315803363919258\n",
            "647 0.05299130827188492\n",
            "648 0.052833154797554016\n",
            "649 0.052673373371362686\n",
            "650 0.05251647159457207\n",
            "651 0.05235620215535164\n",
            "652 0.05220702663064003\n",
            "653 0.05204996466636658\n",
            "654 0.05189278721809387\n",
            "655 0.051733821630477905\n",
            "656 0.0515877828001976\n",
            "657 0.05143319442868233\n",
            "658 0.05127736181020737\n",
            "659 0.05112083628773689\n",
            "660 0.05097750201821327\n",
            "661 0.05082480609416962\n",
            "662 0.05068793147802353\n",
            "663 0.050528720021247864\n",
            "664 0.0503842756152153\n",
            "665 0.050238147377967834\n",
            "666 0.05009045824408531\n",
            "667 0.04995398968458176\n",
            "668 0.049810320138931274\n",
            "669 0.04966979846358299\n",
            "670 0.04952043667435646\n",
            "671 0.04937110096216202\n",
            "672 0.0492418110370636\n",
            "673 0.049095265567302704\n",
            "674 0.04895368590950966\n",
            "675 0.048817653208971024\n",
            "676 0.048671722412109375\n",
            "677 0.048539113253355026\n",
            "678 0.048399701714515686\n",
            "679 0.048264823853969574\n",
            "680 0.04812946915626526\n",
            "681 0.04800000414252281\n",
            "682 0.04786257445812225\n",
            "683 0.04773103818297386\n",
            "684 0.047589436173439026\n",
            "685 0.04746356979012489\n",
            "686 0.047330960631370544\n",
            "687 0.04720340669155121\n",
            "688 0.04707412049174309\n",
            "689 0.0469406321644783\n",
            "690 0.04681098088622093\n",
            "691 0.04668755084276199\n",
            "692 0.04655665159225464\n",
            "693 0.04643230140209198\n",
            "694 0.046299632638692856\n",
            "695 0.04617898911237717\n",
            "696 0.04604741558432579\n",
            "697 0.045923762023448944\n",
            "698 0.04580021649599075\n",
            "699 0.0456823855638504\n",
            "700 0.0455629788339138\n",
            "701 0.04543891176581383\n",
            "702 0.04531760886311531\n",
            "703 0.045186351984739304\n",
            "704 0.0450700968503952\n",
            "705 0.044949304312467575\n",
            "706 0.04483488202095032\n",
            "707 0.0447184182703495\n",
            "708 0.04459572955965996\n",
            "709 0.044479452073574066\n",
            "710 0.044361382722854614\n",
            "711 0.044251248240470886\n",
            "712 0.04413197562098503\n",
            "713 0.04401162266731262\n",
            "714 0.043899308890104294\n",
            "715 0.04378601908683777\n",
            "716 0.04367128387093544\n",
            "717 0.04355613514780998\n",
            "718 0.043448083102703094\n",
            "719 0.04333466291427612\n",
            "720 0.04322662577033043\n",
            "721 0.0431232675909996\n",
            "722 0.043006718158721924\n",
            "723 0.04289322718977928\n",
            "724 0.042786531150341034\n",
            "725 0.042677100747823715\n",
            "726 0.042563341557979584\n",
            "727 0.042462386190891266\n",
            "728 0.042354319244623184\n",
            "729 0.042241085320711136\n",
            "730 0.04213698208332062\n",
            "731 0.04203158617019653\n",
            "732 0.04192323982715607\n",
            "733 0.041823431849479675\n",
            "734 0.04171650856733322\n",
            "735 0.041613586246967316\n",
            "736 0.04150877520442009\n",
            "737 0.0414055734872818\n",
            "738 0.041304633021354675\n",
            "739 0.04120566323399544\n",
            "740 0.041096288710832596\n",
            "741 0.0409955233335495\n",
            "742 0.04089938849210739\n",
            "743 0.0407961830496788\n",
            "744 0.040702059864997864\n",
            "745 0.04059586673974991\n",
            "746 0.040502436459064484\n",
            "747 0.04039938747882843\n",
            "748 0.040303099900484085\n",
            "749 0.04020639508962631\n",
            "750 0.04010656848549843\n",
            "751 0.040010180324316025\n",
            "752 0.039920903742313385\n",
            "753 0.03981515392661095\n",
            "754 0.039718396961688995\n",
            "755 0.03962645307183266\n",
            "756 0.039533793926239014\n",
            "757 0.03943408280611038\n",
            "758 0.03934288024902344\n",
            "759 0.03924987465143204\n",
            "760 0.039158210158348083\n",
            "761 0.03906184062361717\n",
            "762 0.03897068649530411\n",
            "763 0.03888179361820221\n",
            "764 0.03879459947347641\n",
            "765 0.03869681805372238\n",
            "766 0.03860926255583763\n",
            "767 0.03851601481437683\n",
            "768 0.03842557594180107\n",
            "769 0.03833664208650589\n",
            "770 0.0382513701915741\n",
            "771 0.03816327080130577\n",
            "772 0.038070764392614365\n",
            "773 0.03798487037420273\n",
            "774 0.03789389133453369\n",
            "775 0.03780870884656906\n",
            "776 0.03771890699863434\n",
            "777 0.03763633593916893\n",
            "778 0.037548910826444626\n",
            "779 0.037465259432792664\n",
            "780 0.03737849369645119\n",
            "781 0.03729456290602684\n",
            "782 0.03720949590206146\n",
            "783 0.03712230175733566\n",
            "784 0.03703879192471504\n",
            "785 0.03695763647556305\n",
            "786 0.03687239810824394\n",
            "787 0.03678886219859123\n",
            "788 0.036706939339637756\n",
            "789 0.03662661835551262\n",
            "790 0.03654170408844948\n",
            "791 0.03646066412329674\n",
            "792 0.03637995943427086\n",
            "793 0.03629554435610771\n",
            "794 0.03621656075119972\n",
            "795 0.03613922372460365\n",
            "796 0.03605568781495094\n",
            "797 0.035978663712739944\n",
            "798 0.03589623421430588\n",
            "799 0.03581937029957771\n",
            "800 0.03574161231517792\n",
            "801 0.035669345408678055\n",
            "802 0.0355830192565918\n",
            "803 0.03550530597567558\n",
            "804 0.0354311503469944\n",
            "805 0.0353519581258297\n",
            "806 0.035278595983982086\n",
            "807 0.035200342535972595\n",
            "808 0.0351245291531086\n",
            "809 0.03504874184727669\n",
            "810 0.03497151657938957\n",
            "811 0.03489958867430687\n",
            "812 0.03482101857662201\n",
            "813 0.03474688529968262\n",
            "814 0.0346725732088089\n",
            "815 0.034597426652908325\n",
            "816 0.03452647477388382\n",
            "817 0.03445209190249443\n",
            "818 0.03437680006027222\n",
            "819 0.034305013716220856\n",
            "820 0.034233637154102325\n",
            "821 0.03416310250759125\n",
            "822 0.03408977761864662\n",
            "823 0.034021906554698944\n",
            "824 0.03394563868641853\n",
            "825 0.03387200087308884\n",
            "826 0.03380100801587105\n",
            "827 0.033731210976839066\n",
            "828 0.033661387860774994\n",
            "829 0.033592116087675095\n",
            "830 0.033521123230457306\n",
            "831 0.03345399722456932\n",
            "832 0.03338102251291275\n",
            "833 0.03331339731812477\n",
            "834 0.033245913684368134\n",
            "835 0.03317659720778465\n",
            "836 0.03310772776603699\n",
            "837 0.03304224833846092\n",
            "838 0.03296840563416481\n",
            "839 0.03290573135018349\n",
            "840 0.03283919394016266\n",
            "841 0.032771553844213486\n",
            "842 0.03270433470606804\n",
            "843 0.032637130469083786\n",
            "844 0.032571014016866684\n",
            "845 0.03250517696142197\n",
            "846 0.032441746443510056\n",
            "847 0.032374024391174316\n",
            "848 0.03231064975261688\n",
            "849 0.03224217891693115\n",
            "850 0.032176416367292404\n",
            "851 0.03211146593093872\n",
            "852 0.032051119953393936\n",
            "853 0.03198361024260521\n",
            "854 0.031920310109853745\n",
            "855 0.031857218593358994\n",
            "856 0.031795162707567215\n",
            "857 0.03173563629388809\n",
            "858 0.031669434159994125\n",
            "859 0.03160679712891579\n",
            "860 0.03154291585087776\n",
            "861 0.03148427978157997\n",
            "862 0.031420376151800156\n",
            "863 0.03135913982987404\n",
            "864 0.031294308602809906\n",
            "865 0.031231731176376343\n",
            "866 0.031173184514045715\n",
            "867 0.031113499775528908\n",
            "868 0.03105129674077034\n",
            "869 0.0309901125729084\n",
            "870 0.030933119356632233\n",
            "871 0.030874066054821014\n",
            "872 0.03081044927239418\n",
            "873 0.030749723315238953\n",
            "874 0.030693212524056435\n",
            "875 0.03063333034515381\n",
            "876 0.030575396493077278\n",
            "877 0.030522380024194717\n",
            "878 0.030456284061074257\n",
            "879 0.030400481075048447\n",
            "880 0.030347388237714767\n",
            "881 0.030283154919743538\n",
            "882 0.0302256029099226\n",
            "883 0.030168037861585617\n",
            "884 0.030110908672213554\n",
            "885 0.030055467039346695\n",
            "886 0.02999671921133995\n",
            "887 0.029942821711301804\n",
            "888 0.02988276258111\n",
            "889 0.029828758910298347\n",
            "890 0.029773442074656487\n",
            "891 0.029712524265050888\n",
            "892 0.029659157618880272\n",
            "893 0.029602505266666412\n",
            "894 0.029548779129981995\n",
            "895 0.02949426881968975\n",
            "896 0.029438961297273636\n",
            "897 0.029384562745690346\n",
            "898 0.029330914840102196\n",
            "899 0.02927553839981556\n",
            "900 0.029219258576631546\n",
            "901 0.029167184606194496\n",
            "902 0.029111376032233238\n",
            "903 0.029059719294309616\n",
            "904 0.02900863066315651\n",
            "905 0.02895452082157135\n",
            "906 0.028899777680635452\n",
            "907 0.028848763555288315\n",
            "908 0.028793055564165115\n",
            "909 0.028744252398610115\n",
            "910 0.028691459447145462\n",
            "911 0.028635406866669655\n",
            "912 0.028582461178302765\n",
            "913 0.028536012396216393\n",
            "914 0.0284828282892704\n",
            "915 0.02843172289431095\n",
            "916 0.028378864750266075\n",
            "917 0.028330229222774506\n",
            "918 0.028277594596147537\n",
            "919 0.028226440772414207\n",
            "920 0.028175130486488342\n",
            "921 0.02812463417649269\n",
            "922 0.02807459607720375\n",
            "923 0.02802710421383381\n",
            "924 0.027975084260106087\n",
            "925 0.027925357222557068\n",
            "926 0.027874452993273735\n",
            "927 0.02782500721514225\n",
            "928 0.027780679985880852\n",
            "929 0.027728496119379997\n",
            "930 0.027677183970808983\n",
            "931 0.02762746252119541\n",
            "932 0.027581743896007538\n",
            "933 0.027532881125807762\n",
            "934 0.02748623676598072\n",
            "935 0.02743689715862274\n",
            "936 0.02738761156797409\n",
            "937 0.027341404929757118\n",
            "938 0.027291275560855865\n",
            "939 0.027244413271546364\n",
            "940 0.02719735912978649\n",
            "941 0.027150103822350502\n",
            "942 0.027103601023554802\n",
            "943 0.027057699859142303\n",
            "944 0.02700776420533657\n",
            "945 0.02696286141872406\n",
            "946 0.026916533708572388\n",
            "947 0.02687191590666771\n",
            "948 0.02682337909936905\n",
            "949 0.026777414605021477\n",
            "950 0.026731405407190323\n",
            "951 0.026686061173677444\n",
            "952 0.026639994233846664\n",
            "953 0.026594344526529312\n",
            "954 0.02655119076371193\n",
            "955 0.0265041533857584\n",
            "956 0.026459969580173492\n",
            "957 0.02641608566045761\n",
            "958 0.026370886713266373\n",
            "959 0.026326032355427742\n",
            "960 0.026280706748366356\n",
            "961 0.026236237958073616\n",
            "962 0.026192503049969673\n",
            "963 0.026148978620767593\n",
            "964 0.02610321156680584\n",
            "965 0.0260627344250679\n",
            "966 0.026015615090727806\n",
            "967 0.02597467042505741\n",
            "968 0.025929542258381844\n",
            "969 0.02588728815317154\n",
            "970 0.025843115523457527\n",
            "971 0.025800200179219246\n",
            "972 0.025757746770977974\n",
            "973 0.025714673101902008\n",
            "974 0.025672046467661858\n",
            "975 0.025631699711084366\n",
            "976 0.02558794990181923\n",
            "977 0.025544917210936546\n",
            "978 0.025505056604743004\n",
            "979 0.025461144745349884\n",
            "980 0.0254232008010149\n",
            "981 0.02537696436047554\n",
            "982 0.025337249040603638\n",
            "983 0.025296872481703758\n",
            "984 0.025253627449274063\n",
            "985 0.025213534012436867\n",
            "986 0.02517208456993103\n",
            "987 0.02513243816792965\n",
            "988 0.025090379640460014\n",
            "989 0.025050316005945206\n",
            "990 0.025009600445628166\n",
            "991 0.0249702837318182\n",
            "992 0.024929573759436607\n",
            "993 0.024888165295124054\n",
            "994 0.024846885353326797\n",
            "995 0.024809397757053375\n",
            "996 0.024768421426415443\n",
            "997 0.0247298963367939\n",
            "998 0.02468962036073208\n",
            "999 0.024652084335684776\n"
          ]
        }
      ]
    }
  ]
}