{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzG//cliV7KE/MJWXE479p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/BabyGPT/blob/main/Notebook/BabyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch import nn\n",
        "# import math\n",
        "# import numpy as np \n",
        "# import torch.nn.functional  as F\n",
        "# from math import sqrt\n",
        "# import torch.nn as nn \n",
        "\n",
        "\n",
        "\n",
        "# words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "# # words[:20]\n",
        "\n",
        "\n",
        "# chars = sorted(list(set(words)))\n",
        "# string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "# # print(string2integer)\n",
        "\n",
        "# integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "# encode = lambda s: [string2integer[c] for c in s]\n",
        "# # print(encode)\n",
        "\n",
        "# decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "# # print(decode)\n",
        "\n",
        "# data = torch.tensor(encode(words), dtype = torch.long)\n",
        "# # print(data)\n",
        "# # data.size()\n",
        "\n",
        "# ## block_size and batch size has been changed from 64 and 512 to 32 and 128\n",
        "# block_size = 16\n",
        "# batch_size = 32\n",
        "# ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "# ## hidden dimensionality has been changed from 512 to 128.\n",
        "\n",
        "# vocab_size = len(chars)\n",
        "# d_k = 32\n",
        "# token_emb = nn.Embedding(vocab_size, d_k)\n",
        "\n",
        "\n",
        "# x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "# input_embeds = token_emb(x)\n",
        "# # input_embeds.size()\n",
        "\n",
        "\n",
        "# def scaled_dot_product(query, key, value):\n",
        "#   dim_k = query.size(-1)\n",
        "#   scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "#   weights = F.softmax(scores, dim = -1)\n",
        "#   return torch.bmm(weights, value)\n",
        "\n",
        "# key = input_embeds\n",
        "# query = input_embeds\n",
        "# value = input_embeds\n",
        "\n",
        "# # sdp = scaled_dot_product(query, key, value)\n",
        "# # print(sdp.size())\n",
        "\n",
        "# ### Multi headed attention\n",
        "\n",
        "# \"\"\"Having many heads allows the model to focus on different parts of the sentences. \n",
        "# The softmax on one head tends to focus on one aspect of similarity. For example subject verb interaction.\"\"\"\n",
        "# ## A single attention head\n",
        "\n",
        "# class AttentionHead(nn.Module):\n",
        "#   def __init__(self, embedded_dim, head_dim):\n",
        "#     super().__init__()\n",
        "#     self.q = nn.Linear(embedded_dim, head_dim)\n",
        "#     self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "#     self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "#     return attention_outputs\n",
        "\n",
        "# # embedding_dim = embedding dimensions\n",
        "# # num_heads  = number of heads \n",
        "\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#   def __init__(self, embedded_dim, num_heads):\n",
        "#     super().__init__()\n",
        "#     self.embedded_dim = embedded_dim\n",
        "#     self.num_heads = num_heads\n",
        "#     head_dim = embedded_dim // num_heads \n",
        "\n",
        "#     self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "#     self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "#     out = self.output_linear(out)\n",
        "\n",
        "#     return out\n",
        "\n",
        "# # multihead_attention = MultiHeadAttention(128, 8)\n",
        "# # # multihead_attention\n",
        "\n",
        "# # attention_outputs =  multihead_attention(input_embeds)\n",
        "# # # print(attention_outputs.size())\n",
        "\n",
        "\n",
        "# # from karpathy , partially\n",
        "# dropout = 0.2\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#   def __init__(self, embedded_dim):\n",
        "#     super(FeedForward, self).__init__()\n",
        "#     self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "#     nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "#     nn.GELU(),\n",
        "#     nn.Dropout(dropout))\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     return self.net(x)\n",
        "\n",
        "\n",
        "# ### A simple Transformer Block    \n",
        "# class TransformerBlock(nn.Module):\n",
        "#   def __init__(self, embedded_dim, num_heads):\n",
        "#     super(TransformerBlock, self).__init__()\n",
        "#     self.attention = MultiHeadAttention(embedded_dim,  num_heads)\n",
        "#     self.feed_forward = FeedForward(embedded_dim)\n",
        "#     self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "#     self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "#   def forward(self, x):\n",
        "    \n",
        "#     x = x + self.attention(self.layer_norm_1(x))\n",
        "#     x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "#     return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mOdus5D9uFKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class BabyGPTmodel(nn.Module):\n",
        "#   def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, d_k):\n",
        "#     super(BabyGPTmodel, self).__init__()\n",
        "#     self.token = nn.Embedding(vocab_size, d_k)\n",
        "#     self.positional_embeddings = nn.Embedding(block_size, d_k)\n",
        "#     self.layers1 = nn.ModuleList([TransformerBlock(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "#     self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "#     self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "#     # init all weights\n",
        "#     ## from karpathy\n",
        "#     self.apply(self._init_weights)\n",
        "#     # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "#     for pn, p in self.named_parameters():\n",
        "#       if pn.endswith('q.weight'):\n",
        "#         torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "#         # report number of parameters\n",
        "#         print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "#   def _init_weights(self, module):\n",
        "#       if isinstance(module, nn.Linear):\n",
        "#           torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "#           if module.bias is not None:\n",
        "#               torch.nn.init.zeros_(module.bias)\n",
        "#       elif isinstance(module, nn.Embedding):\n",
        "#           torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "#   def forward(self, idx):\n",
        "#     device = idx.device\n",
        "#     b, t = idx.size()\n",
        "#     tok_emb = self.token(x)\n",
        "#     position_ids = torch.arange(x.size(-1), dtype = torch.long).unsqueeze(0)\n",
        "#     pos_emb = self.positional_embeddings(position_ids)\n",
        "#     x = tok_emb + pos_emb\n",
        "#     for layers1 in self.layers1:\n",
        "#       x = layers1(x)\n",
        "#       x = self.ln_f(x)\n",
        "#     logits = self.ln_head(x)\n",
        "#     return logits"
      ],
      "metadata": {
        "id": "uiePG-vfm8Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## number of parameters: 117,187\n",
        "# num_layers = 3\n",
        "# gpt = BabyGPTmodel(num_layers, vocab_size, block_size, 32, 8, 32)\n",
        "# d = gpt(x)\n",
        "# d.size()"
      ],
      "metadata": {
        "id": "Qk_1wRLcHSP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Attention, self).__init__()\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "W1xqB6Arc73u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(3, 3, 16)\n",
        "\n",
        "# att = Attention(16, 8)\n",
        "# tt = att(x)\n",
        "# tt.size()"
      ],
      "metadata": {
        "id": "8fgGPCd9Yxbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "X7zzfrdObFZL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed_forward = FeedForward(embedded_dim = 16)\n",
        "# # feed_forward\n",
        "# ff_outputs = feed_forward(tt)\n",
        "# ff_outputs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQlwF5bbYAz",
        "outputId": "976c7577-da62-4c91-b5aa-8ab53c1f3648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "39qXw2CmbpSO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btt = Transformer(16, 8)\n",
        "# to = btt(x)\n",
        "# print(to.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKLZmauscJiK",
        "outputId": "08fd3c82-8e42-43fc-dba0-313703edda55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "XuvEumpqcdwU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btt = Transformer(16, 8)\n",
        "# to = btt(x)\n",
        "# print(to.size())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9iJ54aRgZDI",
        "outputId": "31e50bd1-d388-4f84-d031-198559eb9f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size =  4\n",
        "# block_size = 4\n",
        "# embedded_dim = 16\n",
        "# num_heads = 4\n",
        "# num_layers = 4\n",
        "# gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "# # number of parameters: 13,315"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br5cx40HjsjY",
        "outputId": "27335a6c-d765-4ab6-c701-751caf51eaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 13315\n",
            "number of parameters: 13315\n",
            "number of parameters: 13315\n",
            "number of parameters: 13315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq = list(map(int, \"1111011110111101101\"))\n",
        "# seq"
      ],
      "metadata": {
        "id": "J2R76Xsfnv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## from karpathy\n",
        "# # convert the sequence to a tensor holding all the individual examples in that sequence\n",
        "# X, Y = [], []\n",
        "# # iterate over the sequence and grab every consecutive 3 bits\n",
        "# # the correct label for what's next is the next bit at each position\n",
        "# for i in range(len(seq) - block_size):\n",
        "#     X.append(seq[i:i+block_size])\n",
        "#     Y.append(seq[i+block_size])\n",
        "#     print(f\"example {i+1:2d}: {X[-1]} --> {Y[-1]}\")\n",
        "# X = torch.tensor(X,  dtype=torch.long)\n",
        "# Y = torch.tensor(Y,  dtype=torch.long)\n",
        "# # print(X.shape, Y.shape)\n",
        "# # print(X.size(), Y.size())\n",
        "\n",
        "# print(X, Y)"
      ],
      "metadata": {
        "id": "2MHJ-fZ0n5c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "id": "fdEuygOMouVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(100):\n",
        "#     logits = gpt(X)\n",
        "#     loss = F.cross_entropy(logits, Y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     optimizer.zero_grad()\n",
        "#     print(i, loss.item())"
      ],
      "metadata": {
        "id": "--b_jDhDoykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open(r\"/content/ALL_eminem.txt\", 'r' , encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n"
      ],
      "metadata": {
        "id": "_19368fGf9zZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+block_size] for i in ix])\n",
        "print((x, y))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwhLy0v-l2I7",
        "outputId": "c93a0f74-5759-44c4-8840-a5570594ee52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[14842,  5142, 20198, 21389],\n",
            "        [13441, 20158,  9745,  7090],\n",
            "        [ 3885, 18807,  7016,  9653],\n",
            "        [17648, 12735, 13699,  3574],\n",
            "        [16082,  2405, 21347, 13018],\n",
            "        [ 6914, 20158, 15964, 13909],\n",
            "        [13685, 16028, 20188, 15593],\n",
            "        [20158, 18854,  2862,  3327],\n",
            "        [12814,   335, 17949,    51],\n",
            "        [ 4304,  3416, 10684,  5534],\n",
            "        [ 6155,  6868, 11101, 12333],\n",
            "        [14842, 20990, 20429, 20158],\n",
            "        [20429, 17270,  3302,  7731],\n",
            "        [14237,  9879, 19527, 22015],\n",
            "        [ 8545,  2917, 14842, 15862],\n",
            "        [14842, 12311,  5800, 13664]]), tensor([11443, 15685, 13677, 21404, 20158, 17882, 15233, 15893,  4282, 20234,\n",
            "        12414,  7325, 20487, 13441, 20571, 20158]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZwnA9UxnPnt",
        "outputId": "9759bacd-730a-4a94-a1fc-f5fc3d528fe7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 4]), torch.Size([16]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "## number of parameters: 28,990"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KO184DtqEFe",
        "outputId": "ff4db598-f48e-40a8-baa8-bd03a78410f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 743407\n",
            "number of parameters: 743407\n",
            "number of parameters: 743407\n",
            "number of parameters: 743407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]"
      ],
      "metadata": {
        "id": "2GxxoDMvgQ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d = torch.tensor(train_data.shape, dtype = torch.long)\n",
        "# p = torch.tensor(val_data.shape, dtype = torch.long)\n",
        "# p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9O0JKqhsmN",
        "outputId": "9caff398-cc88-4b7f-a6eb-75aacfbf8ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([135])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # batch_size = 32\n",
        "# input = torch.randn(d.shape, embedded_dim)\n",
        "# target = torch.tensor(p.shape)\n",
        "# print(input.shape, target.shape)"
      ],
      "metadata": {
        "id": "2mOtGcYLgqzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "id": "-XYEFDhqqk2-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    logits = gpt(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(i, loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG2r1I4PjCFj",
        "outputId": "93010cf1-72a9-4fc5-c54a-90cd666fd28d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 9.970707893371582\n",
            "1 9.909249305725098\n",
            "2 9.867252349853516\n",
            "3 9.830041885375977\n",
            "4 9.787096977233887\n",
            "5 9.748181343078613\n",
            "6 9.710127830505371\n",
            "7 9.67908000946045\n",
            "8 9.646204948425293\n",
            "9 9.619718551635742\n",
            "10 9.59192943572998\n",
            "11 9.562115669250488\n",
            "12 9.529630661010742\n",
            "13 9.495203971862793\n",
            "14 9.467424392700195\n",
            "15 9.43537425994873\n",
            "16 9.406689643859863\n",
            "17 9.372254371643066\n",
            "18 9.343368530273438\n",
            "19 9.314236640930176\n",
            "20 9.287075996398926\n",
            "21 9.250592231750488\n",
            "22 9.218886375427246\n",
            "23 9.18537425994873\n",
            "24 9.154319763183594\n",
            "25 9.123664855957031\n",
            "26 9.087516784667969\n",
            "27 9.054779052734375\n",
            "28 9.019492149353027\n",
            "29 8.98450756072998\n",
            "30 8.949871063232422\n",
            "31 8.914268493652344\n",
            "32 8.878110885620117\n",
            "33 8.843926429748535\n",
            "34 8.807768821716309\n",
            "35 8.77131175994873\n",
            "36 8.733325958251953\n",
            "37 8.698160171508789\n",
            "38 8.658926963806152\n",
            "39 8.62075424194336\n",
            "40 8.58332347869873\n",
            "41 8.543478012084961\n",
            "42 8.50418758392334\n",
            "43 8.465574264526367\n",
            "44 8.426109313964844\n",
            "45 8.38630485534668\n",
            "46 8.345240592956543\n",
            "47 8.304780960083008\n",
            "48 8.264434814453125\n",
            "49 8.223134994506836\n",
            "50 8.180618286132812\n",
            "51 8.13991928100586\n",
            "52 8.096891403198242\n",
            "53 8.053964614868164\n",
            "54 8.01152515411377\n",
            "55 7.967983245849609\n",
            "56 7.924756050109863\n",
            "57 7.880714416503906\n",
            "58 7.83698034286499\n",
            "59 7.793002605438232\n",
            "60 7.747344493865967\n",
            "61 7.701671600341797\n",
            "62 7.656518936157227\n",
            "63 7.611610412597656\n",
            "64 7.564818382263184\n",
            "65 7.518181324005127\n",
            "66 7.472015857696533\n",
            "67 7.425185203552246\n",
            "68 7.37789249420166\n",
            "69 7.330350875854492\n",
            "70 7.283097267150879\n",
            "71 7.2353386878967285\n",
            "72 7.186951637268066\n",
            "73 7.138140678405762\n",
            "74 7.089673042297363\n",
            "75 7.04119348526001\n",
            "76 6.9913506507873535\n",
            "77 6.9419474601745605\n",
            "78 6.892465591430664\n",
            "79 6.8420634269714355\n",
            "80 6.792114734649658\n",
            "81 6.7414655685424805\n",
            "82 6.6911492347717285\n",
            "83 6.639742374420166\n",
            "84 6.589127540588379\n",
            "85 6.537383556365967\n",
            "86 6.4857635498046875\n",
            "87 6.434587001800537\n",
            "88 6.382264137268066\n",
            "89 6.3303375244140625\n",
            "90 6.277928352355957\n",
            "91 6.225093364715576\n",
            "92 6.1728715896606445\n",
            "93 6.120231628417969\n",
            "94 6.067118167877197\n",
            "95 6.014089584350586\n",
            "96 5.960979461669922\n",
            "97 5.907487869262695\n",
            "98 5.85390567779541\n",
            "99 5.80033016204834\n",
            "100 5.746769905090332\n",
            "101 5.692867755889893\n",
            "102 5.6389899253845215\n",
            "103 5.585249900817871\n",
            "104 5.5311126708984375\n",
            "105 5.477425575256348\n",
            "106 5.423417091369629\n",
            "107 5.369664669036865\n",
            "108 5.315937042236328\n",
            "109 5.2618913650512695\n",
            "110 5.208378314971924\n",
            "111 5.154635906219482\n",
            "112 5.100910186767578\n",
            "113 5.047994136810303\n",
            "114 4.994558334350586\n",
            "115 4.941789627075195\n",
            "116 4.888978958129883\n",
            "117 4.83622407913208\n",
            "118 4.783944129943848\n",
            "119 4.7318572998046875\n",
            "120 4.680346488952637\n",
            "121 4.628697872161865\n",
            "122 4.577942848205566\n",
            "123 4.527378559112549\n",
            "124 4.4770355224609375\n",
            "125 4.427636623382568\n",
            "126 4.378093242645264\n",
            "127 4.329585075378418\n",
            "128 4.28154993057251\n",
            "129 4.233805179595947\n",
            "130 4.1868672370910645\n",
            "131 4.14055871963501\n",
            "132 4.095083713531494\n",
            "133 4.050241470336914\n",
            "134 4.005902290344238\n",
            "135 3.9626715183258057\n",
            "136 3.9196665287017822\n",
            "137 3.878183126449585\n",
            "138 3.8368687629699707\n",
            "139 3.796653985977173\n",
            "140 3.757403612136841\n",
            "141 3.718672275543213\n",
            "142 3.6812195777893066\n",
            "143 3.644434928894043\n",
            "144 3.608391284942627\n",
            "145 3.573762893676758\n",
            "146 3.5395493507385254\n",
            "147 3.50641131401062\n",
            "148 3.4742279052734375\n",
            "149 3.442887783050537\n",
            "150 3.4124324321746826\n",
            "151 3.382741689682007\n",
            "152 3.3537325859069824\n",
            "153 3.325819730758667\n",
            "154 3.298372268676758\n",
            "155 3.272036075592041\n",
            "156 3.246774435043335\n",
            "157 3.221766233444214\n",
            "158 3.1979897022247314\n",
            "159 3.174346446990967\n",
            "160 3.1517457962036133\n",
            "161 3.1291167736053467\n",
            "162 3.107879638671875\n",
            "163 3.087048053741455\n",
            "164 3.066678524017334\n",
            "165 3.0469748973846436\n",
            "166 3.027733325958252\n",
            "167 3.0092716217041016\n",
            "168 2.9907114505767822\n",
            "169 2.973320960998535\n",
            "170 2.955719470977783\n",
            "171 2.938676118850708\n",
            "172 2.9223947525024414\n",
            "173 2.905898094177246\n",
            "174 2.890219211578369\n",
            "175 2.8749442100524902\n",
            "176 2.860532760620117\n",
            "177 2.8446202278137207\n",
            "178 2.8309311866760254\n",
            "179 2.815683126449585\n",
            "180 2.801459789276123\n",
            "181 2.7882306575775146\n",
            "182 2.7737154960632324\n",
            "183 2.7618064880371094\n",
            "184 2.7470483779907227\n",
            "185 2.735264778137207\n",
            "186 2.721820592880249\n",
            "187 2.708867311477661\n",
            "188 2.6961746215820312\n",
            "189 2.684391736984253\n",
            "190 2.672089099884033\n",
            "191 2.6594386100769043\n",
            "192 2.647702693939209\n",
            "193 2.6350951194763184\n",
            "194 2.6254849433898926\n",
            "195 2.611996650695801\n",
            "196 2.6002514362335205\n",
            "197 2.5898079872131348\n",
            "198 2.5763399600982666\n",
            "199 2.5677218437194824\n",
            "200 2.5529956817626953\n",
            "201 2.5410068035125732\n",
            "202 2.5311620235443115\n",
            "203 2.5193710327148438\n",
            "204 2.508033037185669\n",
            "205 2.4971189498901367\n",
            "206 2.484746217727661\n",
            "207 2.4737155437469482\n",
            "208 2.462714910507202\n",
            "209 2.4511148929595947\n",
            "210 2.4398655891418457\n",
            "211 2.429455041885376\n",
            "212 2.4187607765197754\n",
            "213 2.40653133392334\n",
            "214 2.396620750427246\n",
            "215 2.384828805923462\n",
            "216 2.3740732669830322\n",
            "217 2.3618900775909424\n",
            "218 2.3519785404205322\n",
            "219 2.3405351638793945\n",
            "220 2.330252170562744\n",
            "221 2.3188977241516113\n",
            "222 2.309013843536377\n",
            "223 2.296069383621216\n",
            "224 2.2856411933898926\n",
            "225 2.275179386138916\n",
            "226 2.2636008262634277\n",
            "227 2.2536518573760986\n",
            "228 2.24251651763916\n",
            "229 2.233006477355957\n",
            "230 2.222262382507324\n",
            "231 2.2099449634552\n",
            "232 2.2008166313171387\n",
            "233 2.188086986541748\n",
            "234 2.1771438121795654\n",
            "235 2.1664369106292725\n",
            "236 2.1545169353485107\n",
            "237 2.144460439682007\n",
            "238 2.134216547012329\n",
            "239 2.121769428253174\n",
            "240 2.1106207370758057\n",
            "241 2.1019718647003174\n",
            "242 2.0881006717681885\n",
            "243 2.076591730117798\n",
            "244 2.067803144454956\n",
            "245 2.0555999279022217\n",
            "246 2.043764352798462\n",
            "247 2.033161163330078\n",
            "248 2.0210165977478027\n",
            "249 2.0118496417999268\n",
            "250 1.9996850490570068\n",
            "251 1.9880750179290771\n",
            "252 1.9772521257400513\n",
            "253 1.9652161598205566\n",
            "254 1.955294132232666\n",
            "255 1.944092869758606\n",
            "256 1.932046890258789\n",
            "257 1.9204115867614746\n",
            "258 1.9106253385543823\n",
            "259 1.8984848260879517\n",
            "260 1.8872464895248413\n",
            "261 1.8774341344833374\n",
            "262 1.8651578426361084\n",
            "263 1.8542505502700806\n",
            "264 1.843296766281128\n",
            "265 1.83164644241333\n",
            "266 1.8213087320327759\n",
            "267 1.8097624778747559\n",
            "268 1.7978506088256836\n",
            "269 1.7872731685638428\n",
            "270 1.7758756875991821\n",
            "271 1.7643343210220337\n",
            "272 1.7541754245758057\n",
            "273 1.7419929504394531\n",
            "274 1.7312040328979492\n",
            "275 1.7201800346374512\n",
            "276 1.708228588104248\n",
            "277 1.6973015069961548\n",
            "278 1.685978651046753\n",
            "279 1.6752772331237793\n",
            "280 1.6640872955322266\n",
            "281 1.6523898839950562\n",
            "282 1.641870141029358\n",
            "283 1.630519986152649\n",
            "284 1.6186600923538208\n",
            "285 1.608717918395996\n",
            "286 1.597479224205017\n",
            "287 1.5861872434616089\n",
            "288 1.575995683670044\n",
            "289 1.5635995864868164\n",
            "290 1.552704930305481\n",
            "291 1.5415153503417969\n",
            "292 1.5304644107818604\n",
            "293 1.5199185609817505\n",
            "294 1.5087890625\n",
            "295 1.4969768524169922\n",
            "296 1.4866816997528076\n",
            "297 1.4757933616638184\n",
            "298 1.46454918384552\n",
            "299 1.4528863430023193\n",
            "300 1.4427244663238525\n",
            "301 1.4310617446899414\n",
            "302 1.4206640720367432\n",
            "303 1.4099774360656738\n",
            "304 1.3986421823501587\n",
            "305 1.3879549503326416\n",
            "306 1.3770532608032227\n",
            "307 1.3669017553329468\n",
            "308 1.3562591075897217\n",
            "309 1.3449194431304932\n",
            "310 1.3344216346740723\n",
            "311 1.3234455585479736\n",
            "312 1.3126360177993774\n",
            "313 1.3025424480438232\n",
            "314 1.2915767431259155\n",
            "315 1.2808932065963745\n",
            "316 1.2702000141143799\n",
            "317 1.2602630853652954\n",
            "318 1.249670147895813\n",
            "319 1.2392548322677612\n",
            "320 1.229524850845337\n",
            "321 1.2186647653579712\n",
            "322 1.2085449695587158\n",
            "323 1.1981098651885986\n",
            "324 1.187713384628296\n",
            "325 1.1774423122406006\n",
            "326 1.1671658754348755\n",
            "327 1.1576616764068604\n",
            "328 1.1475286483764648\n",
            "329 1.1375058889389038\n",
            "330 1.1277894973754883\n",
            "331 1.1173925399780273\n",
            "332 1.1077769994735718\n",
            "333 1.0980318784713745\n",
            "334 1.08818519115448\n",
            "335 1.079270839691162\n",
            "336 1.068834900856018\n",
            "337 1.0603774785995483\n",
            "338 1.0503555536270142\n",
            "339 1.0405774116516113\n",
            "340 1.0322932004928589\n",
            "341 1.0215002298355103\n",
            "342 1.0129590034484863\n",
            "343 1.0034178495407104\n",
            "344 0.9937395453453064\n",
            "345 0.984715461730957\n",
            "346 0.975669264793396\n",
            "347 0.9665283560752869\n",
            "348 0.9567772150039673\n",
            "349 0.94817054271698\n",
            "350 0.9396640658378601\n",
            "351 0.9303668737411499\n",
            "352 0.9221779108047485\n",
            "353 0.9126926064491272\n",
            "354 0.9051753282546997\n",
            "355 0.89520263671875\n",
            "356 0.8870254158973694\n",
            "357 0.878704845905304\n",
            "358 0.8698369264602661\n",
            "359 0.8616896867752075\n",
            "360 0.8526632189750671\n",
            "361 0.8451312184333801\n",
            "362 0.8361195921897888\n",
            "363 0.8279091119766235\n",
            "364 0.8202896118164062\n",
            "365 0.8123541474342346\n",
            "366 0.804100751876831\n",
            "367 0.7965389490127563\n",
            "368 0.7885020971298218\n",
            "369 0.7808011174201965\n",
            "370 0.7731711864471436\n",
            "371 0.7658368945121765\n",
            "372 0.758141279220581\n",
            "373 0.7507362365722656\n",
            "374 0.7431204915046692\n",
            "375 0.7360931038856506\n",
            "376 0.7284952402114868\n",
            "377 0.7214188575744629\n",
            "378 0.7141977548599243\n",
            "379 0.7070404887199402\n",
            "380 0.700743556022644\n",
            "381 0.6932942271232605\n",
            "382 0.6861466765403748\n",
            "383 0.6792258620262146\n",
            "384 0.6727915406227112\n",
            "385 0.6659243702888489\n",
            "386 0.6592247486114502\n",
            "387 0.6526509523391724\n",
            "388 0.6461440324783325\n",
            "389 0.6397737860679626\n",
            "390 0.6332672238349915\n",
            "391 0.6271291971206665\n",
            "392 0.6210490465164185\n",
            "393 0.6147292256355286\n",
            "394 0.6083766222000122\n",
            "395 0.6029022932052612\n",
            "396 0.5964823365211487\n",
            "397 0.5907870531082153\n",
            "398 0.5849376320838928\n",
            "399 0.5788993239402771\n",
            "400 0.5735407471656799\n",
            "401 0.5678154826164246\n",
            "402 0.5621866583824158\n",
            "403 0.5569790005683899\n",
            "404 0.5514177680015564\n",
            "405 0.54588383436203\n",
            "406 0.5405945777893066\n",
            "407 0.5352444648742676\n",
            "408 0.5301337838172913\n",
            "409 0.5246124267578125\n",
            "410 0.5197159051895142\n",
            "411 0.5147473812103271\n",
            "412 0.5093628764152527\n",
            "413 0.5045663118362427\n",
            "414 0.499691903591156\n",
            "415 0.4950525760650635\n",
            "416 0.490349143743515\n",
            "417 0.48533275723457336\n",
            "418 0.48085668683052063\n",
            "419 0.4761401116847992\n",
            "420 0.47162100672721863\n",
            "421 0.4672664701938629\n",
            "422 0.46270444989204407\n",
            "423 0.45844945311546326\n",
            "424 0.45411497354507446\n",
            "425 0.4498288035392761\n",
            "426 0.44553619623184204\n",
            "427 0.44151175022125244\n",
            "428 0.4373086094856262\n",
            "429 0.43329107761383057\n",
            "430 0.4290723204612732\n",
            "431 0.4251275956630707\n",
            "432 0.42109015583992004\n",
            "433 0.41736656427383423\n",
            "434 0.4134593904018402\n",
            "435 0.4097261130809784\n",
            "436 0.4060188829898834\n",
            "437 0.4022504389286041\n",
            "438 0.39864760637283325\n",
            "439 0.395168274641037\n",
            "440 0.39134806394577026\n",
            "441 0.38792458176612854\n",
            "442 0.3845856785774231\n",
            "443 0.3811263144016266\n",
            "444 0.3776986002922058\n",
            "445 0.37434592843055725\n",
            "446 0.3710578382015228\n",
            "447 0.36788085103034973\n",
            "448 0.36454594135284424\n",
            "449 0.36140215396881104\n",
            "450 0.3583690822124481\n",
            "451 0.3551812171936035\n",
            "452 0.3520394265651703\n",
            "453 0.34899646043777466\n",
            "454 0.3460134267807007\n",
            "455 0.3430612087249756\n",
            "456 0.34000861644744873\n",
            "457 0.3371279835700989\n",
            "458 0.3344241976737976\n",
            "459 0.3315797746181488\n",
            "460 0.32866787910461426\n",
            "461 0.3259070813655853\n",
            "462 0.32323938608169556\n",
            "463 0.3205675184726715\n",
            "464 0.3178422152996063\n",
            "465 0.31521153450012207\n",
            "466 0.31266942620277405\n",
            "467 0.3101121187210083\n",
            "468 0.3076059818267822\n",
            "469 0.3050348162651062\n",
            "470 0.30259183049201965\n",
            "471 0.30018743872642517\n",
            "472 0.2978012263774872\n",
            "473 0.29543018341064453\n",
            "474 0.29302528500556946\n",
            "475 0.2906409800052643\n",
            "476 0.2884741425514221\n",
            "477 0.2862054109573364\n",
            "478 0.2838903069496155\n",
            "479 0.2816276550292969\n",
            "480 0.27956393361091614\n",
            "481 0.27731379866600037\n",
            "482 0.27513718605041504\n",
            "483 0.27313196659088135\n",
            "484 0.27099376916885376\n",
            "485 0.26892757415771484\n",
            "486 0.26685813069343567\n",
            "487 0.2647845447063446\n",
            "488 0.2627657353878021\n",
            "489 0.2609231770038605\n",
            "490 0.25892457365989685\n",
            "491 0.25694409012794495\n",
            "492 0.25505930185317993\n",
            "493 0.2531639635562897\n",
            "494 0.25132235884666443\n",
            "495 0.24946168065071106\n",
            "496 0.24766579270362854\n",
            "497 0.2458568513393402\n",
            "498 0.24406179785728455\n",
            "499 0.2423597127199173\n",
            "500 0.2405325174331665\n",
            "501 0.23889444768428802\n",
            "502 0.2371618151664734\n",
            "503 0.23554451763629913\n",
            "504 0.2337731271982193\n",
            "505 0.23220017552375793\n",
            "506 0.23056119680404663\n",
            "507 0.22894509136676788\n",
            "508 0.22736017405986786\n",
            "509 0.22580577433109283\n",
            "510 0.22423498332500458\n",
            "511 0.22274300456047058\n",
            "512 0.22114062309265137\n",
            "513 0.21963994204998016\n",
            "514 0.2181176394224167\n",
            "515 0.21667736768722534\n",
            "516 0.21519121527671814\n",
            "517 0.2137598991394043\n",
            "518 0.2123531997203827\n",
            "519 0.21088269352912903\n",
            "520 0.20951607823371887\n",
            "521 0.20813503861427307\n",
            "522 0.206783726811409\n",
            "523 0.20543761551380157\n",
            "524 0.20406484603881836\n",
            "525 0.2027548849582672\n",
            "526 0.20146560668945312\n",
            "527 0.20015552639961243\n",
            "528 0.19884631037712097\n",
            "529 0.1976069062948227\n",
            "530 0.19631431996822357\n",
            "531 0.1950913965702057\n",
            "532 0.19389723241329193\n",
            "533 0.1926451027393341\n",
            "534 0.19142475724220276\n",
            "535 0.1902848184108734\n",
            "536 0.18905214965343475\n",
            "537 0.18788960576057434\n",
            "538 0.1867642104625702\n",
            "539 0.18560945987701416\n",
            "540 0.18445174396038055\n",
            "541 0.18332554399967194\n",
            "542 0.18224141001701355\n",
            "543 0.1811322122812271\n",
            "544 0.18003986775875092\n",
            "545 0.17894381284713745\n",
            "546 0.17786923050880432\n",
            "547 0.17684103548526764\n",
            "548 0.1757846474647522\n",
            "549 0.17474909126758575\n",
            "550 0.17370907962322235\n",
            "551 0.17267173528671265\n",
            "552 0.17169465124607086\n",
            "553 0.17067988216876984\n",
            "554 0.16970708966255188\n",
            "555 0.16871175169944763\n",
            "556 0.1677400767803192\n",
            "557 0.16678786277770996\n",
            "558 0.1658644825220108\n",
            "559 0.1649366170167923\n",
            "560 0.1639750450849533\n",
            "561 0.1630544513463974\n",
            "562 0.16212989389896393\n",
            "563 0.16120460629463196\n",
            "564 0.16031810641288757\n",
            "565 0.1594717800617218\n",
            "566 0.1585642695426941\n",
            "567 0.15770286321640015\n",
            "568 0.15680855512619019\n",
            "569 0.15595872700214386\n",
            "570 0.1550971120595932\n",
            "571 0.15427017211914062\n",
            "572 0.15343815088272095\n",
            "573 0.15260060131549835\n",
            "574 0.1517930030822754\n",
            "575 0.15098215639591217\n",
            "576 0.15015624463558197\n",
            "577 0.14936383068561554\n",
            "578 0.14859797060489655\n",
            "579 0.1478143185377121\n",
            "580 0.14701852202415466\n",
            "581 0.1462583690881729\n",
            "582 0.14552457630634308\n",
            "583 0.14472295343875885\n",
            "584 0.1439867615699768\n",
            "585 0.14324364066123962\n",
            "586 0.14251670241355896\n",
            "587 0.14177820086479187\n",
            "588 0.1410476267337799\n",
            "589 0.14032909274101257\n",
            "590 0.13961200416088104\n",
            "591 0.1389234960079193\n",
            "592 0.1382146179676056\n",
            "593 0.1375504583120346\n",
            "594 0.13685448467731476\n",
            "595 0.13614904880523682\n",
            "596 0.1354912519454956\n",
            "597 0.13481071591377258\n",
            "598 0.1341308057308197\n",
            "599 0.1334805190563202\n",
            "600 0.13283641636371613\n",
            "601 0.1321898102760315\n",
            "602 0.1315486878156662\n",
            "603 0.13091963529586792\n",
            "604 0.1302851140499115\n",
            "605 0.12964756786823273\n",
            "606 0.12902268767356873\n",
            "607 0.12843552231788635\n",
            "608 0.12781429290771484\n",
            "609 0.12720735371112823\n",
            "610 0.12659543752670288\n",
            "611 0.12602019309997559\n",
            "612 0.12543697655200958\n",
            "613 0.12482231855392456\n",
            "614 0.12425243854522705\n",
            "615 0.12367641925811768\n",
            "616 0.12311290949583054\n",
            "617 0.12253858149051666\n",
            "618 0.12197290360927582\n",
            "619 0.12142922729253769\n",
            "620 0.12086551636457443\n",
            "621 0.12031497061252594\n",
            "622 0.11978010088205338\n",
            "623 0.11924266070127487\n",
            "624 0.11870202422142029\n",
            "625 0.11816969513893127\n",
            "626 0.11762578785419464\n",
            "627 0.11709842830896378\n",
            "628 0.11659567058086395\n",
            "629 0.11608348041772842\n",
            "630 0.1155562624335289\n",
            "631 0.11507367342710495\n",
            "632 0.11455857753753662\n",
            "633 0.11403743177652359\n",
            "634 0.11354239284992218\n",
            "635 0.11307206749916077\n",
            "636 0.11257151514291763\n",
            "637 0.11209050565958023\n",
            "638 0.11159887164831161\n",
            "639 0.11111902445554733\n",
            "640 0.11064021289348602\n",
            "641 0.11017335951328278\n",
            "642 0.10972286015748978\n",
            "643 0.1092490702867508\n",
            "644 0.10878025740385056\n",
            "645 0.1083308681845665\n",
            "646 0.10787031799554825\n",
            "647 0.10742224007844925\n",
            "648 0.10697724670171738\n",
            "649 0.10653596371412277\n",
            "650 0.10608498007059097\n",
            "651 0.10564587265253067\n",
            "652 0.10522457212209702\n",
            "653 0.10478918254375458\n",
            "654 0.1043601855635643\n",
            "655 0.1039389818906784\n",
            "656 0.103518545627594\n",
            "657 0.1030888557434082\n",
            "658 0.10267235338687897\n",
            "659 0.10228048264980316\n",
            "660 0.10186424851417542\n",
            "661 0.10146424174308777\n",
            "662 0.10104899108409882\n",
            "663 0.10065627098083496\n",
            "664 0.10025203973054886\n",
            "665 0.09984498471021652\n",
            "666 0.09945591539144516\n",
            "667 0.09906858205795288\n",
            "668 0.09867599606513977\n",
            "669 0.09830453991889954\n",
            "670 0.09792181104421616\n",
            "671 0.09753039479255676\n",
            "672 0.09715299308300018\n",
            "673 0.09676840156316757\n",
            "674 0.09640786051750183\n",
            "675 0.09603945910930634\n",
            "676 0.09566813707351685\n",
            "677 0.09530993551015854\n",
            "678 0.09494414925575256\n",
            "679 0.09458839893341064\n",
            "680 0.09422999620437622\n",
            "681 0.09387440234422684\n",
            "682 0.09352100640535355\n",
            "683 0.09317168593406677\n",
            "684 0.09282155334949493\n",
            "685 0.09247642755508423\n",
            "686 0.09213417768478394\n",
            "687 0.09179455786943436\n",
            "688 0.09145893901586533\n",
            "689 0.09112029522657394\n",
            "690 0.09078576415777206\n",
            "691 0.09045375138521194\n",
            "692 0.09012634307146072\n",
            "693 0.08979620784521103\n",
            "694 0.08947338163852692\n",
            "695 0.08914060145616531\n",
            "696 0.0888255313038826\n",
            "697 0.08850967884063721\n",
            "698 0.08818771690130234\n",
            "699 0.08788704872131348\n",
            "700 0.0875680148601532\n",
            "701 0.08725075423717499\n",
            "702 0.0869496539235115\n",
            "703 0.08663997054100037\n",
            "704 0.08633610606193542\n",
            "705 0.0860365629196167\n",
            "706 0.08573023974895477\n",
            "707 0.08542453497648239\n",
            "708 0.08512910455465317\n",
            "709 0.08482934534549713\n",
            "710 0.08454417437314987\n",
            "711 0.08424989134073257\n",
            "712 0.08396211266517639\n",
            "713 0.08367524296045303\n",
            "714 0.08338592201471329\n",
            "715 0.08309545367956161\n",
            "716 0.08282272517681122\n",
            "717 0.08253646641969681\n",
            "718 0.08225323259830475\n",
            "719 0.08197557926177979\n",
            "720 0.08170031011104584\n",
            "721 0.08143341541290283\n",
            "722 0.08115922659635544\n",
            "723 0.08088822662830353\n",
            "724 0.08061446249485016\n",
            "725 0.08035089820623398\n",
            "726 0.08007900416851044\n",
            "727 0.07981497049331665\n",
            "728 0.07955387979745865\n",
            "729 0.0792907327413559\n",
            "730 0.07903304696083069\n",
            "731 0.07877538353204727\n",
            "732 0.07851578295230865\n",
            "733 0.07826020568609238\n",
            "734 0.0780067965388298\n",
            "735 0.07775648683309555\n",
            "736 0.07750298827886581\n",
            "737 0.07725409418344498\n",
            "738 0.07700458914041519\n",
            "739 0.07675781100988388\n",
            "740 0.07651623338460922\n",
            "741 0.07627161592245102\n",
            "742 0.07603084295988083\n",
            "743 0.07578640431165695\n",
            "744 0.0755520612001419\n",
            "745 0.07531143724918365\n",
            "746 0.07507479935884476\n",
            "747 0.07483754307031631\n",
            "748 0.07460753619670868\n",
            "749 0.07437888532876968\n",
            "750 0.07414081692695618\n",
            "751 0.07391609996557236\n",
            "752 0.07367907464504242\n",
            "753 0.07345151901245117\n",
            "754 0.07322695106267929\n",
            "755 0.07301091402769089\n",
            "756 0.07278060913085938\n",
            "757 0.07255642116069794\n",
            "758 0.07233806699514389\n",
            "759 0.07211694121360779\n",
            "760 0.07189972698688507\n",
            "761 0.07167966663837433\n",
            "762 0.07146356999874115\n",
            "763 0.0712493434548378\n",
            "764 0.07103490084409714\n",
            "765 0.07082202285528183\n",
            "766 0.07062137871980667\n",
            "767 0.07039611041545868\n",
            "768 0.07019278407096863\n",
            "769 0.06998606771230698\n",
            "770 0.06977353990077972\n",
            "771 0.06957519054412842\n",
            "772 0.06936966627836227\n",
            "773 0.06916297972202301\n",
            "774 0.06895935535430908\n",
            "775 0.06876471638679504\n",
            "776 0.0685637965798378\n",
            "777 0.06835972517728806\n",
            "778 0.06816226243972778\n",
            "779 0.06796855479478836\n",
            "780 0.06776665151119232\n",
            "781 0.06757184118032455\n",
            "782 0.06738325953483582\n",
            "783 0.06718860566616058\n",
            "784 0.06699325889348984\n",
            "785 0.06680170446634293\n",
            "786 0.0666128396987915\n",
            "787 0.06642568856477737\n",
            "788 0.06623992323875427\n",
            "789 0.06605064123868942\n",
            "790 0.06586390733718872\n",
            "791 0.06567555665969849\n",
            "792 0.06549829989671707\n",
            "793 0.06531218439340591\n",
            "794 0.06512860208749771\n",
            "795 0.06495005637407303\n",
            "796 0.06477096676826477\n",
            "797 0.06458663195371628\n",
            "798 0.06440993398427963\n",
            "799 0.06423021852970123\n",
            "800 0.06405828893184662\n",
            "801 0.0638824850320816\n",
            "802 0.06370428204536438\n",
            "803 0.0635291337966919\n",
            "804 0.0633571445941925\n",
            "805 0.06318410485982895\n",
            "806 0.06301114708185196\n",
            "807 0.06284144520759583\n",
            "808 0.06267069280147552\n",
            "809 0.0625077486038208\n",
            "810 0.062336910516023636\n",
            "811 0.062169354408979416\n",
            "812 0.06200612708926201\n",
            "813 0.06183789297938347\n",
            "814 0.06166978180408478\n",
            "815 0.06150973588228226\n",
            "816 0.0613461434841156\n",
            "817 0.06118539348244667\n",
            "818 0.06102156639099121\n",
            "819 0.06086571142077446\n",
            "820 0.0606999397277832\n",
            "821 0.06054142862558365\n",
            "822 0.06038655713200569\n",
            "823 0.06022453308105469\n",
            "824 0.06006978079676628\n",
            "825 0.059917304664850235\n",
            "826 0.05975307151675224\n",
            "827 0.0596083402633667\n",
            "828 0.05945461988449097\n",
            "829 0.05929391086101532\n",
            "830 0.0591452419757843\n",
            "831 0.05899113044142723\n",
            "832 0.05884094908833504\n",
            "833 0.05869487300515175\n",
            "834 0.05854075402021408\n",
            "835 0.058389272540807724\n",
            "836 0.058245301246643066\n",
            "837 0.058096177875995636\n",
            "838 0.05794887989759445\n",
            "839 0.05780512094497681\n",
            "840 0.05765942111611366\n",
            "841 0.05751752480864525\n",
            "842 0.05737060308456421\n",
            "843 0.05722696706652641\n",
            "844 0.05708939954638481\n",
            "845 0.05694365128874779\n",
            "846 0.05680202692747116\n",
            "847 0.05665910616517067\n",
            "848 0.05652651935815811\n",
            "849 0.056382421404123306\n",
            "850 0.05624300241470337\n",
            "851 0.056105777621269226\n",
            "852 0.055971235036849976\n",
            "853 0.0558353066444397\n",
            "854 0.05569513514637947\n",
            "855 0.05555662885308266\n",
            "856 0.055422522127628326\n",
            "857 0.05529051646590233\n",
            "858 0.05515681207180023\n",
            "859 0.055025652050971985\n",
            "860 0.054889947175979614\n",
            "861 0.054758649319410324\n",
            "862 0.05462707206606865\n",
            "863 0.054497700184583664\n",
            "864 0.054369013756513596\n",
            "865 0.054234590381383896\n",
            "866 0.05410764366388321\n",
            "867 0.05397922173142433\n",
            "868 0.053851235657930374\n",
            "869 0.053728971630334854\n",
            "870 0.053601112216711044\n",
            "871 0.05347389727830887\n",
            "872 0.05335057154297829\n",
            "873 0.05322360247373581\n",
            "874 0.05309811979532242\n",
            "875 0.052975092083215714\n",
            "876 0.052850235253572464\n",
            "877 0.05272390693426132\n",
            "878 0.05260408669710159\n",
            "879 0.05248406156897545\n",
            "880 0.05236734822392464\n",
            "881 0.05224065110087395\n",
            "882 0.05212118476629257\n",
            "883 0.05200216546654701\n",
            "884 0.05188293382525444\n",
            "885 0.05176354944705963\n",
            "886 0.05164559558033943\n",
            "887 0.05152774974703789\n",
            "888 0.05141140893101692\n",
            "889 0.05129671469330788\n",
            "890 0.051178108900785446\n",
            "891 0.0510643906891346\n",
            "892 0.050948694348335266\n",
            "893 0.05083535239100456\n",
            "894 0.050719186663627625\n",
            "895 0.05060495063662529\n",
            "896 0.050496604293584824\n",
            "897 0.05038332939147949\n",
            "898 0.05026952922344208\n",
            "899 0.05015626549720764\n",
            "900 0.050046149641275406\n",
            "901 0.04993804916739464\n",
            "902 0.049826446920633316\n",
            "903 0.04971941187977791\n",
            "904 0.04960453882813454\n",
            "905 0.049499303102493286\n",
            "906 0.04939024895429611\n",
            "907 0.0492805540561676\n",
            "908 0.04917312040925026\n",
            "909 0.04906774312257767\n",
            "910 0.04895823076367378\n",
            "911 0.048852335661649704\n",
            "912 0.048745296895504\n",
            "913 0.048639483749866486\n",
            "914 0.04853753745555878\n",
            "915 0.048429232090711594\n",
            "916 0.04832789674401283\n",
            "917 0.04822160303592682\n",
            "918 0.04811921715736389\n",
            "919 0.04801739379763603\n",
            "920 0.04791382700204849\n",
            "921 0.04780949279665947\n",
            "922 0.04770843684673309\n",
            "923 0.047606758773326874\n",
            "924 0.047507863491773605\n",
            "925 0.047406625002622604\n",
            "926 0.04730788990855217\n",
            "927 0.04720855876803398\n",
            "928 0.04710812866687775\n",
            "929 0.047013260424137115\n",
            "930 0.04691049084067345\n",
            "931 0.04681331291794777\n",
            "932 0.04671628773212433\n",
            "933 0.046618979424238205\n",
            "934 0.04652198404073715\n",
            "935 0.04642454534769058\n",
            "936 0.04632806405425072\n",
            "937 0.04623223841190338\n",
            "938 0.046138614416122437\n",
            "939 0.04604286327958107\n",
            "940 0.04594646021723747\n",
            "941 0.04585444554686546\n",
            "942 0.04576012119650841\n",
            "943 0.04566575214266777\n",
            "944 0.04557356610894203\n",
            "945 0.045484758913517\n",
            "946 0.045387547463178635\n",
            "947 0.04529600217938423\n",
            "948 0.04520491510629654\n",
            "949 0.04511570185422897\n",
            "950 0.04502132534980774\n",
            "951 0.04493182525038719\n",
            "952 0.0448409765958786\n",
            "953 0.04475192353129387\n",
            "954 0.04466370865702629\n",
            "955 0.044572848826646805\n",
            "956 0.044485267251729965\n",
            "957 0.04439456760883331\n",
            "958 0.04430726543068886\n",
            "959 0.04421810060739517\n",
            "960 0.0441320575773716\n",
            "961 0.044043730944395065\n",
            "962 0.043956514447927475\n",
            "963 0.04387165233492851\n",
            "964 0.043784067034721375\n",
            "965 0.04369831085205078\n",
            "966 0.043614063411951065\n",
            "967 0.04352806508541107\n",
            "968 0.04344252496957779\n",
            "969 0.04335734248161316\n",
            "970 0.043274570256471634\n",
            "971 0.04318957030773163\n",
            "972 0.04310370981693268\n",
            "973 0.04302290454506874\n",
            "974 0.042939577251672745\n",
            "975 0.04285597428679466\n",
            "976 0.04277472198009491\n",
            "977 0.04269135370850563\n",
            "978 0.042609695345163345\n",
            "979 0.04252784699201584\n",
            "980 0.0424458384513855\n",
            "981 0.04236704856157303\n",
            "982 0.04228590056300163\n",
            "983 0.04220546782016754\n",
            "984 0.042124778032302856\n",
            "985 0.04204589128494263\n",
            "986 0.04196461662650108\n",
            "987 0.04188692569732666\n",
            "988 0.041808128356933594\n",
            "989 0.04172815382480621\n",
            "990 0.04165128991007805\n",
            "991 0.04157223924994469\n",
            "992 0.04149390757083893\n",
            "993 0.04141604155302048\n",
            "994 0.041338346898555756\n",
            "995 0.0412614531815052\n",
            "996 0.041184570640325546\n",
            "997 0.04110749065876007\n",
            "998 0.041031740605831146\n",
            "999 0.040957558900117874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# logits = gpt(x)\n",
        "\n",
        "# probs = nn.functional.softmax(logits, dim=-1)\n",
        "# t = torch.multinomial(probs[0], num_samples=1).item()\n",
        "# t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeQ8XK-Pzm0T",
        "outputId": "a95b4329-ba01-4d26-d2fd-6123fb5b86d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "459"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### sampling from the probability distribution."
      ],
      "metadata": {
        "id": "cfSY4mJ9QyQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y[:3]\n",
        "p = torch.tensor(y[:2], dtype=torch.long)[None, ...]\n",
        "logits = gpt(p)\n",
        "\n",
        "probs = nn.functional.softmax(logits, dim=-1)\n",
        "t = torch.multinomial(probs[0], num_samples=1).item() \n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBiwsE21GLSq",
        "outputId": "29f572dd-2a90-4cbc-ce0a-999b2de2af7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-138-8816f5892bbb>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  p = torch.tensor(y[:2], dtype=torch.long)[None, ...]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "_14rkUwldTJw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TwBt7yLON1P3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}