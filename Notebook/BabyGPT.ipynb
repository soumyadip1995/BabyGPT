{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfeao0DPH0zSQ2xwkDk43B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/language-models/blob/main/Notebook/BabyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "from math import sqrt\n",
        "import torch.nn as nn \n",
        "\n",
        "\n",
        "\n",
        "words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "# words[:20]\n",
        "\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "# print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "# print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "# print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "# print(data)\n",
        "# data.size()\n",
        "\n",
        "## block_size and batch size has been changed from 64 and 512 to 32 and 128\n",
        "block_size = 16\n",
        "batch_size = 32\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "## hidden dimensionality has been changed from 512 to 128.\n",
        "\n",
        "vocab_size = len(chars)\n",
        "d_k = 32\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "\n",
        "\n",
        "x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "input_embeds = token_emb(x)\n",
        "# input_embeds.size()\n",
        "\n",
        "\n",
        "def scaled_dot_product(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)\n",
        "\n",
        "key = input_embeds\n",
        "query = input_embeds\n",
        "value = input_embeds\n",
        "\n",
        "# sdp = scaled_dot_product(query, key, value)\n",
        "# print(sdp.size())\n",
        "\n",
        "### Multi headed attention\n",
        "\n",
        "\"\"\"Having many heads allows the model to focus on different parts of the sentences. \n",
        "The softmax on one head tends to focus on one aspect of similarity. For example subject verb interaction.\"\"\"\n",
        "## A single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# multihead_attention = MultiHeadAttention(128, 8)\n",
        "# # multihead_attention\n",
        "\n",
        "# attention_outputs =  multihead_attention(input_embeds)\n",
        "# # print(attention_outputs.size())\n",
        "\n",
        "\n",
        "# from karpathy , partially\n",
        "dropout = 0.2\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "### A simple Transformer Block    \n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size,  embedded_dim, num_heads, d_k):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, d_k)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, d_k)\n",
        "    self.layers1 = nn.ModuleList([TransformerBlock(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    tok_emb = self.token(x)\n",
        "    position_ids = torch.arange(x.size(-1), dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "      x = self.ln_f(x)\n",
        "    logits = self.ln_head(x)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "mOdus5D9uFKq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 62.6M parametres\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, 32, 8, 32)\n",
        "d = gpt(x)\n",
        "d.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk_1wRLcHSP_",
        "outputId": "78389678-2cdc-4e7a-f7d0-2d50878ddcef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 16, 478])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1xqB6Arc73u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}