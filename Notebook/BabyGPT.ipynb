{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTsVjq5pEESx3uBPXwj9Ls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/language-models/blob/main/Notebook/BabyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "from math import sqrt\n",
        "import torch.nn as nn \n",
        "\n",
        "\n",
        "\n",
        "words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "# words[:20]\n",
        "\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "# print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "# print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "# print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "# print(data)\n",
        "# data.size()\n",
        "\n",
        "## block_size and batch size has been changed from 64 and 512 to 32 and 128\n",
        "block_size = 16\n",
        "batch_size = 32\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "## hidden dimensionality has been changed from 512 to 128.\n",
        "\n",
        "vocab_size = len(chars)\n",
        "d_k = 32\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "\n",
        "\n",
        "x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "input_embeds = token_emb(x)\n",
        "# input_embeds.size()\n",
        "\n",
        "\n",
        "def scaled_dot_product(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)\n",
        "\n",
        "key = input_embeds\n",
        "query = input_embeds\n",
        "value = input_embeds\n",
        "\n",
        "# sdp = scaled_dot_product(query, key, value)\n",
        "# print(sdp.size())\n",
        "\n",
        "### Multi headed attention\n",
        "\n",
        "\"\"\"Having many heads allows the model to focus on different parts of the sentences. \n",
        "The softmax on one head tends to focus on one aspect of similarity. For example subject verb interaction.\"\"\"\n",
        "## A single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# multihead_attention = MultiHeadAttention(128, 8)\n",
        "# # multihead_attention\n",
        "\n",
        "# attention_outputs =  multihead_attention(input_embeds)\n",
        "# # print(attention_outputs.size())\n",
        "\n",
        "\n",
        "# from karpathy , partially\n",
        "dropout = 0.2\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "### A simple Transformer Block    \n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = MultiHeadAttention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mOdus5D9uFKq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, d_k):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, d_k)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, d_k)\n",
        "    self.layers1 = nn.ModuleList([TransformerBlock(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('q.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(x)\n",
        "    position_ids = torch.arange(x.size(-1), dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "      x = self.ln_f(x)\n",
        "    logits = self.ln_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "uiePG-vfm8Hx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## number of parameters: 117,187\n",
        "num_layers = 3\n",
        "gpt = BabyGPTmodel(num_layers, vocab_size, block_size, 32, 8, 32)\n",
        "# d = gpt(x)\n",
        "# d.size()"
      ],
      "metadata": {
        "id": "Qk_1wRLcHSP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F \n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Attention, self).__init__()\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "W1xqB6Arc73u"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(3, 3, 16)\n",
        "\n",
        "# att = Attention(16, 8)\n",
        "# tt = att(x)\n",
        "# tt.size()"
      ],
      "metadata": {
        "id": "8fgGPCd9Yxbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "X7zzfrdObFZL"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed_forward = FeedForward(embedded_dim = 16)\n",
        "# # feed_forward\n",
        "# ff_outputs = feed_forward(tt)\n",
        "# ff_outputs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQlwF5bbYAz",
        "outputId": "976c7577-da62-4c91-b5aa-8ab53c1f3648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "39qXw2CmbpSO"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btt = Transformer(16, 8)\n",
        "# to = btt(x)\n",
        "# print(to.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKLZmauscJiK",
        "outputId": "08fd3c82-8e42-43fc-dba0-313703edda55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer \n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "XuvEumpqcdwU"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# btt = Transformer(16, 8)\n",
        "# to = btt(x)\n",
        "# print(to.size())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9iJ54aRgZDI",
        "outputId": "31e50bd1-d388-4f84-d031-198559eb9f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size =  4\n",
        "# block_size = 4\n",
        "# embedded_dim = 16\n",
        "# num_heads = 4\n",
        "# num_layers = 4\n",
        "# gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "# # number of parameters: 13,315"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br5cx40HjsjY",
        "outputId": "27335a6c-d765-4ab6-c701-751caf51eaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 13315\n",
            "number of parameters: 13315\n",
            "number of parameters: 13315\n",
            "number of parameters: 13315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seq = list(map(int, \"1111011110111101101\"))\n",
        "# seq"
      ],
      "metadata": {
        "id": "J2R76Xsfnv8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## from karpathy\n",
        "# # convert the sequence to a tensor holding all the individual examples in that sequence\n",
        "# X, Y = [], []\n",
        "# # iterate over the sequence and grab every consecutive 3 bits\n",
        "# # the correct label for what's next is the next bit at each position\n",
        "# for i in range(len(seq) - block_size):\n",
        "#     X.append(seq[i:i+block_size])\n",
        "#     Y.append(seq[i+block_size])\n",
        "#     print(f\"example {i+1:2d}: {X[-1]} --> {Y[-1]}\")\n",
        "# X = torch.tensor(X,  dtype=torch.long)\n",
        "# Y = torch.tensor(Y,  dtype=torch.long)\n",
        "# # print(X.shape, Y.shape)\n",
        "# # print(X.size(), Y.size())\n",
        "\n",
        "# print(X, Y)"
      ],
      "metadata": {
        "id": "2MHJ-fZ0n5c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "id": "fdEuygOMouVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(100):\n",
        "#     logits = gpt(X)\n",
        "#     loss = F.cross_entropy(logits, Y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     optimizer.zero_grad()\n",
        "#     print(i, loss.item())"
      ],
      "metadata": {
        "id": "--b_jDhDoykq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n"
      ],
      "metadata": {
        "id": "_19368fGf9zZ"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+block_size] for i in ix])\n",
        "print((x, y))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwhLy0v-l2I7",
        "outputId": "f1037b6e-efd4-49d4-8cb8-50778d2a7839"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 44, 373,  29, 194],\n",
            "        [253, 228, 321, 387],\n",
            "        [ 98, 193, 475, 118],\n",
            "        [451, 285, 238,  20],\n",
            "        [ 52, 185, 470, 358],\n",
            "        [318, 308, 105,  97],\n",
            "        [281, 101,  37, 418],\n",
            "        [ 29, 467, 244, 312],\n",
            "        [470,   4, 470, 403],\n",
            "        [ 75, 256, 407, 389],\n",
            "        [445, 226,  29, 256],\n",
            "        [339, 136, 436, 295],\n",
            "        [295, 131, 295, 323],\n",
            "        [308, 105,  97,  84],\n",
            "        [106, 382, 288,  91],\n",
            "        [206, 249,   4,  29]]), tensor([234, 110,  29, 470, 241,  84, 133,  76, 244, 122, 470, 459,  91,  65,\n",
            "        393, 399]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "block_size = 4\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "## number of parameters: 28,990"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KO184DtqEFe",
        "outputId": "b44d2969-cd37-4a85-f4a6-76584c12237c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 28990\n",
            "number of parameters: 28990\n",
            "number of parameters: 28990\n",
            "number of parameters: 28990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]"
      ],
      "metadata": {
        "id": "2GxxoDMvgQ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d = torch.tensor(train_data.shape, dtype = torch.long)\n",
        "# p = torch.tensor(val_data.shape, dtype = torch.long)\n",
        "# p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9O0JKqhsmN",
        "outputId": "9caff398-cc88-4b7f-a6eb-75aacfbf8ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([135])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # batch_size = 32\n",
        "# input = torch.randn(d.shape, embedded_dim)\n",
        "# target = torch.tensor(p.shape)\n",
        "# print(input.shape, target.shape)"
      ],
      "metadata": {
        "id": "2mOtGcYLgqzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "id": "-XYEFDhqqk2-"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    logits = gpt(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(i, loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG2r1I4PjCFj",
        "outputId": "093c4fed-dbed-4e1b-f536-d4a9c304a02f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 6.1647467613220215\n",
            "1 6.0855512619018555\n",
            "2 6.0388503074646\n",
            "3 6.0027546882629395\n",
            "4 5.960081577301025\n",
            "5 5.9288225173950195\n",
            "6 5.896416187286377\n",
            "7 5.863935470581055\n",
            "8 5.842287540435791\n",
            "9 5.814393520355225\n",
            "10 5.78857421875\n",
            "11 5.760965347290039\n",
            "12 5.739848613739014\n",
            "13 5.711612701416016\n",
            "14 5.6829304695129395\n",
            "15 5.663541793823242\n",
            "16 5.634166717529297\n",
            "17 5.610830307006836\n",
            "18 5.582950592041016\n",
            "19 5.55332088470459\n",
            "20 5.525705337524414\n",
            "21 5.5001301765441895\n",
            "22 5.474945545196533\n",
            "23 5.444350719451904\n",
            "24 5.414790153503418\n",
            "25 5.38487434387207\n",
            "26 5.357189655303955\n",
            "27 5.3289289474487305\n",
            "28 5.299066066741943\n",
            "29 5.267841815948486\n",
            "30 5.234304428100586\n",
            "31 5.203612327575684\n",
            "32 5.1714630126953125\n",
            "33 5.140346527099609\n",
            "34 5.107305526733398\n",
            "35 5.0787858963012695\n",
            "36 5.044053554534912\n",
            "37 5.011567115783691\n",
            "38 4.977425575256348\n",
            "39 4.942967891693115\n",
            "40 4.910092830657959\n",
            "41 4.8771162033081055\n",
            "42 4.8441596031188965\n",
            "43 4.807588577270508\n",
            "44 4.772795677185059\n",
            "45 4.738954544067383\n",
            "46 4.704535961151123\n",
            "47 4.668580055236816\n",
            "48 4.631651401519775\n",
            "49 4.5983567237854\n",
            "50 4.561915397644043\n",
            "51 4.526247024536133\n",
            "52 4.49081563949585\n",
            "53 4.455860137939453\n",
            "54 4.419033050537109\n",
            "55 4.383411884307861\n",
            "56 4.348185062408447\n",
            "57 4.313320636749268\n",
            "58 4.279051303863525\n",
            "59 4.241971492767334\n",
            "60 4.207936763763428\n",
            "61 4.170825004577637\n",
            "62 4.135182857513428\n",
            "63 4.099650859832764\n",
            "64 4.0649871826171875\n",
            "65 4.0313005447387695\n",
            "66 3.998537063598633\n",
            "67 3.9609365463256836\n",
            "68 3.9263622760772705\n",
            "69 3.893320083618164\n",
            "70 3.85738205909729\n",
            "71 3.823812484741211\n",
            "72 3.7904772758483887\n",
            "73 3.7592153549194336\n",
            "74 3.725529193878174\n",
            "75 3.691765546798706\n",
            "76 3.6598548889160156\n",
            "77 3.627715587615967\n",
            "78 3.597505807876587\n",
            "79 3.5648574829101562\n",
            "80 3.5332205295562744\n",
            "81 3.5031309127807617\n",
            "82 3.471472978591919\n",
            "83 3.441662549972534\n",
            "84 3.412412405014038\n",
            "85 3.3821637630462646\n",
            "86 3.353949546813965\n",
            "87 3.3253519535064697\n",
            "88 3.297471284866333\n",
            "89 3.269744634628296\n",
            "90 3.241762399673462\n",
            "91 3.2132296562194824\n",
            "92 3.186490774154663\n",
            "93 3.1614294052124023\n",
            "94 3.133168935775757\n",
            "95 3.1073036193847656\n",
            "96 3.080878257751465\n",
            "97 3.0560522079467773\n",
            "98 3.0311458110809326\n",
            "99 3.0068609714508057\n",
            "100 2.9826819896698\n",
            "101 2.9571194648742676\n",
            "102 2.93420672416687\n",
            "103 2.9089648723602295\n",
            "104 2.8859996795654297\n",
            "105 2.8629517555236816\n",
            "106 2.8420190811157227\n",
            "107 2.817035675048828\n",
            "108 2.7949469089508057\n",
            "109 2.773223400115967\n",
            "110 2.751453161239624\n",
            "111 2.7293593883514404\n",
            "112 2.7076730728149414\n",
            "113 2.6858763694763184\n",
            "114 2.665933609008789\n",
            "115 2.6451964378356934\n",
            "116 2.623549222946167\n",
            "117 2.6041805744171143\n",
            "118 2.582406759262085\n",
            "119 2.562926769256592\n",
            "120 2.5435049533843994\n",
            "121 2.522728681564331\n",
            "122 2.5042099952697754\n",
            "123 2.4808690547943115\n",
            "124 2.462116241455078\n",
            "125 2.44124436378479\n",
            "126 2.4214892387390137\n",
            "127 2.405979871749878\n",
            "128 2.3862831592559814\n",
            "129 2.3655834197998047\n",
            "130 2.346677541732788\n",
            "131 2.328272581100464\n",
            "132 2.308992385864258\n",
            "133 2.2893922328948975\n",
            "134 2.272453546524048\n",
            "135 2.252891778945923\n",
            "136 2.2356650829315186\n",
            "137 2.214404582977295\n",
            "138 2.197305202484131\n",
            "139 2.1784934997558594\n",
            "140 2.160841941833496\n",
            "141 2.1422572135925293\n",
            "142 2.122878074645996\n",
            "143 2.104003429412842\n",
            "144 2.0883078575134277\n",
            "145 2.0696604251861572\n",
            "146 2.051041841506958\n",
            "147 2.0343708992004395\n",
            "148 2.0159826278686523\n",
            "149 2.000575542449951\n",
            "150 1.9820243120193481\n",
            "151 1.9636036157608032\n",
            "152 1.9455478191375732\n",
            "153 1.9301080703735352\n",
            "154 1.9104547500610352\n",
            "155 1.8943448066711426\n",
            "156 1.8768982887268066\n",
            "157 1.8600373268127441\n",
            "158 1.8422900438308716\n",
            "159 1.8266475200653076\n",
            "160 1.8097658157348633\n",
            "161 1.7924598455429077\n",
            "162 1.7766438722610474\n",
            "163 1.7583421468734741\n",
            "164 1.7427928447723389\n",
            "165 1.7249882221221924\n",
            "166 1.7097680568695068\n",
            "167 1.691713571548462\n",
            "168 1.6769384145736694\n",
            "169 1.6607670783996582\n",
            "170 1.6436352729797363\n",
            "171 1.628566861152649\n",
            "172 1.6123466491699219\n",
            "173 1.5969953536987305\n",
            "174 1.5804277658462524\n",
            "175 1.5651745796203613\n",
            "176 1.5496480464935303\n",
            "177 1.5341051816940308\n",
            "178 1.5183581113815308\n",
            "179 1.5027251243591309\n",
            "180 1.487300992012024\n",
            "181 1.4736512899398804\n",
            "182 1.4581769704818726\n",
            "183 1.4425183534622192\n",
            "184 1.4275141954421997\n",
            "185 1.413059115409851\n",
            "186 1.3981987237930298\n",
            "187 1.382454752922058\n",
            "188 1.3692890405654907\n",
            "189 1.3548064231872559\n",
            "190 1.3381481170654297\n",
            "191 1.326500415802002\n",
            "192 1.3108915090560913\n",
            "193 1.2966558933258057\n",
            "194 1.2829958200454712\n",
            "195 1.2695460319519043\n",
            "196 1.2546734809875488\n",
            "197 1.2410454750061035\n",
            "198 1.2279428243637085\n",
            "199 1.21491277217865\n",
            "200 1.2015656232833862\n",
            "201 1.1879786252975464\n",
            "202 1.173705816268921\n",
            "203 1.1616175174713135\n",
            "204 1.1478427648544312\n",
            "205 1.1351091861724854\n",
            "206 1.1222695112228394\n",
            "207 1.1105083227157593\n",
            "208 1.0975770950317383\n",
            "209 1.0839735269546509\n",
            "210 1.0725218057632446\n",
            "211 1.059395670890808\n",
            "212 1.0475823879241943\n",
            "213 1.0367778539657593\n",
            "214 1.0228691101074219\n",
            "215 1.011831283569336\n",
            "216 1.0006952285766602\n",
            "217 0.9890561103820801\n",
            "218 0.976599395275116\n",
            "219 0.9660013318061829\n",
            "220 0.9535385370254517\n",
            "221 0.9426913857460022\n",
            "222 0.932520866394043\n",
            "223 0.9230252504348755\n",
            "224 0.911315381526947\n",
            "225 0.9000114798545837\n",
            "226 0.8890951871871948\n",
            "227 0.8783184885978699\n",
            "228 0.8680440783500671\n",
            "229 0.8583462834358215\n",
            "230 0.8484891653060913\n",
            "231 0.8379020690917969\n",
            "232 0.8271515369415283\n",
            "233 0.8181174993515015\n",
            "234 0.8087332248687744\n",
            "235 0.7984774112701416\n",
            "236 0.788764476776123\n",
            "237 0.7802802324295044\n",
            "238 0.7703005075454712\n",
            "239 0.7607496976852417\n",
            "240 0.7518784403800964\n",
            "241 0.7431157231330872\n",
            "242 0.7346933484077454\n",
            "243 0.7255428433418274\n",
            "244 0.7171918153762817\n",
            "245 0.7083697319030762\n",
            "246 0.6998246908187866\n",
            "247 0.6917385458946228\n",
            "248 0.6833145022392273\n",
            "249 0.6753177046775818\n",
            "250 0.6670955419540405\n",
            "251 0.6592919230461121\n",
            "252 0.6516043543815613\n",
            "253 0.6445876359939575\n",
            "254 0.6362857818603516\n",
            "255 0.6289553642272949\n",
            "256 0.6208696961402893\n",
            "257 0.6141439080238342\n",
            "258 0.6070286631584167\n",
            "259 0.599499523639679\n",
            "260 0.5921828746795654\n",
            "261 0.5859382152557373\n",
            "262 0.5789040923118591\n",
            "263 0.5721572041511536\n",
            "264 0.5650059580802917\n",
            "265 0.5586639046669006\n",
            "266 0.5521204471588135\n",
            "267 0.5462049245834351\n",
            "268 0.5391643643379211\n",
            "269 0.5329165458679199\n",
            "270 0.5274981260299683\n",
            "271 0.5205760598182678\n",
            "272 0.5156844854354858\n",
            "273 0.5091898441314697\n",
            "274 0.5031086206436157\n",
            "275 0.4973357021808624\n",
            "276 0.4916131794452667\n",
            "277 0.48567479848861694\n",
            "278 0.4805251359939575\n",
            "279 0.4751536250114441\n",
            "280 0.4699069857597351\n",
            "281 0.46448448300361633\n",
            "282 0.45968952775001526\n",
            "283 0.45391514897346497\n",
            "284 0.4490954875946045\n",
            "285 0.4444376528263092\n",
            "286 0.4392834007740021\n",
            "287 0.4341300427913666\n",
            "288 0.43012964725494385\n",
            "289 0.4248737394809723\n",
            "290 0.4202462136745453\n",
            "291 0.41604912281036377\n",
            "292 0.41124197840690613\n",
            "293 0.40665167570114136\n",
            "294 0.4024633467197418\n",
            "295 0.3982456922531128\n",
            "296 0.393827885389328\n",
            "297 0.3892686069011688\n",
            "298 0.3856113851070404\n",
            "299 0.38153132796287537\n",
            "300 0.37715253233909607\n",
            "301 0.3731195032596588\n",
            "302 0.369304358959198\n",
            "303 0.3655655086040497\n",
            "304 0.3618500828742981\n",
            "305 0.3578523099422455\n",
            "306 0.3544692099094391\n",
            "307 0.35062503814697266\n",
            "308 0.3471224308013916\n",
            "309 0.34344184398651123\n",
            "310 0.33992135524749756\n",
            "311 0.33667901158332825\n",
            "312 0.33317989110946655\n",
            "313 0.3298685550689697\n",
            "314 0.3266247808933258\n",
            "315 0.3234036862850189\n",
            "316 0.32015419006347656\n",
            "317 0.3168589472770691\n",
            "318 0.31393104791641235\n",
            "319 0.310857892036438\n",
            "320 0.3077448308467865\n",
            "321 0.3048008680343628\n",
            "322 0.301736980676651\n",
            "323 0.29887524247169495\n",
            "324 0.29629936814308167\n",
            "325 0.2932327687740326\n",
            "326 0.29049116373062134\n",
            "327 0.2876426875591278\n",
            "328 0.28505969047546387\n",
            "329 0.28232017159461975\n",
            "330 0.27977749705314636\n",
            "331 0.277184396982193\n",
            "332 0.2747255265712738\n",
            "333 0.272183358669281\n",
            "334 0.26971355080604553\n",
            "335 0.2671669125556946\n",
            "336 0.26476648449897766\n",
            "337 0.2623366117477417\n",
            "338 0.25997817516326904\n",
            "339 0.2577519416809082\n",
            "340 0.25560522079467773\n",
            "341 0.25318586826324463\n",
            "342 0.2508874833583832\n",
            "343 0.24886462092399597\n",
            "344 0.24664005637168884\n",
            "345 0.24436350166797638\n",
            "346 0.242405965924263\n",
            "347 0.2402254343032837\n",
            "348 0.23823973536491394\n",
            "349 0.2361457198858261\n",
            "350 0.23418067395687103\n",
            "351 0.23233424127101898\n",
            "352 0.23052622377872467\n",
            "353 0.22841063141822815\n",
            "354 0.22640033066272736\n",
            "355 0.22453758120536804\n",
            "356 0.22275598347187042\n",
            "357 0.2208576649427414\n",
            "358 0.21901755034923553\n",
            "359 0.21722891926765442\n",
            "360 0.21559970080852509\n",
            "361 0.21383488178253174\n",
            "362 0.21214649081230164\n",
            "363 0.21038123965263367\n",
            "364 0.20877519249916077\n",
            "365 0.20706142485141754\n",
            "366 0.20538069307804108\n",
            "367 0.20378834009170532\n",
            "368 0.20222848653793335\n",
            "369 0.20072755217552185\n",
            "370 0.19911645352840424\n",
            "371 0.1974932700395584\n",
            "372 0.19602903723716736\n",
            "373 0.19455461204051971\n",
            "374 0.19304080307483673\n",
            "375 0.19154906272888184\n",
            "376 0.19014377892017365\n",
            "377 0.18878485262393951\n",
            "378 0.18729421496391296\n",
            "379 0.18594177067279816\n",
            "380 0.18453332781791687\n",
            "381 0.18315285444259644\n",
            "382 0.18181215226650238\n",
            "383 0.18052972853183746\n",
            "384 0.17930927872657776\n",
            "385 0.17796114087104797\n",
            "386 0.17663228511810303\n",
            "387 0.17531315982341766\n",
            "388 0.17407850921154022\n",
            "389 0.17284806072711945\n",
            "390 0.17162877321243286\n",
            "391 0.17047329246997833\n",
            "392 0.16928300261497498\n",
            "393 0.16804322600364685\n",
            "394 0.16685780882835388\n",
            "395 0.16576911509037018\n",
            "396 0.16463088989257812\n",
            "397 0.16346235573291779\n",
            "398 0.1623520404100418\n",
            "399 0.1613200157880783\n",
            "400 0.16013522446155548\n",
            "401 0.15909190475940704\n",
            "402 0.15798667073249817\n",
            "403 0.15693843364715576\n",
            "404 0.15592417120933533\n",
            "405 0.15485765039920807\n",
            "406 0.15383104979991913\n",
            "407 0.15280820429325104\n",
            "408 0.15185989439487457\n",
            "409 0.15084153413772583\n",
            "410 0.1498631089925766\n",
            "411 0.1488964855670929\n",
            "412 0.1479330211877823\n",
            "413 0.14700792729854584\n",
            "414 0.14602011442184448\n",
            "415 0.14509455859661102\n",
            "416 0.14419898390769958\n",
            "417 0.1432965248823166\n",
            "418 0.14238867163658142\n",
            "419 0.1414937674999237\n",
            "420 0.14061379432678223\n",
            "421 0.1397838592529297\n",
            "422 0.13885435461997986\n",
            "423 0.1380363553762436\n",
            "424 0.13720150291919708\n",
            "425 0.13633114099502563\n",
            "426 0.1355154663324356\n",
            "427 0.134693443775177\n",
            "428 0.13390067219734192\n",
            "429 0.13306279480457306\n",
            "430 0.13229593634605408\n",
            "431 0.13149674236774445\n",
            "432 0.1307411938905716\n",
            "433 0.12994833290576935\n",
            "434 0.1292436420917511\n",
            "435 0.12841737270355225\n",
            "436 0.12769679725170135\n",
            "437 0.12694703042507172\n",
            "438 0.1262458711862564\n",
            "439 0.12551183998584747\n",
            "440 0.12477870285511017\n",
            "441 0.12404397875070572\n",
            "442 0.12333329021930695\n",
            "443 0.12264732271432877\n",
            "444 0.12200670689344406\n",
            "445 0.12127787619829178\n",
            "446 0.12060606479644775\n",
            "447 0.1199391633272171\n",
            "448 0.11925610899925232\n",
            "449 0.11858442425727844\n",
            "450 0.11792711168527603\n",
            "451 0.11726175248622894\n",
            "452 0.11662463843822479\n",
            "453 0.11603755503892899\n",
            "454 0.11540994048118591\n",
            "455 0.11475840210914612\n",
            "456 0.11412500590085983\n",
            "457 0.11350420862436295\n",
            "458 0.11289957910776138\n",
            "459 0.11232027411460876\n",
            "460 0.11173910647630692\n",
            "461 0.11113961040973663\n",
            "462 0.11053133010864258\n",
            "463 0.10994980484247208\n",
            "464 0.1093970313668251\n",
            "465 0.10883474349975586\n",
            "466 0.10824093967676163\n",
            "467 0.10769587010145187\n",
            "468 0.10715954750776291\n",
            "469 0.10658217966556549\n",
            "470 0.10605184733867645\n",
            "471 0.10550671070814133\n",
            "472 0.10497689247131348\n",
            "473 0.10442239791154861\n",
            "474 0.10391055792570114\n",
            "475 0.10338403284549713\n",
            "476 0.10286865383386612\n",
            "477 0.1023690402507782\n",
            "478 0.10186822712421417\n",
            "479 0.1013268381357193\n",
            "480 0.10080964863300323\n",
            "481 0.10032506287097931\n",
            "482 0.09984885901212692\n",
            "483 0.09936272352933884\n",
            "484 0.09888014197349548\n",
            "485 0.09841491281986237\n",
            "486 0.0979088693857193\n",
            "487 0.09745533019304276\n",
            "488 0.09697271138429642\n",
            "489 0.09652676433324814\n",
            "490 0.09604756534099579\n",
            "491 0.09558567404747009\n",
            "492 0.09515951573848724\n",
            "493 0.09468323737382889\n",
            "494 0.09424746036529541\n",
            "495 0.09379680454730988\n",
            "496 0.0933789387345314\n",
            "497 0.09294918179512024\n",
            "498 0.09248059242963791\n",
            "499 0.09206659346818924\n",
            "500 0.09164634346961975\n",
            "501 0.09123946726322174\n",
            "502 0.0907939225435257\n",
            "503 0.09038854390382767\n",
            "504 0.08999615907669067\n",
            "505 0.08959116041660309\n",
            "506 0.08917130529880524\n",
            "507 0.08876775205135345\n",
            "508 0.08835499733686447\n",
            "509 0.0879804790019989\n",
            "510 0.08756951242685318\n",
            "511 0.08717598766088486\n",
            "512 0.086793914437294\n",
            "513 0.08641138672828674\n",
            "514 0.08604872971773148\n",
            "515 0.08567572385072708\n",
            "516 0.08528050780296326\n",
            "517 0.08492374420166016\n",
            "518 0.0845460444688797\n",
            "519 0.08417931944131851\n",
            "520 0.08382034301757812\n",
            "521 0.08344687521457672\n",
            "522 0.08311019092798233\n",
            "523 0.08275213837623596\n",
            "524 0.08239614963531494\n",
            "525 0.08204001188278198\n",
            "526 0.08170226216316223\n",
            "527 0.08135943114757538\n",
            "528 0.08100511878728867\n",
            "529 0.08066743612289429\n",
            "530 0.08033835887908936\n",
            "531 0.0799977108836174\n",
            "532 0.07966027408838272\n",
            "533 0.0793401226401329\n",
            "534 0.07900270819664001\n",
            "535 0.07867876440286636\n",
            "536 0.07836530357599258\n",
            "537 0.07805600017309189\n",
            "538 0.07771628350019455\n",
            "539 0.07741477340459824\n",
            "540 0.07709597051143646\n",
            "541 0.07677783817052841\n",
            "542 0.07646732777357101\n",
            "543 0.0761549100279808\n",
            "544 0.07585790753364563\n",
            "545 0.07555852085351944\n",
            "546 0.07526321709156036\n",
            "547 0.07495671510696411\n",
            "548 0.07465314120054245\n",
            "549 0.07435242086648941\n",
            "550 0.07406982034444809\n",
            "551 0.07376774400472641\n",
            "552 0.07349177449941635\n",
            "553 0.07320250570774078\n",
            "554 0.07291236519813538\n",
            "555 0.07264281064271927\n",
            "556 0.07234825938940048\n",
            "557 0.07207493484020233\n",
            "558 0.07180320471525192\n",
            "559 0.07151611894369125\n",
            "560 0.0712486058473587\n",
            "561 0.07098813354969025\n",
            "562 0.07070343941450119\n",
            "563 0.07044576108455658\n",
            "564 0.07017993181943893\n",
            "565 0.0699048712849617\n",
            "566 0.06964302808046341\n",
            "567 0.06937763839960098\n",
            "568 0.06913132965564728\n",
            "569 0.06886712461709976\n",
            "570 0.06860890239477158\n",
            "571 0.0683588832616806\n",
            "572 0.06811270862817764\n",
            "573 0.06785351037979126\n",
            "574 0.06759601086378098\n",
            "575 0.06737280637025833\n",
            "576 0.06711643189191818\n",
            "577 0.06686502695083618\n",
            "578 0.06662517786026001\n",
            "579 0.0663895234465599\n",
            "580 0.06615051627159119\n",
            "581 0.06590715050697327\n",
            "582 0.06566281616687775\n",
            "583 0.06543458253145218\n",
            "584 0.0651981458067894\n",
            "585 0.06497571617364883\n",
            "586 0.0647445023059845\n",
            "587 0.06451719254255295\n",
            "588 0.06428004056215286\n",
            "589 0.06404801458120346\n",
            "590 0.06382805854082108\n",
            "591 0.06360075622797012\n",
            "592 0.0633908286690712\n",
            "593 0.06316494941711426\n",
            "594 0.06294331699609756\n",
            "595 0.06272215396165848\n",
            "596 0.06251729279756546\n",
            "597 0.06229427456855774\n",
            "598 0.06207782030105591\n",
            "599 0.06186541169881821\n",
            "600 0.06165027618408203\n",
            "601 0.061453960835933685\n",
            "602 0.061236754059791565\n",
            "603 0.06102219223976135\n",
            "604 0.060823213309049606\n",
            "605 0.06061737611889839\n",
            "606 0.060416270047426224\n",
            "607 0.0602082759141922\n",
            "608 0.06002264469861984\n",
            "609 0.059816837310791016\n",
            "610 0.059620171785354614\n",
            "611 0.05941929295659065\n",
            "612 0.05921376496553421\n",
            "613 0.05901434272527695\n",
            "614 0.058837875723838806\n",
            "615 0.05863914638757706\n",
            "616 0.05844851955771446\n",
            "617 0.05826251208782196\n",
            "618 0.058059245347976685\n",
            "619 0.057873278856277466\n",
            "620 0.05769189074635506\n",
            "621 0.05749242380261421\n",
            "622 0.05731799826025963\n",
            "623 0.057121992111206055\n",
            "624 0.056936394423246384\n",
            "625 0.05675797164440155\n",
            "626 0.05658085644245148\n",
            "627 0.056398019194602966\n",
            "628 0.05622047930955887\n",
            "629 0.056035786867141724\n",
            "630 0.0558612234890461\n",
            "631 0.05568852648139\n",
            "632 0.05551255866885185\n",
            "633 0.05533652752637863\n",
            "634 0.05515503138303757\n",
            "635 0.054991208016872406\n",
            "636 0.054813992232084274\n",
            "637 0.05464506521821022\n",
            "638 0.05447594076395035\n",
            "639 0.05431581288576126\n",
            "640 0.0541440024971962\n",
            "641 0.053985875099897385\n",
            "642 0.05380643159151077\n",
            "643 0.053645405918359756\n",
            "644 0.053474899381399155\n",
            "645 0.05331346392631531\n",
            "646 0.053158026188611984\n",
            "647 0.05299132317304611\n",
            "648 0.05283313989639282\n",
            "649 0.052673351019620895\n",
            "650 0.05251649022102356\n",
            "651 0.05235620215535164\n",
            "652 0.052206993103027344\n",
            "653 0.05204994976520538\n",
            "654 0.05189278721809387\n",
            "655 0.05173379182815552\n",
            "656 0.051587797701358795\n",
            "657 0.051433224231004715\n",
            "658 0.051277369260787964\n",
            "659 0.051120825111866\n",
            "660 0.05097750946879387\n",
            "661 0.05082482099533081\n",
            "662 0.05068795382976532\n",
            "663 0.05052872747182846\n",
            "664 0.050384290516376495\n",
            "665 0.05023816227912903\n",
            "666 0.05009044334292412\n",
            "667 0.04995398223400116\n",
            "668 0.049810320138931274\n",
            "669 0.049669794738292694\n",
            "670 0.04952046647667885\n",
            "671 0.04937109351158142\n",
            "672 0.0492418110370636\n",
            "673 0.0490952804684639\n",
            "674 0.048953667283058167\n",
            "675 0.04881768301129341\n",
            "676 0.04867172986268997\n",
            "677 0.04853910580277443\n",
            "678 0.048399705439805984\n",
            "679 0.04826483875513077\n",
            "680 0.048129454255104065\n",
            "681 0.048000019043684006\n",
            "682 0.04786260426044464\n",
            "683 0.047731030732393265\n",
            "684 0.04758945852518082\n",
            "685 0.0474635474383831\n",
            "686 0.04733099043369293\n",
            "687 0.04720339924097061\n",
            "688 0.04707413539290428\n",
            "689 0.04694066196680069\n",
            "690 0.04681096225976944\n",
            "691 0.046687543392181396\n",
            "692 0.04655664414167404\n",
            "693 0.04643230140209198\n",
            "694 0.04629962518811226\n",
            "695 0.046178996562957764\n",
            "696 0.04604741930961609\n",
            "697 0.045923762023448944\n",
            "698 0.04580020159482956\n",
            "699 0.045682378113269806\n",
            "700 0.0455629788339138\n",
            "701 0.045438919216394424\n",
            "702 0.04531758651137352\n",
            "703 0.04518634453415871\n",
            "704 0.04507007822394371\n",
            "705 0.04494928941130638\n",
            "706 0.044834889471530914\n",
            "707 0.04471844062209129\n",
            "708 0.04459570720791817\n",
            "709 0.044479452073574066\n",
            "710 0.04436136782169342\n",
            "711 0.04425124078989029\n",
            "712 0.04413198307156563\n",
            "713 0.044011615216732025\n",
            "714 0.0438992939889431\n",
            "715 0.04378601163625717\n",
            "716 0.04367128387093544\n",
            "717 0.04355615749955177\n",
            "718 0.0434480756521225\n",
            "719 0.04333464801311493\n",
            "720 0.04322664067149162\n",
            "721 0.04312325268983841\n",
            "722 0.04300669580698013\n",
            "723 0.042893245816230774\n",
            "724 0.04278652369976044\n",
            "725 0.042677074670791626\n",
            "726 0.04256332665681839\n",
            "727 0.04246239364147186\n",
            "728 0.04235431179404259\n",
            "729 0.042241089046001434\n",
            "730 0.04213700443506241\n",
            "731 0.04203158617019653\n",
            "732 0.041923247277736664\n",
            "733 0.04182344675064087\n",
            "734 0.041716545820236206\n",
            "735 0.04161359369754791\n",
            "736 0.041508760303258896\n",
            "737 0.04140555113554001\n",
            "738 0.04130466282367706\n",
            "739 0.04120567440986633\n",
            "740 0.041096292436122894\n",
            "741 0.040995530784130096\n",
            "742 0.040899403393268585\n",
            "743 0.0407961830496788\n",
            "744 0.04070209711790085\n",
            "745 0.040595874190330505\n",
            "746 0.04050244763493538\n",
            "747 0.04039940610527992\n",
            "748 0.04030311480164528\n",
            "749 0.040206409990787506\n",
            "750 0.040106553584337234\n",
            "751 0.040010180324316025\n",
            "752 0.03992091119289398\n",
            "753 0.03981516882777214\n",
            "754 0.039718396961688995\n",
            "755 0.03962645307183266\n",
            "756 0.03953377157449722\n",
            "757 0.03943408280611038\n",
            "758 0.03934287279844284\n",
            "759 0.03924987465143204\n",
            "760 0.03915819525718689\n",
            "761 0.03906185179948807\n",
            "762 0.038970693945884705\n",
            "763 0.03888174891471863\n",
            "764 0.03879459947347641\n",
            "765 0.03869679570198059\n",
            "766 0.038609255105257034\n",
            "767 0.03851601481437683\n",
            "768 0.03842558339238167\n",
            "769 0.0383366234600544\n",
            "770 0.038251377642154694\n",
            "771 0.03816327825188637\n",
            "772 0.038070786744356155\n",
            "773 0.03798489645123482\n",
            "774 0.037893906235694885\n",
            "775 0.03780871629714966\n",
            "776 0.03771888464689255\n",
            "777 0.03763635829091072\n",
            "778 0.03754890337586403\n",
            "779 0.03746526688337326\n",
            "780 0.037378471344709396\n",
            "781 0.03729457035660744\n",
            "782 0.037209492176771164\n",
            "783 0.03712227940559387\n",
            "784 0.03703879192471504\n",
            "785 0.03695764020085335\n",
            "786 0.03687240928411484\n",
            "787 0.03678883612155914\n",
            "788 0.03670693188905716\n",
            "789 0.036626603454351425\n",
            "790 0.03654174134135246\n",
            "791 0.036460667848587036\n",
            "792 0.036379944533109665\n",
            "793 0.03629552945494652\n",
            "794 0.03621656820178032\n",
            "795 0.03613922372460365\n",
            "796 0.03605570271611214\n",
            "797 0.03597863391041756\n",
            "798 0.035896241664886475\n",
            "799 0.03581934794783592\n",
            "800 0.03574158996343613\n",
            "801 0.035669367760419846\n",
            "802 0.0355830118060112\n",
            "803 0.03550529107451439\n",
            "804 0.0354311428964138\n",
            "805 0.03535197302699089\n",
            "806 0.03527860343456268\n",
            "807 0.035200342535972595\n",
            "808 0.0351245254278183\n",
            "809 0.035048726946115494\n",
            "810 0.03497150167822838\n",
            "811 0.034899573773145676\n",
            "812 0.034821003675460815\n",
            "813 0.03474687784910202\n",
            "814 0.034672580659389496\n",
            "815 0.03459741920232773\n",
            "816 0.03452647477388382\n",
            "817 0.03445209935307503\n",
            "818 0.03437679260969162\n",
            "819 0.03430502116680145\n",
            "820 0.03423364460468292\n",
            "821 0.03416310250759125\n",
            "822 0.03408978506922722\n",
            "823 0.034021906554698944\n",
            "824 0.03394564613699913\n",
            "825 0.033872008323669434\n",
            "826 0.033801011741161346\n",
            "827 0.03373120352625847\n",
            "828 0.0336613804101944\n",
            "829 0.03359213098883629\n",
            "830 0.033521123230457306\n",
            "831 0.033454012125730515\n",
            "832 0.033381037414073944\n",
            "833 0.03331337496638298\n",
            "834 0.03324592858552933\n",
            "835 0.033176589757204056\n",
            "836 0.03310772776603699\n",
            "837 0.03304222598671913\n",
            "838 0.032968420535326004\n",
            "839 0.03290575370192528\n",
            "840 0.03283919394016266\n",
            "841 0.03277155011892319\n",
            "842 0.03270432725548744\n",
            "843 0.03263714537024498\n",
            "844 0.03257102146744728\n",
            "845 0.03250516578555107\n",
            "846 0.03244175389409065\n",
            "847 0.032374024391174316\n",
            "848 0.03231065720319748\n",
            "849 0.03224217891693115\n",
            "850 0.03217640891671181\n",
            "851 0.03211146593093872\n",
            "852 0.032051119953393936\n",
            "853 0.03198358044028282\n",
            "854 0.03192032501101494\n",
            "855 0.031857240945100784\n",
            "856 0.03179517015814781\n",
            "857 0.03173564374446869\n",
            "858 0.03166944161057472\n",
            "859 0.03160679340362549\n",
            "860 0.03154291957616806\n",
            "861 0.03148425370454788\n",
            "862 0.03142039477825165\n",
            "863 0.031359124928712845\n",
            "864 0.031294312328100204\n",
            "865 0.03123171627521515\n",
            "866 0.031173188239336014\n",
            "867 0.031113499775528908\n",
            "868 0.031051311641931534\n",
            "869 0.0309901125729084\n",
            "870 0.03093312680721283\n",
            "871 0.030874080955982208\n",
            "872 0.03081044927239418\n",
            "873 0.030749700963497162\n",
            "874 0.030693212524056435\n",
            "875 0.03063332289457321\n",
            "876 0.03057538904249668\n",
            "877 0.03052239492535591\n",
            "878 0.030456291511654854\n",
            "879 0.030400509014725685\n",
            "880 0.030347373336553574\n",
            "881 0.030283154919743538\n",
            "882 0.030225617811083794\n",
            "883 0.03016803041100502\n",
            "884 0.03011091612279415\n",
            "885 0.030055474489927292\n",
            "886 0.029996734112501144\n",
            "887 0.0299428291618824\n",
            "888 0.029882747679948807\n",
            "889 0.029828771948814392\n",
            "890 0.029773442074656487\n",
            "891 0.02971251681447029\n",
            "892 0.029659157618880272\n",
            "893 0.02960251271724701\n",
            "894 0.02954878658056259\n",
            "895 0.02949424646794796\n",
            "896 0.02943897433578968\n",
            "897 0.029384547844529152\n",
            "898 0.0293309073895216\n",
            "899 0.029275553300976753\n",
            "900 0.0292192529886961\n",
            "901 0.029167210683226585\n",
            "902 0.029111376032233238\n",
            "903 0.02905968949198723\n",
            "904 0.029008645564317703\n",
            "905 0.02895452082157135\n",
            "906 0.028899777680635452\n",
            "907 0.028848763555288315\n",
            "908 0.028793063014745712\n",
            "909 0.028744230046868324\n",
            "910 0.028691451996564865\n",
            "911 0.028635399416089058\n",
            "912 0.028582468628883362\n",
            "913 0.028536023572087288\n",
            "914 0.0284828282892704\n",
            "915 0.02843172289431095\n",
            "916 0.028378857299685478\n",
            "917 0.02833026461303234\n",
            "918 0.028277594596147537\n",
            "919 0.028226448222994804\n",
            "920 0.028175152838230133\n",
            "921 0.028124641627073288\n",
            "922 0.02807459607720375\n",
            "923 0.028027096763253212\n",
            "924 0.02797507680952549\n",
            "925 0.027925357222557068\n",
            "926 0.02787443809211254\n",
            "927 0.027824999764561653\n",
            "928 0.027780679985880852\n",
            "929 0.02772849053144455\n",
            "930 0.027677183970808983\n",
            "931 0.02762744016945362\n",
            "932 0.027581721544265747\n",
            "933 0.027532894164323807\n",
            "934 0.027486244216561317\n",
            "935 0.027436882257461548\n",
            "936 0.027387598529458046\n",
            "937 0.02734139747917652\n",
            "938 0.027291283011436462\n",
            "939 0.02724442072212696\n",
            "940 0.027197351679205894\n",
            "941 0.0271501112729311\n",
            "942 0.027103615924715996\n",
            "943 0.027057714760303497\n",
            "944 0.02700774557888508\n",
            "945 0.026962874457240105\n",
            "946 0.02691654860973358\n",
            "947 0.026871901005506516\n",
            "948 0.026823386549949646\n",
            "949 0.02677740715444088\n",
            "950 0.026731420308351517\n",
            "951 0.026686061173677444\n",
            "952 0.02664000913500786\n",
            "953 0.026594344526529312\n",
            "954 0.026551173999905586\n",
            "955 0.026504138484597206\n",
            "956 0.026459967717528343\n",
            "957 0.026416100561618805\n",
            "958 0.02637089416384697\n",
            "959 0.026326024904847145\n",
            "960 0.026280706748366356\n",
            "961 0.02623625285923481\n",
            "962 0.026192503049969673\n",
            "963 0.026149000972509384\n",
            "964 0.026103219017386436\n",
            "965 0.026062749326229095\n",
            "966 0.026015618816018105\n",
            "967 0.02597467042505741\n",
            "968 0.025929534807801247\n",
            "969 0.02588726580142975\n",
            "970 0.025843100622296333\n",
            "971 0.025800200179219246\n",
            "972 0.025757761672139168\n",
            "973 0.025714658200740814\n",
            "974 0.02567204274237156\n",
            "975 0.025631684809923172\n",
            "976 0.025587957352399826\n",
            "977 0.0255449116230011\n",
            "978 0.02550504170358181\n",
            "979 0.025461137294769287\n",
            "980 0.025423230603337288\n",
            "981 0.02537696436047554\n",
            "982 0.025337249040603638\n",
            "983 0.025296879932284355\n",
            "984 0.025253605097532272\n",
            "985 0.025213534012436867\n",
            "986 0.025172099471092224\n",
            "987 0.025132447481155396\n",
            "988 0.02509036473929882\n",
            "989 0.025050316005945206\n",
            "990 0.025009578093886375\n",
            "991 0.0249702837318182\n",
            "992 0.024929581210017204\n",
            "993 0.02488817274570465\n",
            "994 0.0248468779027462\n",
            "995 0.024809397757053375\n",
            "996 0.024768436327576637\n",
            "997 0.0247298963367939\n",
            "998 0.024689627811312675\n",
            "999 0.024652061983942986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# logits = gpt(x)\n",
        "\n",
        "# probs = nn.functional.softmax(logits, dim=-1)\n",
        "# t = torch.multinomial(probs[0], num_samples=1).item()\n",
        "# t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeQ8XK-Pzm0T",
        "outputId": "a95b4329-ba01-4d26-d2fd-6123fb5b86d9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "459"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### sampling from the probability distribution."
      ],
      "metadata": {
        "id": "cfSY4mJ9QyQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y[:3]\n",
        "p = torch.tensor(y[:2], dtype=torch.long)[None, ...]\n",
        "logits = gpt(p)\n",
        "\n",
        "probs = nn.functional.softmax(logits, dim=-1)\n",
        "t = torch.multinomial(probs[0], num_samples=1).item() \n",
        "t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBiwsE21GLSq",
        "outputId": "29f572dd-2a90-4cbc-ce0a-999b2de2af7c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-138-8816f5892bbb>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  p = torch.tensor(y[:2], dtype=torch.long)[None, ...]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    }
  ]
}