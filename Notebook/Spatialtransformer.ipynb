{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaEV04qmbDphTpk+dZxs2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/language-models/blob/main/Notebook/Spatialtransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "words = open(r\"/content/text.txt\", 'r' , encoding='utf-8').read().split()\n",
        "# words[:20]\n",
        "\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "# print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "# print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "# print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "# print(data)\n",
        "# data.size()\n",
        "\n",
        "## block_size and batch size has been changed from 64 and 512 to 32 and 128\n",
        "block_size = 32\n",
        "batch_size = 128\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "## hidden dimensionality has been changed from 512 to 128.\n",
        "\n",
        "vocab_size = len(chars)\n",
        "d_k = 128\n",
        "token_emb = nn.Embedding(vocab_size, d_k)\n",
        "\n",
        "\n",
        "x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "input_embeds = token_emb(x)\n",
        "# input_embeds.size()\n",
        "\n",
        "\n",
        "def scaled_dot_product(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)\n",
        "\n",
        "key = input_embeds\n",
        "query = input_embeds\n",
        "value = input_embeds\n",
        "\n",
        "# sdp = scaled_dot_product(query, key, value)\n",
        "# print(sdp.size())\n",
        "\n",
        "### Multi headed attention\n",
        "\n",
        "\"\"\"Having many heads allows the model to focus on different parts of the sentences. \n",
        "The softmax on one head tends to focus on one aspect of similarity. For example subject verb interaction.\"\"\"\n",
        "## A single attention head\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_outputs = scaled_dot_product(self.q(x), self.k(x), self.v(x))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# multihead_attention = MultiHeadAttention(128, 8)\n",
        "# # multihead_attention\n",
        "\n",
        "# attention_outputs =  multihead_attention(input_embeds)\n",
        "# # print(attention_outputs.size())\n",
        "\n",
        "\n",
        "# from karpathy , partially\n",
        "dropout = 0.2\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "### A simple Transformer Block    \n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = MultiHeadAttention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "btt = Transformer(128, 8)\n",
        "to = btt(input_embeds)\n",
        "print(to.size())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m85RSE70SO7_",
        "outputId": "1363caa7-0645-4214-ffe0-232a65e18168"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From [ldm](https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/attention.py)"
      ],
      "metadata": {
        "id": "OBSvflcVybol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_quzSQ8-yKgk",
        "outputId": "280cce7c-d176-4a37-fbd8-56adb6f5551f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spatial Attention Head"
      ],
      "metadata": {
        "id": "8TDXCy5qSnlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "from math import sqrt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "\n",
        "class SpatialAttentionHead(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super(SpatialAttentionHead, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "    self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "    self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "    self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "    self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
        "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
        "\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
        "        w_ = rearrange(w_, 'b i j -> b j i')\n",
        "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
        "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n"
      ],
      "metadata": {
        "id": "j_-ewxv65qKm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 32, 64, 64)\n",
        "\n",
        "stn = SpatialAttentionHead(32)\n",
        "st = stn(x)\n",
        "st[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soL0rDYv7UG2",
        "outputId": "5b256aa4-0659-4ab3-fbec-2ea6bcc15e30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 2.4144e+00, -6.2536e-01, -4.5577e-01,  ...,  9.1269e-01,\n",
              "           -5.6247e-01, -1.6154e+00],\n",
              "          [ 3.5866e-01, -3.4775e-01,  5.0385e-01,  ..., -3.4040e-01,\n",
              "           -2.5032e-01, -1.6667e+00],\n",
              "          [-1.3572e+00,  1.3564e+00,  1.2945e+00,  ..., -8.7441e-03,\n",
              "            1.8555e+00,  1.2009e+00],\n",
              "          ...,\n",
              "          [-1.7396e+00,  4.1048e-02, -1.3828e-01,  ..., -9.7439e-01,\n",
              "           -8.5793e-01,  6.3062e-01],\n",
              "          [-1.5686e+00,  1.2455e+00, -5.5006e-01,  ...,  4.4212e-01,\n",
              "            8.0098e-01,  3.0116e-01],\n",
              "          [-1.4802e-01,  2.1312e-01, -1.3096e+00,  ..., -9.9902e-01,\n",
              "            5.3561e-02, -2.5475e+00]],\n",
              "\n",
              "         [[-1.3619e+00, -8.7990e-02,  1.6321e+00,  ...,  4.3035e-01,\n",
              "            8.3480e-01, -9.4382e-01],\n",
              "          [-3.1477e-01, -6.3020e-01, -7.7731e-01,  ..., -8.1120e-01,\n",
              "           -5.2567e-01, -2.4942e-02],\n",
              "          [-1.2581e+00, -2.6787e+00,  8.3537e-01,  ..., -1.5512e+00,\n",
              "            3.9807e-01,  1.4524e-01],\n",
              "          ...,\n",
              "          [-1.7913e+00,  5.5662e-01,  1.0454e+00,  ...,  9.0468e-01,\n",
              "           -5.6642e-01,  5.2374e-01],\n",
              "          [ 7.3854e-01,  4.2809e-02,  1.6300e+00,  ...,  1.6032e-01,\n",
              "           -1.1295e+00, -7.8295e-01],\n",
              "          [-6.8788e-01, -5.8528e-01,  7.5094e-02,  ..., -5.9164e-01,\n",
              "           -4.4186e-01,  2.2397e-01]],\n",
              "\n",
              "         [[ 6.1946e-01,  8.5556e-01,  4.0021e-01,  ...,  1.6229e+00,\n",
              "            5.5838e-01,  1.1656e+00],\n",
              "          [-2.9463e-02, -1.0836e+00, -4.4458e-01,  ..., -1.1751e+00,\n",
              "            1.5961e+00,  9.9999e-01],\n",
              "          [-2.8318e-01,  6.7316e-01,  1.7666e+00,  ...,  1.2491e+00,\n",
              "            8.0393e-01, -1.5730e+00],\n",
              "          ...,\n",
              "          [ 3.7105e-01,  1.0032e+00,  1.6819e+00,  ...,  8.4668e-01,\n",
              "           -3.4881e-01,  8.6532e-01],\n",
              "          [-4.6564e-01,  4.3278e-01,  1.4087e+00,  ..., -7.7969e-01,\n",
              "            3.1385e-01, -1.2548e+00],\n",
              "          [-3.8190e-01,  1.0110e-01, -4.6241e-02,  ..., -1.4858e-01,\n",
              "            1.6591e+00, -1.4089e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 1.8510e+00,  1.2435e+00, -8.9401e-02,  ..., -3.3673e-02,\n",
              "            9.3541e-01, -9.0152e-01],\n",
              "          [-1.3170e+00, -1.9064e+00,  1.2207e-01,  ..., -3.0101e-01,\n",
              "            7.5909e-01,  2.5415e-01],\n",
              "          [ 3.8152e-02,  7.8191e-01, -1.4690e+00,  ..., -5.2527e-01,\n",
              "            2.6806e-01, -1.1569e+00],\n",
              "          ...,\n",
              "          [ 1.3339e+00,  1.5061e-01, -4.7534e-01,  ..., -3.3183e-01,\n",
              "           -2.3686e-01,  8.0100e-01],\n",
              "          [-7.0784e-01, -1.2790e+00,  1.6702e+00,  ..., -2.3100e-01,\n",
              "            1.8883e+00, -4.5125e-02],\n",
              "          [ 8.3336e-01,  2.8233e-03, -1.5862e+00,  ...,  1.2299e-02,\n",
              "            1.0344e+00,  3.7375e-01]],\n",
              "\n",
              "         [[ 1.3456e-01,  9.1554e-01, -3.1920e-01,  ...,  2.8424e+00,\n",
              "           -1.3580e+00,  8.2788e-01],\n",
              "          [ 3.9496e-01,  8.1219e-03,  1.3059e+00,  ..., -4.0099e-01,\n",
              "            1.2431e+00,  1.5693e+00],\n",
              "          [ 3.7889e-01, -8.1839e-01,  1.2332e+00,  ...,  1.8180e-01,\n",
              "            1.7945e-01,  9.3173e-01],\n",
              "          ...,\n",
              "          [ 1.4826e-01, -2.9970e-01,  3.0889e-01,  ...,  1.4228e+00,\n",
              "            2.4461e+00, -1.6415e-01],\n",
              "          [-6.1390e-01,  1.3801e+00,  9.2660e-01,  ..., -2.8063e-01,\n",
              "            3.8430e-01, -9.1934e-01],\n",
              "          [ 1.2765e+00, -6.9553e-01, -3.1130e-01,  ..., -1.0511e-01,\n",
              "            1.1799e+00,  1.3496e+00]],\n",
              "\n",
              "         [[ 1.0987e-03,  9.1874e-02, -5.1703e-01,  ..., -1.9733e+00,\n",
              "           -6.3479e-01,  6.7463e-01],\n",
              "          [-1.3174e+00,  1.2374e+00, -7.2559e-01,  ...,  2.2396e-01,\n",
              "           -9.0092e-01,  6.4632e-01],\n",
              "          [ 1.6219e-01, -3.4283e-01, -1.7969e-01,  ..., -1.1769e+00,\n",
              "            2.3005e-01,  1.3294e+00],\n",
              "          ...,\n",
              "          [-5.8649e-01, -1.3876e+00, -6.7544e-01,  ..., -9.8986e-01,\n",
              "            4.8883e-01, -5.0225e-03],\n",
              "          [-1.3528e+00, -1.3519e+00,  8.5919e-01,  ..., -1.6495e-02,\n",
              "           -9.5745e-02, -2.9377e-01],\n",
              "          [ 5.6872e-01,  5.1726e-01,  3.5812e-01,  ...,  3.2173e-01,\n",
              "           -1.4573e+00, -1.1863e+00]]],\n",
              "\n",
              "\n",
              "        [[[-1.5998e+00,  1.2381e+00,  1.4528e+00,  ...,  1.7347e-01,\n",
              "           -4.5031e-01,  6.8998e-01],\n",
              "          [-4.5609e-01, -1.9830e+00,  9.4918e-01,  ..., -1.7205e+00,\n",
              "           -1.8607e-01,  5.9098e-01],\n",
              "          [-1.6471e+00, -1.9099e+00,  6.6131e-01,  ..., -7.9404e-01,\n",
              "           -1.6162e+00, -1.3152e+00],\n",
              "          ...,\n",
              "          [ 3.2764e-02, -2.2229e-01,  1.0977e+00,  ..., -1.4434e-01,\n",
              "            1.4068e-01, -1.0235e+00],\n",
              "          [-1.3380e+00, -7.5103e-01, -4.6866e-01,  ..., -2.9766e-01,\n",
              "           -4.5710e-01, -2.0525e+00],\n",
              "          [-4.4736e-01, -1.2235e+00,  3.0351e+00,  ...,  9.2230e-01,\n",
              "            3.2256e-01,  3.9028e-01]],\n",
              "\n",
              "         [[-1.1509e+00, -1.6263e+00,  8.3301e-01,  ..., -1.5202e+00,\n",
              "            2.9851e-01, -3.2830e-02],\n",
              "          [-1.8688e+00, -3.4003e-01,  1.5141e+00,  ...,  1.0937e-01,\n",
              "           -9.2223e-02,  1.4360e-01],\n",
              "          [-1.9549e+00, -1.0313e+00,  8.6422e-01,  ...,  3.3300e-01,\n",
              "           -2.2844e+00,  6.9308e-01],\n",
              "          ...,\n",
              "          [ 1.2989e+00, -1.1744e-01,  1.3121e+00,  ...,  2.1608e+00,\n",
              "           -1.1479e-01, -8.7315e-01],\n",
              "          [-4.0547e-01,  8.4080e-01,  6.8027e-02,  ...,  4.6285e-01,\n",
              "           -1.7718e+00, -4.6772e-01],\n",
              "          [ 7.5627e-01, -1.2224e+00,  4.3010e-01,  ...,  1.1524e-01,\n",
              "            7.0211e-01,  7.1932e-01]],\n",
              "\n",
              "         [[-4.4166e-01,  1.5777e+00, -4.9255e-01,  ..., -1.5845e+00,\n",
              "            4.1658e-01, -2.8682e-01],\n",
              "          [ 1.1353e+00,  5.9855e-01,  7.8130e-01,  ...,  1.5749e+00,\n",
              "            4.3009e-01,  1.3496e+00],\n",
              "          [ 1.8574e-01,  1.4728e+00, -8.3464e-01,  ...,  4.4548e-01,\n",
              "           -8.9506e-01, -5.8915e-02],\n",
              "          ...,\n",
              "          [-6.1339e-02, -2.7172e-01,  5.4647e-01,  ...,  1.4631e-01,\n",
              "            4.7819e-02, -1.5130e+00],\n",
              "          [-1.2962e+00,  3.4752e-01, -1.2637e+00,  ...,  1.3392e+00,\n",
              "           -1.1733e+00,  6.4499e-01],\n",
              "          [ 8.7195e-02,  5.3687e-01, -9.2883e-01,  ...,  7.8960e-01,\n",
              "            1.0705e-01, -1.2620e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-1.4172e+00,  5.4243e-03,  9.7277e-03,  ...,  3.4401e-01,\n",
              "           -6.5810e-01, -3.0899e-02],\n",
              "          [-8.9021e-01,  2.0209e-01,  7.5123e-01,  ..., -6.3208e-01,\n",
              "            1.7746e+00, -1.0477e+00],\n",
              "          [-1.3099e+00,  9.7074e-01, -6.1504e-01,  ...,  3.6218e-02,\n",
              "           -2.4794e-01,  1.6931e+00],\n",
              "          ...,\n",
              "          [-1.1345e+00,  1.8737e+00, -1.0192e+00,  ...,  8.3028e-01,\n",
              "            6.1820e-01, -1.8599e-01],\n",
              "          [ 1.0283e+00, -7.6798e-01, -1.1760e+00,  ..., -9.0224e-01,\n",
              "            5.5215e-01,  1.7196e+00],\n",
              "          [-7.7561e-01, -8.9430e-01,  5.0098e-01,  ..., -1.0257e+00,\n",
              "            2.2464e+00, -1.1625e+00]],\n",
              "\n",
              "         [[ 1.6705e+00, -3.3049e-01,  7.2307e-02,  ..., -4.6541e-02,\n",
              "            8.6161e-01,  6.6854e-01],\n",
              "          [-1.0454e+00, -1.7889e+00, -2.9703e-01,  ...,  1.0479e+00,\n",
              "            2.0188e-01, -1.2868e+00],\n",
              "          [ 1.2308e-01,  6.9178e-01, -3.2254e-01,  ..., -2.1285e+00,\n",
              "           -4.8673e-01, -7.9240e-01],\n",
              "          ...,\n",
              "          [ 1.0718e+00, -5.1068e-01, -1.1642e+00,  ..., -1.0932e+00,\n",
              "           -1.4041e+00,  5.9058e-01],\n",
              "          [-1.0839e+00, -2.8648e+00,  9.0058e-01,  ...,  5.3140e-01,\n",
              "           -3.9364e-01,  1.0400e+00],\n",
              "          [ 2.7673e-01, -7.8856e-01, -6.4143e-01,  ..., -4.8322e-01,\n",
              "           -2.1286e+00,  1.2102e-01]],\n",
              "\n",
              "         [[ 4.1660e-01, -3.7750e-01,  3.5885e-02,  ..., -6.3103e-01,\n",
              "           -3.8925e-01, -4.3610e-01],\n",
              "          [-6.0661e-02,  1.4228e-01, -3.8099e+00,  ...,  6.0784e-01,\n",
              "           -5.2586e-01, -1.3530e+00],\n",
              "          [-7.9471e-01,  9.3997e-01, -4.1637e-01,  ...,  9.3322e-01,\n",
              "            2.9664e-01,  3.4207e-01],\n",
              "          ...,\n",
              "          [ 4.8783e-01, -5.1837e-01, -3.8385e-01,  ..., -1.1611e+00,\n",
              "           -2.9089e-01, -6.4413e-01],\n",
              "          [ 4.7852e-01, -7.0034e-01,  8.5366e-01,  ...,  1.0723e+00,\n",
              "           -8.5393e-01,  4.6472e-01],\n",
              "          [-8.5442e-01,  2.5549e+00, -1.0649e+00,  ..., -1.0330e+00,\n",
              "            7.0203e-01,  2.3215e-01]]],\n",
              "\n",
              "\n",
              "        [[[ 1.7931e+00, -3.9953e-01, -5.8705e-01,  ..., -9.8303e-02,\n",
              "           -7.2582e-01, -2.8126e+00],\n",
              "          [-2.6507e-01,  7.0888e-01, -5.2956e-01,  ..., -5.1551e-01,\n",
              "           -1.3164e-01, -1.6267e+00],\n",
              "          [-1.7483e+00, -5.3551e-01,  7.3089e-01,  ...,  1.2579e+00,\n",
              "            1.5496e+00,  6.3236e-01],\n",
              "          ...,\n",
              "          [ 1.3787e+00,  1.1773e+00,  1.0340e+00,  ..., -3.9269e-01,\n",
              "           -1.6705e+00,  9.6503e-01],\n",
              "          [ 1.4013e+00,  1.2178e-01, -2.1671e-01,  ...,  6.6037e-01,\n",
              "            1.7416e-01,  9.1593e-01],\n",
              "          [ 8.2501e-01, -8.2841e-01, -1.2062e+00,  ...,  6.8140e-01,\n",
              "           -1.3138e+00,  9.4882e-01]],\n",
              "\n",
              "         [[-1.1854e+00, -1.1628e+00,  5.8722e-03,  ..., -1.4671e+00,\n",
              "            8.0466e-01, -9.6949e-01],\n",
              "          [-2.0375e-01,  1.6303e-01, -9.9609e-02,  ..., -2.0793e+00,\n",
              "           -2.2400e-01, -2.1607e+00],\n",
              "          [-1.0658e+00, -1.9215e-01,  1.1774e+00,  ..., -1.6947e-01,\n",
              "           -5.9963e-01, -8.5981e-02],\n",
              "          ...,\n",
              "          [ 9.0687e-02,  6.7334e-01,  6.5879e-01,  ..., -2.4991e+00,\n",
              "           -9.0877e-01,  3.2507e-01],\n",
              "          [ 9.0804e-02, -1.0016e+00,  8.4749e-01,  ..., -2.5499e-01,\n",
              "           -1.2197e+00, -8.1200e-01],\n",
              "          [-9.2050e-01,  1.5349e+00, -2.5001e-01,  ...,  1.8941e+00,\n",
              "           -7.3533e-01,  1.6744e-01]],\n",
              "\n",
              "         [[ 1.0708e+00,  1.4184e-01,  1.9158e+00,  ...,  7.7802e-01,\n",
              "            3.5761e-02,  7.4974e-01],\n",
              "          [-8.2711e-01, -6.3707e-02,  1.1378e+00,  ...,  4.5654e-01,\n",
              "            1.9278e+00,  6.1838e-02],\n",
              "          [-2.9586e-01, -6.6345e-01,  1.1279e+00,  ...,  1.0483e+00,\n",
              "            8.3099e-01, -1.3484e-01],\n",
              "          ...,\n",
              "          [ 8.1054e-01, -4.2903e-01,  6.9175e-01,  ...,  1.8158e+00,\n",
              "            1.1203e+00, -1.5469e+00],\n",
              "          [-2.4774e-01, -6.8796e-01,  1.6631e+00,  ...,  3.8264e-01,\n",
              "            4.3809e-01, -1.8212e+00],\n",
              "          [ 2.6945e-01, -4.2778e-01, -1.0006e+00,  ..., -3.0142e-01,\n",
              "           -8.5445e-01, -3.4605e-01]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[ 1.3179e+00,  1.6102e-02,  5.0058e-02,  ...,  1.1719e+00,\n",
              "            5.9539e-01, -1.2619e+00],\n",
              "          [ 2.0163e+00,  1.5951e+00,  6.1642e-01,  ...,  7.6987e-01,\n",
              "            8.5688e-01, -3.6864e-01],\n",
              "          [-6.9718e-01, -2.6863e-01,  1.0671e-01,  ...,  1.3109e-01,\n",
              "            5.1902e-01,  2.8409e-01],\n",
              "          ...,\n",
              "          [ 1.7789e-01,  6.1587e-01,  3.7330e-01,  ...,  2.1425e-01,\n",
              "           -7.3135e-01, -5.2233e-01],\n",
              "          [ 1.6692e+00, -2.6136e-02,  1.8833e+00,  ...,  7.4137e-01,\n",
              "            1.8568e+00,  5.5810e-01],\n",
              "          [ 8.2497e-01, -3.9331e-01,  2.7847e-01,  ...,  1.2445e+00,\n",
              "           -7.4957e-01, -5.3033e-01]],\n",
              "\n",
              "         [[ 1.0159e+00, -4.2982e-01, -1.8844e-01,  ...,  1.8566e+00,\n",
              "            4.5430e-01,  8.0226e-02],\n",
              "          [-1.3860e+00, -1.5307e+00,  7.5299e-01,  ..., -9.0190e-01,\n",
              "           -5.0321e-01,  1.0415e+00],\n",
              "          [ 1.7186e+00,  8.8753e-01, -3.3554e-01,  ...,  9.6321e-01,\n",
              "            4.2948e-01, -1.1334e+00],\n",
              "          ...,\n",
              "          [ 1.0093e+00, -4.1719e-01,  1.0022e-01,  ...,  9.0786e-01,\n",
              "           -1.2658e+00,  1.9402e-01],\n",
              "          [ 2.4565e-02,  2.2517e-01, -4.1112e-01,  ..., -1.3395e+00,\n",
              "            1.0799e+00, -2.6465e+00],\n",
              "          [ 1.5773e+00,  1.7503e-01,  9.2595e-01,  ..., -5.6071e-02,\n",
              "            7.9177e-01, -2.9106e-02]],\n",
              "\n",
              "         [[-2.2986e+00, -5.3652e-01, -1.0163e+00,  ...,  1.6229e+00,\n",
              "           -3.0474e-01, -3.5219e-01],\n",
              "          [ 6.9770e-01,  1.0262e+00, -1.6673e+00,  ...,  7.1574e-01,\n",
              "           -1.3381e-01,  1.0958e+00],\n",
              "          [-9.9716e-01,  1.2874e+00,  5.7888e-01,  ..., -1.0151e+00,\n",
              "           -3.5887e-01,  2.1404e-01],\n",
              "          ...,\n",
              "          [-1.0769e+00,  4.3973e-01,  1.4390e+00,  ..., -1.8403e-02,\n",
              "            7.6839e-01,  4.8354e-01],\n",
              "          [-3.2690e-01,  1.6229e+00,  1.6328e+00,  ...,  5.3024e-02,\n",
              "           -1.9395e+00,  8.9332e-01],\n",
              "          [ 4.8280e-01,  8.1984e-01,  2.9648e-01,  ..., -1.7691e-02,\n",
              "           -4.2430e-01, -1.5427e+00]]]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spatial Transformer"
      ],
      "metadata": {
        "id": "54zUyAFXSuxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "  def __init__(self, in_channels, embedded_dim, num_heads):\n",
        "    super(SpatialTransformer, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "    self.proj_in = nn.Conv2d(in_channels,\n",
        "                                 embedded_dim,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "    self.transformer_blocks = nn.ModuleList([Transformer(embedded_dim, num_heads)for _ in range(num_heads)])\n",
        "    self.proj_out = nn.Conv2d(embedded_dim, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # note: if no context is given, cross-attention defaults to self-attention\n",
        "    b, c, h, w = x.shape\n",
        "    x_in = x\n",
        "    x = self.norm(x)\n",
        "    x = self.proj_in(x)\n",
        "    x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "    for block in self.transformer_blocks:\n",
        "        x = block(x)\n",
        "    x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "    x = self.proj_out(x)\n",
        "    return x + x_in"
      ],
      "metadata": {
        "id": "yj3Nt6Mu78VZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 32, 48, 48)\n",
        "stn = SpatialTransformer(32, 16, 8)\n",
        "st = stn(x)\n",
        "st[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5cXqlx39Juq",
        "outputId": "c9659469-7b29-4855-cfce-a51175231328"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-7.3886e-01,  6.3820e-01,  4.4061e-01,  ...,  1.2937e+00,\n",
              "           -1.8474e+00,  2.9752e+00],\n",
              "          [ 7.4824e-01, -4.1092e-01,  3.0088e-01,  ...,  1.1771e+00,\n",
              "            5.1551e-01, -1.0566e-01],\n",
              "          [-2.4397e-01,  7.3126e-01, -5.7814e-01,  ..., -1.1877e+00,\n",
              "           -6.8078e-01, -5.3009e-01],\n",
              "          ...,\n",
              "          [-2.8724e-01, -1.2830e+00,  9.1008e-01,  ...,  6.8512e-01,\n",
              "            1.3750e+00,  6.0598e-01],\n",
              "          [ 4.5885e-01,  6.3332e-01,  3.6134e-02,  ..., -1.8209e+00,\n",
              "           -1.2418e-01,  1.7355e+00],\n",
              "          [ 1.9166e+00,  8.2430e-01,  2.6554e-01,  ..., -5.1318e-01,\n",
              "           -6.7094e-01, -5.3458e-01]],\n",
              "\n",
              "         [[ 2.0268e-01, -6.4168e-02, -4.4359e-01,  ...,  5.4447e-01,\n",
              "            4.5099e-01,  5.6532e-01],\n",
              "          [-4.3931e-01,  1.1415e+00,  9.5912e-01,  ..., -5.3576e-01,\n",
              "            1.7886e+00,  1.6422e+00],\n",
              "          [-7.3811e-02,  9.1319e-01,  1.4463e+00,  ...,  5.1679e-01,\n",
              "            3.6386e-02,  4.2644e-01],\n",
              "          ...,\n",
              "          [ 2.2762e-01,  9.1563e-01, -1.4742e+00,  ..., -1.3764e-01,\n",
              "           -1.0514e+00, -9.4025e-01],\n",
              "          [-1.8186e+00,  6.8831e-01,  2.9187e-01,  ..., -1.3982e-01,\n",
              "           -6.0399e-01, -7.9576e-01],\n",
              "          [ 5.9206e-01,  1.2051e+00,  2.5369e-01,  ..., -1.0701e-01,\n",
              "            8.2766e-01,  8.8672e-01]],\n",
              "\n",
              "         [[-1.8060e+00, -1.3937e+00, -7.9886e-01,  ..., -1.4665e+00,\n",
              "           -2.2262e+00, -1.0328e+00],\n",
              "          [ 1.0665e+00,  3.2389e-01, -1.4865e+00,  ..., -1.0754e+00,\n",
              "            4.8783e-01, -5.9155e-03],\n",
              "          [-8.2144e-01,  6.1403e-01,  1.6058e+00,  ..., -1.4049e+00,\n",
              "           -1.3660e+00, -1.1608e+00],\n",
              "          ...,\n",
              "          [-9.6462e-01,  7.8000e-01, -1.7439e+00,  ...,  9.2364e-02,\n",
              "           -2.6199e+00, -4.1525e-01],\n",
              "          [-4.2251e-01, -3.8825e-02,  2.5150e-01,  ..., -5.0334e-01,\n",
              "            3.7746e-01, -1.0471e+00],\n",
              "          [-1.3907e+00, -3.3243e-01, -2.2425e+00,  ..., -1.3804e+00,\n",
              "            8.7901e-01, -1.2194e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-9.3100e-01, -3.1349e-01, -1.0140e+00,  ...,  4.4624e-01,\n",
              "            1.9730e+00,  2.0144e+00],\n",
              "          [ 6.5673e-01,  5.8159e-01, -7.2871e-01,  ...,  1.4885e+00,\n",
              "            3.3083e-01, -1.3390e+00],\n",
              "          [ 2.8514e+00, -1.4601e+00, -1.7481e+00,  ...,  1.0479e-01,\n",
              "           -1.5331e+00,  2.6736e+00],\n",
              "          ...,\n",
              "          [-2.5002e+00,  2.1715e-01, -2.1418e-01,  ...,  1.4078e+00,\n",
              "            5.9782e-01,  1.5655e+00],\n",
              "          [ 7.8729e-01, -8.4189e-01,  8.5334e-01,  ..., -2.1991e+00,\n",
              "            1.0256e+00,  1.2209e+00],\n",
              "          [ 4.9277e-01, -1.3963e+00,  2.2729e+00,  ..., -1.0994e+00,\n",
              "            4.3525e-01, -1.5061e+00]],\n",
              "\n",
              "         [[-5.3800e-01,  1.7969e-01,  2.4640e+00,  ..., -3.8938e-02,\n",
              "           -1.5685e-01,  1.8458e+00],\n",
              "          [ 2.6859e+00, -1.0079e+00,  7.8874e-01,  ...,  7.3579e-01,\n",
              "           -4.5831e-01,  7.4875e-01],\n",
              "          [ 1.7347e+00, -2.1681e-01, -5.3085e-02,  ..., -5.1627e-02,\n",
              "            4.3209e-01, -5.7270e-01],\n",
              "          ...,\n",
              "          [ 1.7020e-01, -1.1262e+00,  1.0317e-01,  ...,  3.4568e-01,\n",
              "           -4.3188e-02, -1.2430e+00],\n",
              "          [ 1.1336e+00, -1.9102e-01, -1.2005e+00,  ...,  4.1210e-01,\n",
              "           -1.0367e+00,  3.0594e-02],\n",
              "          [ 2.7627e-01, -2.5591e-03, -1.1458e+00,  ...,  1.7274e+00,\n",
              "           -4.8868e-01,  1.2400e+00]],\n",
              "\n",
              "         [[ 3.9641e-01, -1.1194e+00, -1.0520e+00,  ..., -2.5235e-01,\n",
              "            9.0838e-01,  8.4118e-01],\n",
              "          [-2.9105e-01,  5.7503e-01, -7.7339e-01,  ...,  1.9056e-01,\n",
              "            2.6706e+00, -1.3991e+00],\n",
              "          [ 2.4925e+00, -1.0011e+00, -6.1977e-01,  ..., -6.4655e-02,\n",
              "            1.1063e+00,  8.8534e-01],\n",
              "          ...,\n",
              "          [-6.3828e-01,  6.3031e-01,  5.3930e-01,  ..., -1.2132e+00,\n",
              "            1.7330e+00,  1.5945e+00],\n",
              "          [-4.1426e-01,  5.7406e-01, -2.8850e-01,  ..., -1.7043e+00,\n",
              "            5.4683e-01,  2.0122e+00],\n",
              "          [ 1.8857e+00,  1.7922e+00,  1.8212e+00,  ..., -2.1857e-01,\n",
              "            2.9409e+00, -1.0362e+00]]],\n",
              "\n",
              "\n",
              "        [[[ 1.0990e+00,  1.0698e+00, -1.9571e+00,  ...,  1.6193e+00,\n",
              "            7.1578e-01, -1.6335e+00],\n",
              "          [-1.2457e-01,  5.7106e-01, -7.1953e-01,  ..., -1.9755e+00,\n",
              "           -2.5296e+00,  1.6553e-02],\n",
              "          [-1.2100e+00, -3.7068e-01, -3.9298e-01,  ...,  1.2185e+00,\n",
              "           -1.3647e+00,  2.0741e+00],\n",
              "          ...,\n",
              "          [-1.8359e-01, -9.1797e-01,  8.8879e-01,  ..., -6.0004e-01,\n",
              "            1.5688e-01,  7.4773e-01],\n",
              "          [-6.9893e-01, -4.1797e-01, -8.1060e-01,  ..., -2.7987e-01,\n",
              "            2.7354e-01,  9.4677e-01],\n",
              "          [-1.0561e+00, -1.9399e+00,  1.2875e+00,  ..., -2.2318e+00,\n",
              "           -1.4343e+00,  8.3988e-01]],\n",
              "\n",
              "         [[ 1.3336e-01, -8.7956e-01,  1.6693e+00,  ...,  1.0576e+00,\n",
              "            1.0806e+00,  4.6868e-01],\n",
              "          [-1.5178e-01,  5.5746e-01,  7.1207e-01,  ..., -1.1286e+00,\n",
              "            8.9886e-01, -2.3015e-01],\n",
              "          [-5.7794e-01, -1.9121e+00,  9.4520e-01,  ...,  1.0249e-01,\n",
              "            2.3797e-01, -7.9814e-01],\n",
              "          ...,\n",
              "          [-1.1470e+00,  7.8979e-01,  5.5815e-01,  ...,  2.1858e-01,\n",
              "            9.8062e-01,  4.1954e-01],\n",
              "          [ 1.4677e+00, -2.2318e+00,  1.2441e-01,  ..., -1.7776e-01,\n",
              "           -5.8695e-01,  4.4982e-01],\n",
              "          [ 5.7424e-01, -1.3914e-01,  4.1702e-01,  ..., -3.9785e-01,\n",
              "            7.9149e-01, -2.0033e-01]],\n",
              "\n",
              "         [[-1.7771e+00, -3.7694e+00, -8.9653e-01,  ...,  7.5433e-01,\n",
              "           -1.3965e+00,  5.2185e-01],\n",
              "          [-1.6857e+00, -1.4831e+00, -2.7389e+00,  ..., -1.3972e+00,\n",
              "           -2.1038e+00, -5.1509e-01],\n",
              "          [-5.6094e-01, -1.4611e+00, -1.7264e+00,  ..., -2.1535e+00,\n",
              "           -5.0555e-02, -8.1858e-01],\n",
              "          ...,\n",
              "          [-2.8657e+00, -3.8147e+00, -1.8389e+00,  ..., -1.1043e+00,\n",
              "           -1.0964e+00, -1.6395e+00],\n",
              "          [ 1.8071e-01, -1.9989e+00, -1.7092e+00,  ..., -2.1052e+00,\n",
              "            6.0068e-01, -2.4309e-01],\n",
              "          [-3.9884e-01, -1.7510e-01,  8.8591e-01,  ..., -1.0792e+00,\n",
              "           -2.1744e+00, -2.1248e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-1.1434e+00, -2.6122e+00,  2.5390e+00,  ..., -6.1301e-01,\n",
              "           -2.0489e+00, -3.1939e-01],\n",
              "          [-1.0808e+00,  2.7207e+00,  3.2569e-01,  ...,  9.9780e-01,\n",
              "           -1.1786e+00,  1.3476e+00],\n",
              "          [ 1.3587e+00,  6.1609e-01, -3.0770e-01,  ...,  5.8699e-01,\n",
              "            1.5808e+00, -7.2846e-03],\n",
              "          ...,\n",
              "          [-1.0671e+00,  5.2284e-01,  1.7016e+00,  ...,  1.2308e+00,\n",
              "            1.3523e+00,  1.6456e+00],\n",
              "          [-7.6722e-01, -6.0550e-01,  5.8329e-01,  ...,  1.1249e-02,\n",
              "            1.1041e+00, -2.1007e-01],\n",
              "          [ 7.3485e-01,  1.3823e-01, -1.8712e+00,  ..., -2.1185e+00,\n",
              "           -4.9308e-01,  8.1944e-01]],\n",
              "\n",
              "         [[ 8.4778e-01,  1.3346e-01, -1.2923e+00,  ...,  6.6509e-01,\n",
              "           -1.9757e+00,  1.6095e+00],\n",
              "          [-2.5844e-01,  8.2455e-01, -1.4345e+00,  ..., -2.1783e-01,\n",
              "            3.3242e-01, -2.0574e+00],\n",
              "          [-1.4587e+00, -9.0569e-03, -2.2004e-01,  ...,  5.4141e-01,\n",
              "            6.3862e-01, -9.1762e-01],\n",
              "          ...,\n",
              "          [ 8.8676e-01, -1.0035e+00,  2.6887e-01,  ..., -1.3753e+00,\n",
              "            2.1828e+00,  1.2722e-01],\n",
              "          [-1.0675e-03, -1.1039e-01, -5.0430e-01,  ..., -1.2851e+00,\n",
              "           -2.9884e-01, -6.9486e-01],\n",
              "          [-5.3601e-01,  2.0755e+00,  1.1719e-01,  ...,  9.9863e-01,\n",
              "           -2.1063e+00, -2.8816e-01]],\n",
              "\n",
              "         [[-1.4214e+00,  8.9442e-01, -7.0429e-01,  ...,  1.4793e+00,\n",
              "            4.5193e-01, -9.6503e-01],\n",
              "          [ 5.3357e-01,  9.2703e-01,  1.9907e+00,  ...,  9.9828e-01,\n",
              "           -1.8864e+00,  3.4227e-01],\n",
              "          [ 4.8657e-01,  8.1746e-01,  1.4071e+00,  ...,  1.3927e+00,\n",
              "           -8.1292e-01,  7.0266e-01],\n",
              "          ...,\n",
              "          [-2.0544e-01,  2.1518e+00,  7.8462e-01,  ...,  1.6271e+00,\n",
              "           -8.6132e-01,  1.3356e+00],\n",
              "          [ 1.5608e-01,  1.5889e-01, -1.9347e+00,  ...,  1.9169e+00,\n",
              "            1.3042e+00, -5.0761e-01],\n",
              "          [-9.9305e-01,  5.2759e-01, -6.3883e-02,  ..., -5.3629e-01,\n",
              "            1.1362e-01,  4.9388e-01]]],\n",
              "\n",
              "\n",
              "        [[[-3.7364e-01, -1.5257e+00,  1.0781e-01,  ..., -8.2517e-01,\n",
              "           -1.5280e+00, -1.1253e+00],\n",
              "          [ 1.9079e-01,  8.5159e-01,  4.1588e-01,  ...,  7.5357e-01,\n",
              "            1.0260e+00,  2.8025e-01],\n",
              "          [-1.2215e+00, -8.7241e-01, -3.9791e-01,  ..., -1.2006e+00,\n",
              "           -1.9012e+00, -5.9034e-03],\n",
              "          ...,\n",
              "          [-1.4889e+00, -1.4173e+00, -3.4049e+00,  ...,  1.3134e+00,\n",
              "           -6.3889e-01, -3.6788e-02],\n",
              "          [-3.3730e-01, -5.0481e-01, -4.8189e-01,  ...,  2.5916e+00,\n",
              "            9.9073e-01,  3.7112e-01],\n",
              "          [ 4.8411e-01, -2.2431e+00, -8.3261e-01,  ...,  1.2449e-01,\n",
              "            2.5827e+00, -1.3316e+00]],\n",
              "\n",
              "         [[-1.0309e-01, -4.0250e-01,  2.0418e-01,  ...,  5.5538e-01,\n",
              "           -1.3519e+00, -2.4162e-01],\n",
              "          [ 3.2418e-01,  1.7102e+00, -1.3507e+00,  ...,  4.2063e-01,\n",
              "            1.0696e+00, -1.0073e+00],\n",
              "          [-6.6642e-01,  2.5501e+00,  1.6632e+00,  ...,  5.4315e-01,\n",
              "            1.8100e-01, -1.7733e-01],\n",
              "          ...,\n",
              "          [ 5.2025e-01,  7.7978e-01,  6.0290e-01,  ..., -7.1881e-01,\n",
              "           -6.3613e-01, -2.8446e-01],\n",
              "          [-2.4350e-01, -9.1887e-01,  9.8390e-01,  ..., -2.3156e-01,\n",
              "           -3.4153e-02, -1.8488e+00],\n",
              "          [-2.9869e-01,  3.1287e+00,  2.0756e-01,  ...,  2.1941e-01,\n",
              "           -9.5713e-01, -4.0784e-01]],\n",
              "\n",
              "         [[-2.6596e+00, -2.3076e+00,  3.7705e-01,  ..., -1.7748e+00,\n",
              "           -1.6831e+00, -1.8969e+00],\n",
              "          [-1.6366e-01, -1.6717e+00, -1.6413e+00,  ..., -1.2555e+00,\n",
              "           -6.6099e-01, -1.0113e+00],\n",
              "          [-2.0374e+00, -2.1204e+00, -1.9158e+00,  ..., -2.2681e-02,\n",
              "           -7.3622e-02, -4.7757e-02],\n",
              "          ...,\n",
              "          [-6.8989e-01, -1.8485e+00, -1.8927e+00,  ..., -1.7351e+00,\n",
              "           -1.4291e+00, -5.4329e-01],\n",
              "          [-2.8289e+00,  1.8578e+00,  5.7988e-01,  ..., -1.6669e+00,\n",
              "           -2.4779e+00, -7.8408e-01],\n",
              "          [-1.1439e+00, -1.9044e-01, -1.4001e+00,  ..., -2.2946e+00,\n",
              "           -1.3664e+00, -1.0225e+00]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[-7.6956e-02, -1.3944e+00, -2.2883e+00,  ..., -2.8265e+00,\n",
              "            7.4409e-01, -2.1101e-01],\n",
              "          [-1.8931e+00,  3.4850e-01, -6.5890e-01,  ..., -1.6779e+00,\n",
              "           -2.1089e+00,  2.8471e-01],\n",
              "          [-5.3745e-01,  1.2366e+00,  1.4606e+00,  ..., -6.0600e-01,\n",
              "           -1.4698e+00, -1.9359e+00],\n",
              "          ...,\n",
              "          [-1.1953e+00, -7.2914e-01, -2.1150e+00,  ..., -6.5580e-01,\n",
              "           -2.0359e-01, -5.1646e-01],\n",
              "          [ 6.9704e-01, -7.7872e-03, -2.0486e-01,  ..., -5.6479e-01,\n",
              "           -1.2464e+00, -1.9566e+00],\n",
              "          [-1.1245e+00,  2.6028e+00, -5.8511e-01,  ..., -8.6382e-01,\n",
              "            4.5433e-01, -1.5452e+00]],\n",
              "\n",
              "         [[ 9.5724e-01,  1.9293e-01,  5.2099e-01,  ..., -5.2577e-01,\n",
              "           -5.0840e-01, -1.1816e+00],\n",
              "          [ 1.9829e+00,  7.2294e-01, -1.8873e-01,  ..., -1.0380e+00,\n",
              "            3.6327e-02,  5.9755e-02],\n",
              "          [ 6.8078e-01,  2.6946e+00,  2.8260e-01,  ..., -1.5466e+00,\n",
              "           -1.6846e+00, -5.9633e-01],\n",
              "          ...,\n",
              "          [ 4.6988e-01, -1.7454e-01, -3.4675e-01,  ...,  1.1576e+00,\n",
              "           -1.9755e+00,  2.5973e-01],\n",
              "          [ 1.9200e-01, -1.9865e+00,  6.5878e-01,  ...,  1.8788e-02,\n",
              "            5.8025e-01,  4.3815e-01],\n",
              "          [ 8.9514e-01,  2.1331e-01,  2.0842e+00,  ..., -5.7641e-01,\n",
              "           -1.5178e-02,  6.7276e-01]],\n",
              "\n",
              "         [[ 4.7059e-01, -2.7369e+00, -1.7252e+00,  ..., -3.6653e-01,\n",
              "            2.3159e-01,  2.6810e+00],\n",
              "          [ 5.9681e-01,  1.6361e+00,  2.5920e-01,  ...,  2.3874e+00,\n",
              "           -2.6158e-01, -3.3901e-01],\n",
              "          [-1.0067e+00,  2.3846e+00,  9.2068e-01,  ...,  6.4108e-01,\n",
              "           -6.9137e-01,  1.9696e-01],\n",
              "          ...,\n",
              "          [ 1.7058e+00, -8.1613e-01,  1.0690e+00,  ..., -5.8275e-01,\n",
              "            1.4458e+00, -3.5723e-01],\n",
              "          [ 3.7537e-01, -1.6838e+00,  1.4428e+00,  ..., -1.0846e-01,\n",
              "           -5.6986e-01, -2.5602e-02],\n",
              "          [-1.3601e+00,  1.4595e+00, -2.2118e-01,  ..., -1.2510e-01,\n",
              "            2.5251e+00, -2.5836e+00]]]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 32, 64, 64)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kofe97LVLhMW",
        "outputId": "dd92134b-4fdd-434b-d2d6-163f7e6dfae9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class SpatialTransformer(nn.Module):\n",
        "#     def __init__(self, input_size, output_size):\n",
        "#         super(SpatialTransformer, self).__init__()\n",
        "\n",
        "#         self.input_size = input_size\n",
        "#         self.output_size = output_size\n",
        "\n",
        "#         # Localization network\n",
        "#         self.localization = nn.Sequential(\n",
        "#             nn.Conv2d(input_size[0], 8, kernel_size=7),\n",
        "#             nn.MaxPool2d(2, stride=2),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(8, 10, kernel_size=5),\n",
        "#             nn.MaxPool2d(2, stride=2),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Conv2d(10, 12, kernel_size=3),\n",
        "#             nn.MaxPool2d(2, stride=2),\n",
        "#             nn.ReLU(True)\n",
        "#         )\n",
        "\n",
        "#         # Output size of the localization network\n",
        "#         out_size = self._get_output_size(input_size)\n",
        "\n",
        "#         # Affine transformation matrix theta\n",
        "#         self.fc_loc = nn.Sequential(\n",
        "#             nn.Linear(out_size, 32),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.Linear(32, 3 * 2)\n",
        "#         )\n",
        "\n",
        "#         # Initialize theta to identity transformation\n",
        "#         self.fc_loc[2].weight.data.zero_()\n",
        "#         self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "#     def _get_output_size(self, input_size):\n",
        "#         input_tensor = torch.zeros(1, *input_size)\n",
        "#         output_tensor = self.localization(input_tensor)\n",
        "#         out_size = output_tensor.data.size()[1:]\n",
        "#         return int(torch.prod(torch.tensor(out_size)))\n",
        "\n",
        "#     def stn(self, x):\n",
        "#         # Get theta\n",
        "#         theta = self.fc_loc(x.view(-1, self._get_output_size(self.input_size)))\n",
        "#         theta = theta.view(-1, 2, 3)\n",
        "\n",
        "#         # Generate grid\n",
        "#         grid = F.affine_grid(theta, torch.Size((x.size(0), self.input_size[0], self.output_size[1], self.output_size[2])))\n",
        "\n",
        "#         # Apply transformation\n",
        "#         x = F.grid_sample(x, grid)\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Apply localization network\n",
        "#         x = self.localization(x)\n",
        "\n",
        "#         # Apply spatial transformation\n",
        "#         x = self.stn(x)\n",
        "\n",
        "#         return x"
      ],
      "metadata": {
        "id": "VzANmAhx_R4Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_size = (3, 64, 64)\n",
        "# output_size = (3, 128, 128)\n",
        "\n",
        "# st = SpatialTransformer(input_size, output_size)\n",
        "\n",
        "# input_data = torch.randn(1, 3, 64, 64)\n",
        "# output_data = st(input_data)\n",
        "# output_data.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQrqxZdS_Sj_",
        "outputId": "f9bdc679-96f4-4bf9-eea3-e51b806ecadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 12, 128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}