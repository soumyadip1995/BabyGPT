## language models

Trying to build language models from scratch to develop intuition. Will slowly scale up to transformers. A toy implementation of a bigram language model and n gram language model has been included above. 

- A very basic implementation of a scaled dot product attention has been done.  Notebook has been added. Added a new section called demystifying queries keys and values.

### TO DO

- Tensor Manipulations need to be done. Tensor ops Need to be explored. Received attention and value tensors . 
- Develop intuition as to how to make use of it further in a bigram or an n-gram model.
- Tensor ops on ngram and bigram models. On multi head attention.

- Understand Attention mechanisms. Scaled Dot product attention. Also keep updating the document.
- Encoder - Decoder architecture needs to be explored.
- Work on the encoder side of the architecture.( Multi head self attention and a  feedforward layer.)
- Then move onto the decoder side of things.
- Also, document the above.

You can run the above for now. ```text.txt ``` is based on Eminem's Rap God.
