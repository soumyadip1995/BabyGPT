## language models

Trying to build language models from scratch to develop intuition. Will slowly scale up to transformers. A toy implementation of a bigram language model and n gram language model has been included above. 




### TO DO

- :heavy_check_mark: Tensor Manipulations need to be done. Tensor ops Need to be explored. Received attention and value tensors . 
- :heavy_check_mark: A preliminary multi head attention has been added. needs cleaning up. A single head attention has been added.
- :heavy_check_mark: A feedforward layer on the encoder side has been added.
- :heavy_check_mark: A Transformer Encoder layer has been added.
- :heavy_check_mark: Positional Embedding has been added.
- :heavy_check_mark: An implementation of a GPT has been done.
- A model of a basic transformer has been added. A babyGPT model has been added. Training also done. 
- Need to update Readme later on.
- The GPT notebook has been added.  Refactoring of the original notebook needs to be done.
- Develop intuition as to how to make use of it further in a bigram or an n-gram model.
- Tensor ops on ngram and bigram models. On multi head attention.

- Understand Attention mechanisms. Scaled Dot product attention. Also keep updating the document.
- Encoder - Decoder architecture needs to be explored.
- Work on the encoder side of the architecture.( Multi head self attention and a  feedforward layer.)
- Then move onto the decoder side of things.
- Also, document the above.

You can run the above for now. ```text.txt ``` is based on Eminem's Stan.
