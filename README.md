## language models

Trying to build language models from scratch to develop intuition. Will slowly scale up to transformers. A toy implementation of a bigram language model and n gram language model has been included above. 




### TO DO

- :heavy_check_mark: Tensor Manipulations need to be done. Tensor ops Need to be explored. Received attention and value tensors . 
- :heavy_check_mark: A preliminary multi head attention has been added. needs cleaning up. A single head attention has been added.
- :heavy_check_mark: A feedforward layer on the encoder side has been added.
- :heavy_check_mark: A Transformer Encoder layer has been added.
- :heavy_check_mark: Positional Embedding has been added.
- :heavy_check_mark: An implementation of a GPT has been done.
- A model of a basic transformer has been added. A babyGPT model has been added. Training also done. 
- Need to update Readme later on.
- The GPT notebook has been added.  
- Develop intuition as to how to make use of it further in a bigram or an n-gram model.
- Tensor ops on ngram and bigram models. On multi head attention.


- Also, document the above.

You can run the above for now. ```text.txt ``` is based on Eminem's Stan.
