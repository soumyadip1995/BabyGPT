{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx5LabZWGMDf6FhFDMpJg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/language-models/blob/main/Multi_head_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-iyjhxM5BKS",
        "outputId": "2b871ba2-bb48-43c8-b735-e662adf78e3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So',\n",
              " 'Ray',\n",
              " 'J',\n",
              " 'went',\n",
              " 'straight',\n",
              " 'to',\n",
              " 'the',\n",
              " 'radio',\n",
              " 'station',\n",
              " 'The',\n",
              " 'very',\n",
              " 'next',\n",
              " 'day,',\n",
              " '\"Hey',\n",
              " 'Fab,',\n",
              " \"I'ma\",\n",
              " 'kill',\n",
              " 'you!\"',\n",
              " 'Lyrics',\n",
              " \"comin'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "words = open('/content/text.txt', 'r').read().split()\n",
        "words[:20]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "print(string2integer)\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "print(encode)\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "print(decode)\n",
        "\n",
        "data = torch.tensor(encode(words), dtype = torch.float)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuriIPRQ5WA9",
        "outputId": "7816503a-9c12-40f0-99f9-aba6b15a5519"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\"': 0, '\"Hey': 1, '\"It\\'s': 2, '\"Lose': 3, '\"Oh,': 4, \"'Cause\": 5, \"'cause\": 6, \"'em\": 7, '(J.J.': 8, 'Doc': 9, 'Fab,': 10, 'Fad)': 11, 'For': 12, 'How': 13, 'I': 14, \"I'm\": 15, \"I'ma\": 16, 'Innovative': 17, 'J': 18, 'Lyrics': 19, 'Never': 20, 'Ray': 21, 'So': 22, 'The': 23, 'Throw': 24, 'Uh,': 25, 'Well,': 26, 'What': 27, 'With': 28, 'You': 29, 'Yourself\"': 30, 'a': 31, 'and': 32, 'anything': 33, 'are': 34, \"assumin'\": 35, 'at': 36, 'audience': 37, 'be': 38, 'can': 39, 'celebrating': 40, \"comin'\": 41, 'confuse': 42, 'day': 43, 'day,': 44, 'demonstrating': 45, 'devastating,': 46, 'do': 47, 'dooma-lumma,': 48, 'elevating': 49, 'elevator': 50, 'ever': 51, 'fading,': 52, 'feeling': 53, 'fell': 54, 'forever': 55, 'found': 56, 'fuse': 57, 'get': 58, 'give': 59, 'glue': 60, 'gotta': 61, 'haters': 62, \"he's\": 63, 'hella': 64, 'hip-hop,': 65, 'human': 66, 'is': 67, 'it': 68, \"it'll\": 69, \"it's\": 70, 'jealous,': 71, 'kill': 72, 'know': 73, 'levitating': 74, 'like': 75, 'lose': 76, 'made': 77, 'mainstream\"': 78, 'make': 79, 'me,': 80, 'more': 81, \"motherfuckin'\": 82, 'motivated': 83, 'music': 84, 'music,': 85, 'next': 86, 'not': 87, 'of': 88, 'off': 89, 'off,': 90, 'on': 91, 'pop,': 92, 'radio': 93, 'rap': 94, \"ricochetin'\": 95, 'rock,': 96, 'rubber': 97, 'say': 98, 'shock': 99, 'so': 100, 'speed': 101, 'station': 102, 'straight': 103, 'summa-lumma,': 104, 'superhuman?': 105, 'supersonic': 106, 'than': 107, 'that': 108, \"that's\": 109, 'the': 110, 'they': 111, \"they'll\": 112, 'through': 113, 'to': 114, 'too': 115, 'very': 116, 'waiting': 117, 'way': 118, 'went': 119, 'what': 120, 'when': 121, 'with': 122, 'you': 123, 'you!\"': 124}\n",
            "<function <lambda> at 0x7fb1d6bc6e50>\n",
            "<function <lambda> at 0x7fb1e287a8b0>\n",
            "tensor([ 22.,  21.,  18., 119., 103., 114., 110.,  93., 102.,  23., 116.,  86.,\n",
            "         44.,   1.,  10.,  16.,  72., 124.,  19.,  41.,  36., 123.,  36., 106.,\n",
            "        101.,   8.,  11.,  25., 104.,  48., 123.,  35.,  15.,  31.,  66.,  27.,\n",
            "         14.,  61.,  47., 114.,  58.,  68., 113., 114., 123.,  15., 105.,  17.,\n",
            "         32.,  15.,  77.,  88.,  97., 100., 108.,  33.,  29.,  98.,  67.,  95.,\n",
            "         89.,  88.,  80.,  32.,  69.,  60., 114., 123.,  32.,  15.,  46.,  81.,\n",
            "        107.,  51.,  45.,  13., 114.,  59.,  31.,  82.,  37.,  31.,  53.,  75.,\n",
            "         70.,  74.,  20.,  52.,  32.,  14.,  73., 110.,  62.,  34.,  55., 117.,\n",
            "         12., 110.,  43., 108., 111.,  39.,  98.,  14.,  54.,  90., 112.,  38.,\n",
            "         40.,   5.,  14.,  73., 110., 118., 114.,  58.,   7.,  83.,  14.,  79.,\n",
            "         49.,  85., 123.,  79.,  50.,  84.,   4.,  63., 115.,  78.,  26., 109.,\n",
            "        120., 111.,  47., 121., 111.,  58.,  71., 111.,  42.,  68.,   2.,  87.,\n",
            "         65.,  70.,  92.,   0.,   6.,  14.,  56.,  31.,  64., 118., 114.,  57.,\n",
            "         68.,  28.,  96.,  99.,  94., 122.,   9.,  24.,  91.,   3.,  30.,  32.,\n",
            "         79.,   7.,  76.,  68.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# - blocksize, (batchsize))\n",
        "ix = torch.randint(len(data) - 64, (512,))\n",
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Y9VdIx5h5k",
        "outputId": "667b7e98-a677-4270-a749-84cf5499fa9a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 90,  35,  82,  79,  43,  57, 102,  69,  68,  98,  83,  68,  83,  59,\n",
              "          5,  89,  61,  32,  27,  71,  24,  24,  19,  96,  33,  60,  40,  94,\n",
              "         18,  38,  79,  70, 107,  82,  88,  74,   8,  58,  37,   6, 100,  48,\n",
              "         59,  15,  99,  31,  56,  60,  97,   7,  86,   6,  30,  33, 107,  81,\n",
              "         68, 107,  57,  94,  83,  81,  14,  79,  70,  27,  71,  65,  37,   7,\n",
              "         66,  77,  78,  61,  70,  75,   6,  31,  16,  49,  78,  84,  28,  97,\n",
              "        100,  72,  99,  92,  58,  59,  94,  56,  41,  10,  98,  74,  54,   0,\n",
              "         60,   4, 103,  78,  43, 104,  43,  93,  26,  20,  58,  52,   5,   7,\n",
              "         96, 107, 107,  84,  30,  58,  33,   4, 101,  88,  45,  23,  65,  19,\n",
              "         97,  11,  73,  38,  37,  38,  61,  61,  23,  50,  68,  74,  74,  86,\n",
              "         69,  84,  31,  53,  49,  64, 103,  94,   3,  51,  36, 104,  33,  31,\n",
              "         78,  55,  72, 102,  82,  43,  70,  77,  64,  17,  32,  26,  94,  86,\n",
              "         46,  65,  16,  78,   8,  78,  21,  18,   9,   0,  48,  11,  70,  20,\n",
              "         58,   0,   1,   9,   8,  61,  13, 107,  94,  64,  95,   5,  95,  53,\n",
              "         91,  88,  38,  18,   5,  56,  11,  59,  17,  85,  72,  24,   2,  11,\n",
              "         62,  59,  27,  17,  86, 107,  69, 102,  62,  65,  89,  65,  77,  68,\n",
              "         52,  77,  42,  94, 100,  12,  93,  12,  27,  62,  11,  15, 107,   4,\n",
              "         59,  33,  99,  94,  38,  93,  29,  68,  15,  68,  32,  95,  74,  98,\n",
              "         77,  71,  77,  77,  81,  39,  43,  11,  58,  66,  53,  21, 103,  68,\n",
              "          8,   3,  20, 105, 102,  33,  54, 103,  80, 102,  87,  19,  60, 106,\n",
              "         64,  85,  89,   0,  99,  53,  58,  76, 103,  42, 102,  90,   2,  18,\n",
              "         60,   7,  80,  62,  92,  93,  70, 106,  59,   6,  54,  63,  47,  99,\n",
              "         63,  64,  70,  66,  57,  35,  99, 103,  55,  10,  84,   8,  64,  70,\n",
              "         21,  41,  65,  69,  42,   2,  24, 103,  48,  88,   2,  23,  52,  27,\n",
              "         75,  38,  69,  56,  12,  83,  46,  13,  67, 106,  96,  87,  60,  71,\n",
              "         42,  22, 101,  30,  88,  69,  66,  26,  41,  13,  13,  50,  88,   7,\n",
              "         42, 104,  69,  11, 103,  16,  47,  82,  48,  14,  40,  52,  91,  38,\n",
              "         58,  56,  16, 101,  90,  58,  71,  93,  10,  18,  82,  62,  79,  86,\n",
              "         94,  54,  79,  50,  37,  67,  93,  44,  15,  67, 102,  87,  35,  94,\n",
              "         19,  70,  23, 105,  98,  51,  33,  39,  63,   1,  42,   3,  58,  83,\n",
              "         66,  21,  52,   4,  90,  96,  61,  25,  61,  45,  68,   1,  66,  77,\n",
              "         55,  14,   5, 100,  29,   1,   0,   8,   3,  44,  36,   0,  21,  30,\n",
              "          4,   1,  93,  62,  73,   0,  50,  37,   2,  84,  88,  99,  63,  23,\n",
              "         73,  71,  35,  67, 101,  46,  92,  61,  33,  12,  37,   8,  38,  50,\n",
              "         11,  63,  93,  12,  37,  19,  45,  42,   6,  26,  70,  84,  69,   2,\n",
              "         55,  46,   9,  49, 102,  39,  70,  67,  62,  59,  95,   5,  68,  28,\n",
              "          4,  19,  35,  17,  89,  91,  18,  99])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.stack([data[i:i+64] for i in ix])\n",
        "\n",
        "d = x.float()\n",
        "d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9X-oqv75k4U",
        "outputId": "442a3ea0-8c2c-4c13-9029-6e6168ce0129"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73., 110.,  62.,  ...,  31.,  64., 118.],\n",
              "        [ 27.,  14.,  61.,  ...,  12., 110.,  43.],\n",
              "        [ 53.,  75.,  70.,  ...,  87.,  65.,  70.],\n",
              "        ...,\n",
              "        [110.,  62.,  34.,  ...,  64., 118., 114.],\n",
              "        [ 19.,  41.,  36.,  ...,  82.,  37.,  31.],\n",
              "        [108., 111.,  39.,  ...,  94., 122.,   9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "p = d.transpose(-2, 1)\n",
        "p.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1y83mbKHM5c",
        "outputId": "ebc03479-4ea3-4929-9fed-e504fc18b8d5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np \n",
        "import torch.nn.functional  as F\n",
        "import torch\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  dim_k = query.size(-1)\n",
        "  scores = torch.bmm(query, key.transpose(-2, -1)) / sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim = -1)\n",
        "  return torch.bmm(weights, value)\n",
        "  \n"
      ],
      "metadata": {
        "id": "MHWCNqg0egmK"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "size = len(chars) # sequence length\n",
        "# print(size)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "dim_k = 2\n",
        "\n",
        "\n",
        "key = torch.randn(512, size, dim_k)\n",
        "value = torch.randn(512, size, dim_k) \n",
        "query = torch.randn(512, size, dim_k)\n",
        "\n",
        "\n",
        "\n",
        "sdp = scaled_dot_product_attention(query, key, value)\n",
        "sdp.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1zKRv0NiPph",
        "outputId": "3692520d-842d-4fa1-dbf6-b11c7929234b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 125, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedded_dim, head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embedded_dim, head_dim, bias = False)\n",
        "    self.k = nn.Linear(embedded_dim,  head_dim, bias = False)\n",
        "    self.v = nn.Linear(embedded_dim,  head_dim, bias = False)\n",
        "\n",
        "  def forward(self, p):\n",
        "    attention_outputs = scaled_dot_product_attention(self.q(p), self.k(p), self.v(p))\n",
        "    \n",
        "\n",
        "    return attention_outputs\n",
        "    "
      ],
      "metadata": {
        "id": "vxajBtrOglFC"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = embedding dimensions\n",
        "# num_heads  = number of heads \n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super().__init__()\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.num_heads = num_heads\n",
        "    head_dim = embedded_dim // num_heads \n",
        "\n",
        "    self.heads = nn.ModuleList([AttentionHead(embedded_dim, head_dim) for _ in range(num_heads)])\n",
        "    self.output_linear = nn.Linear(embedded_dim, embedded_dim)\n",
        "\n",
        "  def forward(self, p):\n",
        "    out = torch.cat([h(p) for h in self.heads], dim = -1)\n",
        "    \n",
        "    out = self.output_linear(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "9YHPWifthH2K"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attention = MultiHeadAttention(embedded_dim = 512, num_heads = 8)\n",
        "multihead_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzkvfL57heSZ",
        "outputId": "09936246-1218-4f80-b838-22188d5ff0be"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (heads): ModuleList(\n",
              "    (0): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (1): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (2): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (3): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (4): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (5): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (6): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "    (7): AttentionHead(\n",
              "      (q): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (k): Linear(in_features=512, out_features=64, bias=False)\n",
              "      (v): Linear(in_features=512, out_features=64, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Single headed attention\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SingleAttention(nn.Module):\n",
        "    def __init__(self, embedded_dim):\n",
        "        super(SingleAttention, self).__init__()\n",
        "        self.embedded_dim = embedded_dim\n",
        "        \n",
        "        self.W_q = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.W_k = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.W_v = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.W_o = nn.Linear(embedded_dim, embedded_dim)\n",
        "        \n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        \n",
        "        # Apply linear transformation to Q, K, and V\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "        \n",
        "        # Compute attention scores and weights\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.embedded_dim).float())\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        weights = nn.Softmax(dim=-1)(scores)\n",
        "        \n",
        "        # Apply attention weights to values\n",
        "        output = torch.matmul(weights, V)\n",
        "        output = self.W_o(output)\n",
        "        \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "-bfLrHoQnFnL"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "chars = sorted(list(set(words)))\n",
        "# print(chars)\n",
        "seq_len = len(chars) # sequence length\n",
        "embedded_dim = 512\n",
        "batch_size = 32\n",
        "\n",
        "# Create some dummy input tensors\n",
        "Q = torch.randn(batch_size, seq_len, embedded_dim)\n",
        "K = torch.randn(batch_size, seq_len, embedded_dim)\n",
        "V = torch.randn(batch_size, seq_len, embedded_dim)\n",
        "\n",
        "# Create an instance of the single attention model\n",
        "single_attn = SingleAttention(embedded_dim)\n",
        "\n",
        "# Pass the input tensors through the model\n",
        "output = single_attn(Q, K, V)\n",
        "\n",
        "# The output tensor has shape (batch_size, seq_len, embedded_dim)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIxYtzE8nWhI",
        "outputId": "4b103fde-8913-4613-d8d2-b8139125c390"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 125, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUvY3gu9pXIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}