{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVLWhLv3d9PQ1c84BQaa8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/BabyGPT/blob/main/Additional%20BabyGPT\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal BabyGPT"
      ],
      "metadata": {
        "id": "P5DbV8Mii_qE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63aHoa3jix8C",
        "outputId": "f72235c9-78ff-4b62-f8c5-20230e87bd70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50787, 17350,  8212, 61123, 18205, 23400, 11072, 47328],\n",
            "        [39423, 14964, 64144, 32402, 46828, 32316, 18580, 30649],\n",
            "        [45169, 46828, 45355, 12695,  2410, 66146, 53951,  2411],\n",
            "        [ 1423, 10636,  4823, 11013, 15555,  3540, 11469,  3539],\n",
            "        [58467, 20442, 34471, 47360, 57328,  1928, 43207, 17106],\n",
            "        [15430, 18580, 60551,  7372, 23176, 20625, 40223, 11471],\n",
            "        [17106, 50566, 10903, 42130, 17106, 36585, 56927, 66992],\n",
            "        [60524, 28938, 14957, 40223, 65437, 36865, 39274, 36865],\n",
            "        [66610, 37247, 38007, 37707, 61383, 64947,  7982, 21958],\n",
            "        [ 1928, 22640, 63688, 61174, 61470,  6195, 14025, 55483],\n",
            "        [60557, 17106, 35929, 38836, 18580, 63712, 38007, 19241],\n",
            "        [12304, 46107, 45027, 23682, 15070, 41046,  7725, 28863],\n",
            "        [40160, 60551, 38579, 10112,  3254,  6237,   490, 48250],\n",
            "        [50825, 61383, 43954, 43790, 62558, 38588, 56201, 18580],\n",
            "        [ 6301, 10590, 47696, 40936, 16263,  8212, 64906, 17259],\n",
            "        [46828, 60848, 41309, 16311,  8212, 45539, 65897, 41578]]) tensor([64491, 16605,  7873,  4824, 45526, 14783, 55370, 31777, 51326, 40160,\n",
            "        37309, 61383, 30850, 37309, 61193, 17106])\n",
            "number of parameters: 2227811\n",
            "number of parameters: 2227811\n",
            "number of parameters: 2227811\n",
            "number of parameters: 2227811\n",
            "0 11.138203620910645\n",
            "1 11.05502700805664\n",
            "2 11.001397132873535\n",
            "3 10.95842170715332\n",
            "4 10.921546936035156\n",
            "5 10.88674259185791\n",
            "6 10.858298301696777\n",
            "7 10.829719543457031\n",
            "8 10.809672355651855\n",
            "9 10.779553413391113\n",
            "10 10.75738525390625\n",
            "11 10.730128288269043\n",
            "12 10.702835083007812\n",
            "13 10.6813325881958\n",
            "14 10.659756660461426\n",
            "15 10.634039878845215\n",
            "16 10.61115550994873\n",
            "17 10.586128234863281\n",
            "18 10.561418533325195\n",
            "19 10.533382415771484\n",
            "20 10.505067825317383\n",
            "21 10.479866027832031\n",
            "22 10.451208114624023\n",
            "23 10.423227310180664\n",
            "24 10.393095016479492\n",
            "25 10.367406845092773\n",
            "26 10.33486270904541\n",
            "27 10.306488037109375\n",
            "28 10.273500442504883\n",
            "29 10.243342399597168\n",
            "30 10.209444999694824\n",
            "31 10.176932334899902\n",
            "32 10.144190788269043\n",
            "33 10.112773895263672\n",
            "34 10.076888084411621\n",
            "35 10.045866966247559\n",
            "36 10.010176658630371\n",
            "37 9.974699020385742\n",
            "38 9.938902854919434\n",
            "39 9.902552604675293\n",
            "40 9.867574691772461\n",
            "41 9.829586029052734\n",
            "42 9.79383373260498\n",
            "43 9.753876686096191\n",
            "44 9.715841293334961\n",
            "45 9.67694091796875\n",
            "46 9.636507034301758\n",
            "47 9.597700119018555\n",
            "48 9.557412147521973\n",
            "49 9.519076347351074\n",
            "50 9.477095603942871\n",
            "51 9.434235572814941\n",
            "52 9.393669128417969\n",
            "53 9.351713180541992\n",
            "54 9.30893611907959\n",
            "55 9.266972541809082\n",
            "56 9.223600387573242\n",
            "57 9.183351516723633\n",
            "58 9.135836601257324\n",
            "59 9.091121673583984\n",
            "60 9.047388076782227\n",
            "61 9.002239227294922\n",
            "62 8.95676040649414\n",
            "63 8.911896705627441\n",
            "64 8.865419387817383\n",
            "65 8.818589210510254\n",
            "66 8.773327827453613\n",
            "67 8.725147247314453\n",
            "68 8.677186012268066\n",
            "69 8.631392478942871\n",
            "70 8.583222389221191\n",
            "71 8.533624649047852\n",
            "72 8.484783172607422\n",
            "73 8.436909675598145\n",
            "74 8.3886079788208\n",
            "75 8.336596488952637\n",
            "76 8.288347244262695\n",
            "77 8.237101554870605\n",
            "78 8.185211181640625\n",
            "79 8.135498046875\n",
            "80 8.084147453308105\n",
            "81 8.03274917602539\n",
            "82 7.980369567871094\n",
            "83 7.92793607711792\n",
            "84 7.876242160797119\n",
            "85 7.822653293609619\n",
            "86 7.769588470458984\n",
            "87 7.716149806976318\n",
            "88 7.662961959838867\n",
            "89 7.609200954437256\n",
            "90 7.554642677307129\n",
            "91 7.4999589920043945\n",
            "92 7.445584297180176\n",
            "93 7.390321731567383\n",
            "94 7.334747314453125\n",
            "95 7.280167102813721\n",
            "96 7.223687171936035\n",
            "97 7.168187618255615\n",
            "98 7.112568378448486\n",
            "99 7.0552897453308105\n",
            "100 6.9978108406066895\n",
            "101 6.941091537475586\n",
            "102 6.8832902908325195\n",
            "103 6.825881481170654\n",
            "104 6.7684454917907715\n",
            "105 6.7100510597229\n",
            "106 6.652364730834961\n",
            "107 6.594030857086182\n",
            "108 6.5356245040893555\n",
            "109 6.477192401885986\n",
            "110 6.417768478393555\n",
            "111 6.35858154296875\n",
            "112 6.299053192138672\n",
            "113 6.2399187088012695\n",
            "114 6.180279731750488\n",
            "115 6.120167255401611\n",
            "116 6.060602188110352\n",
            "117 6.000474452972412\n",
            "118 5.940752983093262\n",
            "119 5.880652904510498\n",
            "120 5.8206257820129395\n",
            "121 5.7605390548706055\n",
            "122 5.700224876403809\n",
            "123 5.639611721038818\n",
            "124 5.579540252685547\n",
            "125 5.519590377807617\n",
            "126 5.458465099334717\n",
            "127 5.398829460144043\n",
            "128 5.338178634643555\n",
            "129 5.278425216674805\n",
            "130 5.218324184417725\n",
            "131 5.158753395080566\n",
            "132 5.099316120147705\n",
            "133 5.039962291717529\n",
            "134 4.980830192565918\n",
            "135 4.921696186065674\n",
            "136 4.863279342651367\n",
            "137 4.804815292358398\n",
            "138 4.747455596923828\n",
            "139 4.6895036697387695\n",
            "140 4.632544994354248\n",
            "141 4.575776100158691\n",
            "142 4.519781589508057\n",
            "143 4.464446067810059\n",
            "144 4.40960168838501\n",
            "145 4.35537576675415\n",
            "146 4.301244258880615\n",
            "147 4.248228549957275\n",
            "148 4.196123123168945\n",
            "149 4.144562721252441\n",
            "150 4.094522953033447\n",
            "151 4.044156074523926\n",
            "152 3.995145082473755\n",
            "153 3.9473631381988525\n",
            "154 3.8999156951904297\n",
            "155 3.8535993099212646\n",
            "156 3.808856964111328\n",
            "157 3.7646355628967285\n",
            "158 3.72108793258667\n",
            "159 3.679285764694214\n",
            "160 3.638172149658203\n",
            "161 3.5980100631713867\n",
            "162 3.558605670928955\n",
            "163 3.520742416381836\n",
            "164 3.4840118885040283\n",
            "165 3.448540687561035\n",
            "166 3.4134390354156494\n",
            "167 3.379532814025879\n",
            "168 3.3467419147491455\n",
            "169 3.3145029544830322\n",
            "170 3.2836453914642334\n",
            "171 3.2541422843933105\n",
            "172 3.2244839668273926\n",
            "173 3.1967906951904297\n",
            "174 3.168778419494629\n",
            "175 3.14275860786438\n",
            "176 3.1168463230133057\n",
            "177 3.0919899940490723\n",
            "178 3.0678956508636475\n",
            "179 3.044281482696533\n",
            "180 3.0216403007507324\n",
            "181 2.9994773864746094\n",
            "182 2.9786317348480225\n",
            "183 2.9571478366851807\n",
            "184 2.9371471405029297\n",
            "185 2.9171488285064697\n",
            "186 2.8977956771850586\n",
            "187 2.8793885707855225\n",
            "188 2.860621452331543\n",
            "189 2.842791795730591\n",
            "190 2.8255720138549805\n",
            "191 2.8074028491973877\n",
            "192 2.7913546562194824\n",
            "193 2.7745463848114014\n",
            "194 2.7593190670013428\n",
            "195 2.7431774139404297\n",
            "196 2.726771593093872\n",
            "197 2.711517095565796\n",
            "198 2.6961734294891357\n",
            "199 2.680744171142578\n",
            "200 2.6673176288604736\n",
            "201 2.652729034423828\n",
            "202 2.637479543685913\n",
            "203 2.623307704925537\n",
            "204 2.609159469604492\n",
            "205 2.595804452896118\n",
            "206 2.5819084644317627\n",
            "207 2.567244529724121\n",
            "208 2.553964614868164\n",
            "209 2.5412096977233887\n",
            "210 2.5287814140319824\n",
            "211 2.5145199298858643\n",
            "212 2.5005226135253906\n",
            "213 2.489004135131836\n",
            "214 2.4750232696533203\n",
            "215 2.4615774154663086\n",
            "216 2.44939923286438\n",
            "217 2.4355874061584473\n",
            "218 2.422809600830078\n",
            "219 2.4102530479431152\n",
            "220 2.396432399749756\n",
            "221 2.385120153427124\n",
            "222 2.3708531856536865\n",
            "223 2.3605849742889404\n",
            "224 2.3467857837677\n",
            "225 2.3324368000030518\n",
            "226 2.321033000946045\n",
            "227 2.3083276748657227\n",
            "228 2.2960152626037598\n",
            "229 2.2823379039764404\n",
            "230 2.2698750495910645\n",
            "231 2.2573184967041016\n",
            "232 2.2454257011413574\n",
            "233 2.232304811477661\n",
            "234 2.220905303955078\n",
            "235 2.2071750164031982\n",
            "236 2.1938600540161133\n",
            "237 2.181419610977173\n",
            "238 2.169802188873291\n",
            "239 2.156320571899414\n",
            "240 2.1433191299438477\n",
            "241 2.1300792694091797\n",
            "242 2.118274450302124\n",
            "243 2.1058080196380615\n",
            "244 2.0920143127441406\n",
            "245 2.0811994075775146\n",
            "246 2.068155527114868\n",
            "247 2.053999185562134\n",
            "248 2.0423338413238525\n",
            "249 2.0305891036987305\n",
            "250 2.0167951583862305\n",
            "251 2.0052428245544434\n",
            "252 1.991134524345398\n",
            "253 1.9797980785369873\n",
            "254 1.966673731803894\n",
            "255 1.9540259838104248\n",
            "256 1.9421650171279907\n",
            "257 1.9294756650924683\n",
            "258 1.9155844449996948\n",
            "259 1.905461072921753\n",
            "260 1.891908884048462\n",
            "261 1.8796741962432861\n",
            "262 1.8684940338134766\n",
            "263 1.853643536567688\n",
            "264 1.8416239023208618\n",
            "265 1.8287445306777954\n",
            "266 1.815069317817688\n",
            "267 1.8028913736343384\n",
            "268 1.7906849384307861\n",
            "269 1.777667760848999\n",
            "270 1.7652521133422852\n",
            "271 1.7530001401901245\n",
            "272 1.7409305572509766\n",
            "273 1.7288532257080078\n",
            "274 1.7149124145507812\n",
            "275 1.7034237384796143\n",
            "276 1.6916922330856323\n",
            "277 1.6793965101242065\n",
            "278 1.6667284965515137\n",
            "279 1.6552870273590088\n",
            "280 1.641925573348999\n",
            "281 1.6324328184127808\n",
            "282 1.6181567907333374\n",
            "283 1.6083118915557861\n",
            "284 1.5934877395629883\n",
            "285 1.5829213857650757\n",
            "286 1.5677509307861328\n",
            "287 1.558797001838684\n",
            "288 1.5457736253738403\n",
            "289 1.534438133239746\n",
            "290 1.5219862461090088\n",
            "291 1.509129524230957\n",
            "292 1.4989601373672485\n",
            "293 1.4876759052276611\n",
            "294 1.4740145206451416\n",
            "295 1.4653236865997314\n",
            "296 1.452392816543579\n",
            "297 1.4395217895507812\n",
            "298 1.4292391538619995\n",
            "299 1.4156267642974854\n",
            "300 1.4070569276809692\n",
            "301 1.3927115201950073\n",
            "302 1.3843766450881958\n",
            "303 1.368977665901184\n",
            "304 1.358384132385254\n",
            "305 1.3469043970108032\n",
            "306 1.3357079029083252\n",
            "307 1.3230522871017456\n",
            "308 1.3116480112075806\n",
            "309 1.3001774549484253\n",
            "310 1.2894474267959595\n",
            "311 1.2789437770843506\n",
            "312 1.2665045261383057\n",
            "313 1.259086012840271\n",
            "314 1.245280146598816\n",
            "315 1.2363148927688599\n",
            "316 1.2225769758224487\n",
            "317 1.2129299640655518\n",
            "318 1.2022323608398438\n",
            "319 1.1906827688217163\n",
            "320 1.1802812814712524\n",
            "321 1.1696126461029053\n",
            "322 1.1591659784317017\n",
            "323 1.148893117904663\n",
            "324 1.1383386850357056\n",
            "325 1.1276602745056152\n",
            "326 1.1172510385513306\n",
            "327 1.1075862646102905\n",
            "328 1.097784399986267\n",
            "329 1.0887583494186401\n",
            "330 1.0761557817459106\n",
            "331 1.0665786266326904\n",
            "332 1.0582903623580933\n",
            "333 1.0467464923858643\n",
            "334 1.037121057510376\n",
            "335 1.0277504920959473\n",
            "336 1.0173332691192627\n",
            "337 1.0077975988388062\n",
            "338 0.9987403154373169\n",
            "339 0.9889407753944397\n",
            "340 0.9792038798332214\n",
            "341 0.970029890537262\n",
            "342 0.9606406092643738\n",
            "343 0.9513756036758423\n",
            "344 0.9424691796302795\n",
            "345 0.934065580368042\n",
            "346 0.924822986125946\n",
            "347 0.9155656099319458\n",
            "348 0.9073505997657776\n",
            "349 0.8988132476806641\n",
            "350 0.8898270726203918\n",
            "351 0.8812856078147888\n",
            "352 0.8724581003189087\n",
            "353 0.8636325597763062\n",
            "354 0.8559110164642334\n",
            "355 0.8472039699554443\n",
            "356 0.8395024538040161\n",
            "357 0.8307850360870361\n",
            "358 0.8227840662002563\n",
            "359 0.8147636651992798\n",
            "360 0.8069648742675781\n",
            "361 0.7984082698822021\n",
            "362 0.7908226251602173\n",
            "363 0.7833709120750427\n",
            "364 0.7751948237419128\n",
            "365 0.7678235769271851\n",
            "366 0.7597542405128479\n",
            "367 0.753224790096283\n",
            "368 0.7456093430519104\n",
            "369 0.7373720407485962\n",
            "370 0.7313096523284912\n",
            "371 0.7235695123672485\n",
            "372 0.716157853603363\n",
            "373 0.7093469500541687\n",
            "374 0.7023553848266602\n",
            "375 0.6950601935386658\n",
            "376 0.6880547404289246\n",
            "377 0.681499719619751\n",
            "378 0.6744052767753601\n",
            "379 0.6680140495300293\n",
            "380 0.6614382863044739\n",
            "381 0.6550701260566711\n",
            "382 0.6484329700469971\n",
            "383 0.6423835158348083\n",
            "384 0.6359464526176453\n",
            "385 0.629996120929718\n",
            "386 0.623461127281189\n",
            "387 0.6172671914100647\n",
            "388 0.6111037135124207\n",
            "389 0.6052567362785339\n",
            "390 0.5993921160697937\n",
            "391 0.5937851071357727\n",
            "392 0.5878251791000366\n",
            "393 0.5820440053939819\n",
            "394 0.576616644859314\n",
            "395 0.5706570148468018\n",
            "396 0.5653828382492065\n",
            "397 0.5599421262741089\n",
            "398 0.5543879866600037\n",
            "399 0.5495970249176025\n",
            "400 0.5438718795776367\n",
            "401 0.5386209487915039\n",
            "402 0.5340818166732788\n",
            "403 0.5285075902938843\n",
            "404 0.5234376192092896\n",
            "405 0.51857990026474\n",
            "406 0.5135893821716309\n",
            "407 0.5085058212280273\n",
            "408 0.5037811398506165\n",
            "409 0.49918118119239807\n",
            "410 0.49448925256729126\n",
            "411 0.4898410141468048\n",
            "412 0.4852946400642395\n",
            "413 0.48072198033332825\n",
            "414 0.47637346386909485\n",
            "415 0.47165337204933167\n",
            "416 0.46742820739746094\n",
            "417 0.46306368708610535\n",
            "418 0.458804726600647\n",
            "419 0.45470649003982544\n",
            "420 0.45056459307670593\n",
            "421 0.4465382695198059\n",
            "422 0.44258347153663635\n",
            "423 0.4382394552230835\n",
            "424 0.43438804149627686\n",
            "425 0.4299982488155365\n",
            "426 0.42631545662879944\n",
            "427 0.4226720631122589\n",
            "428 0.418712854385376\n",
            "429 0.4149923622608185\n",
            "430 0.4112499952316284\n",
            "431 0.40754157304763794\n",
            "432 0.40392643213272095\n",
            "433 0.4004139304161072\n",
            "434 0.39677026867866516\n",
            "435 0.39343470335006714\n",
            "436 0.3899276554584503\n",
            "437 0.38650694489479065\n",
            "438 0.3831727206707001\n",
            "439 0.37986597418785095\n",
            "440 0.3766833543777466\n",
            "441 0.37335899472236633\n",
            "442 0.3701170086860657\n",
            "443 0.3669546842575073\n",
            "444 0.3639329671859741\n",
            "445 0.3607849180698395\n",
            "446 0.3576686978340149\n",
            "447 0.354776531457901\n",
            "448 0.35186097025871277\n",
            "449 0.34884023666381836\n",
            "450 0.34588655829429626\n",
            "451 0.34306463599205017\n",
            "452 0.3402450978755951\n",
            "453 0.337458997964859\n",
            "454 0.3346879184246063\n",
            "455 0.33187735080718994\n",
            "456 0.3292519450187683\n",
            "457 0.3265661597251892\n",
            "458 0.3239116370677948\n",
            "459 0.3212622106075287\n",
            "460 0.3188345432281494\n",
            "461 0.3161853849887848\n",
            "462 0.31372764706611633\n",
            "463 0.31127360463142395\n",
            "464 0.30877241492271423\n",
            "465 0.3063376247882843\n",
            "466 0.3040952682495117\n",
            "467 0.30159857869148254\n",
            "468 0.29909175634384155\n",
            "469 0.29695606231689453\n",
            "470 0.2946898639202118\n",
            "471 0.2924104332923889\n",
            "472 0.2901148498058319\n",
            "473 0.28778156638145447\n",
            "474 0.28566545248031616\n",
            "475 0.28368106484413147\n",
            "476 0.2816615402698517\n",
            "477 0.2792012393474579\n",
            "478 0.2771984040737152\n",
            "479 0.27517595887184143\n",
            "480 0.27312010526657104\n",
            "481 0.2710760235786438\n",
            "482 0.26907747983932495\n",
            "483 0.26722651720046997\n",
            "484 0.2651967406272888\n",
            "485 0.263266384601593\n",
            "486 0.2615240812301636\n",
            "487 0.2595454156398773\n",
            "488 0.257882684469223\n",
            "489 0.255970299243927\n",
            "490 0.25398650765419006\n",
            "491 0.2521716356277466\n",
            "492 0.2503635883331299\n",
            "493 0.24865351617336273\n",
            "494 0.24696655571460724\n",
            "495 0.245186448097229\n",
            "496 0.24344483017921448\n",
            "497 0.2416188269853592\n",
            "498 0.2400158941745758\n",
            "499 0.2384716421365738\n",
            "500 0.23673462867736816\n",
            "501 0.23514845967292786\n",
            "502 0.23356395959854126\n",
            "503 0.23195414245128632\n",
            "504 0.2304803729057312\n",
            "505 0.2288651317358017\n",
            "506 0.22736376523971558\n",
            "507 0.22589509189128876\n",
            "508 0.2242952138185501\n",
            "509 0.22288043797016144\n",
            "510 0.22133520245552063\n",
            "511 0.2199564427137375\n",
            "512 0.21856963634490967\n",
            "513 0.21709012985229492\n",
            "514 0.21568606793880463\n",
            "515 0.21433520317077637\n",
            "516 0.21292468905448914\n",
            "517 0.21149002015590668\n",
            "518 0.21021054685115814\n",
            "519 0.20888687670230865\n",
            "520 0.2075362652540207\n",
            "521 0.20620256662368774\n",
            "522 0.20490935444831848\n",
            "523 0.20360681414604187\n",
            "524 0.2023514211177826\n",
            "525 0.2011193037033081\n",
            "526 0.19984164834022522\n",
            "527 0.19868804514408112\n",
            "528 0.19746245443820953\n",
            "529 0.19634592533111572\n",
            "530 0.1950908750295639\n",
            "531 0.1939321905374527\n",
            "532 0.19278272986412048\n",
            "533 0.19154925644397736\n",
            "534 0.1903752237558365\n",
            "535 0.18927335739135742\n",
            "536 0.18814660608768463\n",
            "537 0.1870276927947998\n",
            "538 0.18598198890686035\n",
            "539 0.18485821783542633\n",
            "540 0.18374931812286377\n",
            "541 0.18274766206741333\n",
            "542 0.18161533772945404\n",
            "543 0.18063373863697052\n",
            "544 0.17953334748744965\n",
            "545 0.1785571575164795\n",
            "546 0.17747074365615845\n",
            "547 0.17648261785507202\n",
            "548 0.17549975216388702\n",
            "549 0.17449542880058289\n",
            "550 0.17358584702014923\n",
            "551 0.17252852022647858\n",
            "552 0.17163565754890442\n",
            "553 0.1706278920173645\n",
            "554 0.16963930428028107\n",
            "555 0.1688014417886734\n",
            "556 0.1678493171930313\n",
            "557 0.16687873005867004\n",
            "558 0.1659822314977646\n",
            "559 0.16507872939109802\n",
            "560 0.16419336199760437\n",
            "561 0.16335196793079376\n",
            "562 0.16246122121810913\n",
            "563 0.16155417263507843\n",
            "564 0.16071516275405884\n",
            "565 0.15989285707473755\n",
            "566 0.15898166596889496\n",
            "567 0.15817749500274658\n",
            "568 0.157328262925148\n",
            "569 0.15653881430625916\n",
            "570 0.1556997448205948\n",
            "571 0.15492165088653564\n",
            "572 0.1541033387184143\n",
            "573 0.15326912701129913\n",
            "574 0.15246228873729706\n",
            "575 0.1517084687948227\n",
            "576 0.15094749629497528\n",
            "577 0.15015213191509247\n",
            "578 0.14939865469932556\n",
            "579 0.14868320524692535\n",
            "580 0.1478915959596634\n",
            "581 0.14717508852481842\n",
            "582 0.14640551805496216\n",
            "583 0.14572560787200928\n",
            "584 0.14503225684165955\n",
            "585 0.14426714181900024\n",
            "586 0.1435597687959671\n",
            "587 0.1428559124469757\n",
            "588 0.14215007424354553\n",
            "589 0.14150089025497437\n",
            "590 0.14081071317195892\n",
            "591 0.1401064544916153\n",
            "592 0.13942447304725647\n",
            "593 0.1387455314397812\n",
            "594 0.13809262216091156\n",
            "595 0.13744334876537323\n",
            "596 0.13679170608520508\n",
            "597 0.13613207638263702\n",
            "598 0.13549865782260895\n",
            "599 0.1348336786031723\n",
            "600 0.1342160999774933\n",
            "601 0.1336134374141693\n",
            "602 0.13296720385551453\n",
            "603 0.13233479857444763\n",
            "604 0.1317332535982132\n",
            "605 0.13111308217048645\n",
            "606 0.1305055171251297\n",
            "607 0.12993715703487396\n",
            "608 0.1293906569480896\n",
            "609 0.1287642866373062\n",
            "610 0.1281626671552658\n",
            "611 0.1276116818189621\n",
            "612 0.12700466811656952\n",
            "613 0.12645117938518524\n",
            "614 0.12590107321739197\n",
            "615 0.12532220780849457\n",
            "616 0.12474919855594635\n",
            "617 0.12420030683279037\n",
            "618 0.12370723485946655\n",
            "619 0.12315619736909866\n",
            "620 0.12260192632675171\n",
            "621 0.12208385020494461\n",
            "622 0.12153806537389755\n",
            "623 0.12100240588188171\n",
            "624 0.12050001323223114\n",
            "625 0.11998721957206726\n",
            "626 0.11946692317724228\n",
            "627 0.11892685294151306\n",
            "628 0.11842232197523117\n",
            "629 0.11792538315057755\n",
            "630 0.1174139752984047\n",
            "631 0.11693777143955231\n",
            "632 0.11646290868520737\n",
            "633 0.11599189788103104\n",
            "634 0.11546006798744202\n",
            "635 0.11495933681726456\n",
            "636 0.11451262980699539\n",
            "637 0.11400934308767319\n",
            "638 0.11355190724134445\n",
            "639 0.11307353526353836\n",
            "640 0.11261171102523804\n",
            "641 0.11215635389089584\n",
            "642 0.11169859766960144\n",
            "643 0.11123934388160706\n",
            "644 0.11079040914773941\n",
            "645 0.11037281900644302\n",
            "646 0.10989108681678772\n",
            "647 0.10945046693086624\n",
            "648 0.10900387167930603\n",
            "649 0.10859794169664383\n",
            "650 0.10814819484949112\n",
            "651 0.10771764814853668\n",
            "652 0.10730690509080887\n",
            "653 0.10687275975942612\n",
            "654 0.10645998269319534\n",
            "655 0.10604017972946167\n",
            "656 0.10562426596879959\n",
            "657 0.10520104318857193\n",
            "658 0.1048000305891037\n",
            "659 0.10439186543226242\n",
            "660 0.10397188365459442\n",
            "661 0.10357468575239182\n",
            "662 0.10321300476789474\n",
            "663 0.10279589891433716\n",
            "664 0.10238151252269745\n",
            "665 0.10202312469482422\n",
            "666 0.1016170084476471\n",
            "667 0.10124015063047409\n",
            "668 0.10088053345680237\n",
            "669 0.10052823275327682\n",
            "670 0.1001141220331192\n",
            "671 0.09974360466003418\n",
            "672 0.09936085343360901\n",
            "673 0.09900057315826416\n",
            "674 0.09864732623100281\n",
            "675 0.09827457368373871\n",
            "676 0.09790108352899551\n",
            "677 0.0975455716252327\n",
            "678 0.09718850255012512\n",
            "679 0.09681900590658188\n",
            "680 0.09647780656814575\n",
            "681 0.0961453765630722\n",
            "682 0.09578428417444229\n",
            "683 0.09543432295322418\n",
            "684 0.09508594125509262\n",
            "685 0.09475770592689514\n",
            "686 0.09440245479345322\n",
            "687 0.09407024830579758\n",
            "688 0.09374382346868515\n",
            "689 0.09341304749250412\n",
            "690 0.09310352802276611\n",
            "691 0.09275361895561218\n",
            "692 0.09241539984941483\n",
            "693 0.09212253242731094\n",
            "694 0.09177540987730026\n",
            "695 0.09147044271230698\n",
            "696 0.09113645553588867\n",
            "697 0.0908333957195282\n",
            "698 0.09051716327667236\n",
            "699 0.09019214659929276\n",
            "700 0.0899086594581604\n",
            "701 0.08959406614303589\n",
            "702 0.08928946405649185\n",
            "703 0.08896888047456741\n",
            "704 0.08869165182113647\n",
            "705 0.08836501836776733\n",
            "706 0.08807326853275299\n",
            "707 0.08777254074811935\n",
            "708 0.08748352527618408\n",
            "709 0.08719973266124725\n",
            "710 0.08690263330936432\n",
            "711 0.08660735934972763\n",
            "712 0.08632414788007736\n",
            "713 0.08603808283805847\n",
            "714 0.08575240522623062\n",
            "715 0.08546353876590729\n",
            "716 0.085203617811203\n",
            "717 0.08491343259811401\n",
            "718 0.08462903648614883\n",
            "719 0.08436893671751022\n",
            "720 0.08408764004707336\n",
            "721 0.08382043242454529\n",
            "722 0.08353440463542938\n",
            "723 0.08328224718570709\n",
            "724 0.0830048993229866\n",
            "725 0.08274319767951965\n",
            "726 0.08247216790914536\n",
            "727 0.08221173286437988\n",
            "728 0.08195175975561142\n",
            "729 0.08167964965105057\n",
            "730 0.08144351840019226\n",
            "731 0.08116477727890015\n",
            "732 0.08092056214809418\n",
            "733 0.08065434545278549\n",
            "734 0.08041493594646454\n",
            "735 0.08015812188386917\n",
            "736 0.07991081476211548\n",
            "737 0.07966096699237823\n",
            "738 0.07940177619457245\n",
            "739 0.07916096597909927\n",
            "740 0.07891065627336502\n",
            "741 0.0786711573600769\n",
            "742 0.0784287378191948\n",
            "743 0.07820341736078262\n",
            "744 0.07796193659305573\n",
            "745 0.07771538943052292\n",
            "746 0.07747835665941238\n",
            "747 0.07724487781524658\n",
            "748 0.07700541615486145\n",
            "749 0.07676775753498077\n",
            "750 0.07653626054525375\n",
            "751 0.07631149888038635\n",
            "752 0.07609077543020248\n",
            "753 0.0758669376373291\n",
            "754 0.07563391327857971\n",
            "755 0.0754031091928482\n",
            "756 0.07518041878938675\n",
            "757 0.07495380192995071\n",
            "758 0.07473243027925491\n",
            "759 0.07452357560396194\n",
            "760 0.074296735227108\n",
            "761 0.07407932728528976\n",
            "762 0.07387157529592514\n",
            "763 0.0736360102891922\n",
            "764 0.07343540340662003\n",
            "765 0.0732092410326004\n",
            "766 0.07301115244626999\n",
            "767 0.07279019802808762\n",
            "768 0.07259819656610489\n",
            "769 0.07238249480724335\n",
            "770 0.07217691838741302\n",
            "771 0.0719597116112709\n",
            "772 0.07174564898014069\n",
            "773 0.07154373079538345\n",
            "774 0.07133843004703522\n",
            "775 0.07113837450742722\n",
            "776 0.07094642519950867\n",
            "777 0.07073841989040375\n",
            "778 0.07055629044771194\n",
            "779 0.07036638259887695\n",
            "780 0.07014662027359009\n",
            "781 0.06995238363742828\n",
            "782 0.06975006312131882\n",
            "783 0.06956367194652557\n",
            "784 0.06937161087989807\n",
            "785 0.0691828727722168\n",
            "786 0.06898890435695648\n",
            "787 0.06880352646112442\n",
            "788 0.06860610097646713\n",
            "789 0.06843216717243195\n",
            "790 0.06823073327541351\n",
            "791 0.06805216521024704\n",
            "792 0.06788139790296555\n",
            "793 0.0676790252327919\n",
            "794 0.0674998015165329\n",
            "795 0.06731688231229782\n",
            "796 0.067137710750103\n",
            "797 0.06695163995027542\n",
            "798 0.06678373366594315\n",
            "799 0.06660253554582596\n",
            "800 0.06641947478055954\n",
            "801 0.06625310331583023\n",
            "802 0.06606495380401611\n",
            "803 0.06588567048311234\n",
            "804 0.06573858857154846\n",
            "805 0.06554969400167465\n",
            "806 0.0653761699795723\n",
            "807 0.0651998445391655\n",
            "808 0.06504511088132858\n",
            "809 0.06487464904785156\n",
            "810 0.0646982192993164\n",
            "811 0.06452717632055283\n",
            "812 0.0643746480345726\n",
            "813 0.06419792026281357\n",
            "814 0.06403619050979614\n",
            "815 0.06386671215295792\n",
            "816 0.06371094286441803\n",
            "817 0.06353957951068878\n",
            "818 0.06338653713464737\n",
            "819 0.06322403252124786\n",
            "820 0.06306219846010208\n",
            "821 0.06289970129728317\n",
            "822 0.06273938715457916\n",
            "823 0.06259423494338989\n",
            "824 0.062431853264570236\n",
            "825 0.06227048113942146\n",
            "826 0.06211045756936073\n",
            "827 0.06196090206503868\n",
            "828 0.06181043013930321\n",
            "829 0.06166401877999306\n",
            "830 0.06150970980525017\n",
            "831 0.06135351210832596\n",
            "832 0.06119897961616516\n",
            "833 0.061052463948726654\n",
            "834 0.060901641845703125\n",
            "835 0.060760870575904846\n",
            "836 0.060602810233831406\n",
            "837 0.0604679249227047\n",
            "838 0.06031104922294617\n",
            "839 0.060161639004945755\n",
            "840 0.06002717837691307\n",
            "841 0.05986662581562996\n",
            "842 0.05972997844219208\n",
            "843 0.059591472148895264\n",
            "844 0.059451136738061905\n",
            "845 0.059313394129276276\n",
            "846 0.059163279831409454\n",
            "847 0.059015654027462006\n",
            "848 0.058883681893348694\n",
            "849 0.05873358994722366\n",
            "850 0.05859412997961044\n",
            "851 0.05846117436885834\n",
            "852 0.058350395411252975\n",
            "853 0.058191072195768356\n",
            "854 0.0580420158803463\n",
            "855 0.05791308730840683\n",
            "856 0.057776983827352524\n",
            "857 0.05765248090028763\n",
            "858 0.05751762539148331\n",
            "859 0.05739438161253929\n",
            "860 0.0572521947324276\n",
            "861 0.05711681395769119\n",
            "862 0.05698082596063614\n",
            "863 0.05685538798570633\n",
            "864 0.05672503262758255\n",
            "865 0.05658727511763573\n",
            "866 0.05646318569779396\n",
            "867 0.056336659938097\n",
            "868 0.05621301010251045\n",
            "869 0.05608026683330536\n",
            "870 0.05595528334379196\n",
            "871 0.05583232641220093\n",
            "872 0.055692873895168304\n",
            "873 0.055572353303432465\n",
            "874 0.055454012006521225\n",
            "875 0.05532928183674812\n",
            "876 0.055199477821588516\n",
            "877 0.055074866861104965\n",
            "878 0.054962437599897385\n",
            "879 0.054830241948366165\n",
            "880 0.05470339581370354\n",
            "881 0.05459034815430641\n",
            "882 0.054467275738716125\n",
            "883 0.05434219539165497\n",
            "884 0.054226018488407135\n",
            "885 0.05410876125097275\n",
            "886 0.05398111790418625\n",
            "887 0.05386563017964363\n",
            "888 0.05374901741743088\n",
            "889 0.05363756790757179\n",
            "890 0.05351709574460983\n",
            "891 0.05339788645505905\n",
            "892 0.05328764766454697\n",
            "893 0.05317027121782303\n",
            "894 0.05305059626698494\n",
            "895 0.052931927144527435\n",
            "896 0.052818235009908676\n",
            "897 0.052705928683280945\n",
            "898 0.05259000509977341\n",
            "899 0.05247892439365387\n",
            "900 0.05236714333295822\n",
            "901 0.0522550567984581\n",
            "902 0.05214064568281174\n",
            "903 0.0520450733602047\n",
            "904 0.051917605102062225\n",
            "905 0.05180946737527847\n",
            "906 0.0517050139605999\n",
            "907 0.05159241706132889\n",
            "908 0.05147398263216019\n",
            "909 0.05137433111667633\n",
            "910 0.05125616863369942\n",
            "911 0.05115780606865883\n",
            "912 0.05104496702551842\n",
            "913 0.05094045773148537\n",
            "914 0.05083500221371651\n",
            "915 0.05072421580553055\n",
            "916 0.05061779171228409\n",
            "917 0.05051691085100174\n",
            "918 0.05040822923183441\n",
            "919 0.05029728263616562\n",
            "920 0.05019994452595711\n",
            "921 0.05008987337350845\n",
            "922 0.04999348521232605\n",
            "923 0.04989112541079521\n",
            "924 0.0497877299785614\n",
            "925 0.049692898988723755\n",
            "926 0.04957503452897072\n",
            "927 0.049475789070129395\n",
            "928 0.049379199743270874\n",
            "929 0.049275461584329605\n",
            "930 0.04917636886239052\n",
            "931 0.049071576446294785\n",
            "932 0.048971544951200485\n",
            "933 0.04887344688177109\n",
            "934 0.04877282306551933\n",
            "935 0.04867668077349663\n",
            "936 0.048578981310129166\n",
            "937 0.04848518222570419\n",
            "938 0.048387251794338226\n",
            "939 0.04828771948814392\n",
            "940 0.04818551987409592\n",
            "941 0.048083849251270294\n",
            "942 0.048000067472457886\n",
            "943 0.047903209924697876\n",
            "944 0.047802217304706573\n",
            "945 0.0477200411260128\n",
            "946 0.04760781675577164\n",
            "947 0.04751179739832878\n",
            "948 0.04742075130343437\n",
            "949 0.047323960810899734\n",
            "950 0.04723746329545975\n",
            "951 0.04714129865169525\n",
            "952 0.04705096781253815\n",
            "953 0.04695897921919823\n",
            "954 0.04685552418231964\n",
            "955 0.046763185411691666\n",
            "956 0.046675361692905426\n",
            "957 0.046586427837610245\n",
            "958 0.046495452523231506\n",
            "959 0.04640506953001022\n",
            "960 0.04632456228137016\n",
            "961 0.04622228443622589\n",
            "962 0.046132516115903854\n",
            "963 0.046041809022426605\n",
            "964 0.04595624655485153\n",
            "965 0.045861583203077316\n",
            "966 0.04577409848570824\n",
            "967 0.04569026082754135\n",
            "968 0.045600466430187225\n",
            "969 0.04550881311297417\n",
            "970 0.04542423039674759\n",
            "971 0.04533937945961952\n",
            "972 0.045260872691869736\n",
            "973 0.04517337679862976\n",
            "974 0.045084789395332336\n",
            "975 0.045000892132520676\n",
            "976 0.044910021126270294\n",
            "977 0.044819317758083344\n",
            "978 0.04473476484417915\n",
            "979 0.04464669153094292\n",
            "980 0.0445709228515625\n",
            "981 0.044484443962574005\n",
            "982 0.04439646378159523\n",
            "983 0.044325996190309525\n",
            "984 0.044236160814762115\n",
            "985 0.04415137693285942\n",
            "986 0.04406662657856941\n",
            "987 0.04399484023451805\n",
            "988 0.043919146060943604\n",
            "989 0.043831679970026016\n",
            "990 0.043741289526224136\n",
            "991 0.04366087168455124\n",
            "992 0.04357977211475372\n",
            "993 0.04349749535322189\n",
            "994 0.043419402092695236\n",
            "995 0.04334224760532379\n",
            "996 0.0432559996843338\n",
            "997 0.04317711293697357\n",
            "998 0.043096959590911865\n",
            "999 0.04302165284752846\n",
            "\"\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "words = open(r\"/content/shakespeare.txt\", 'r' , encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 16\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Attention, self).__init__()\n",
        "    self.atten = nn.Linear(embedded_dim, 3 * embedded_dim)\n",
        "    self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "    self.num_heads = num_heads\n",
        "    self.embedded_dim = embedded_dim\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    # Add an attribute to store attention weights\n",
        "    self.attention_weights = None\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.embedded_dim, dim=2)\n",
        "    q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "    v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    # Store attention weights\n",
        "    self.attention_weights = att\n",
        "\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y\n",
        "\n",
        "dropout = 0.1\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedded_dim):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "    nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "### A simple Transformer Block\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embedded_dim, num_heads):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(embedded_dim,  num_heads)\n",
        "    self.feed_forward = FeedForward(embedded_dim)\n",
        "    self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "    self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "    super(BabyGPTmodel, self).__init__()\n",
        "    self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "    self.positional_embeddings = nn.Embedding(block_size, embedded_dim)\n",
        "    self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_heads)])\n",
        "    self.ln_f = nn.LayerNorm(embedded_dim, eps = 1e-12) # final layer\n",
        "    self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "\n",
        "\n",
        "    # init all weights\n",
        "    ## from karpathy\n",
        "    self.apply(self._init_weights)\n",
        "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "    for pn, p in self.named_parameters():\n",
        "      if pn.endswith('projection.weight'):\n",
        "        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    device = idx.device\n",
        "    b, t = idx.size()\n",
        "    tok_emb = self.token(idx)\n",
        "    position_ids = torch.arange(0, t, dtype = torch.long).unsqueeze(0)\n",
        "    pos_emb = self.positional_embeddings(position_ids)\n",
        "    x = tok_emb + pos_emb\n",
        "    for layers1 in self.layers1:\n",
        "      x = layers1(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_head(x[:, -1, :])\n",
        "    return logits\n",
        "\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+block_size] for i in ix])\n",
        "print( x, y)\n",
        "# Dataset: all pairs (a, b) with label (a+b) mod N\n",
        "\n",
        "\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)\n",
        "\n",
        "## Training\n",
        "for i in range(1000):\n",
        "    logits = gpt(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(i, loss.item())\n",
        "\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode((context)[0].tolist()))\n",
        "# samplinf from the probability distribution is added in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original BabyGPT trainer"
      ],
      "metadata": {
        "id": "QsccZXs_jLkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # these are default GPT-2 hyperparameters\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    bias :bool = False\n",
        "\n",
        "\n",
        "### other hyperparametres\n",
        "batch_size = 64\n",
        "max_iters = 11000\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "words = open(r\"/content/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i:ch for ch,i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype = torch.long)\n",
        "\n",
        "\n",
        "## train and split the data\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+ config.block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+ config.block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "### from pytorch GPT tutorial\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "\n",
        "    self.atten = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "    self.projection = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.size()\n",
        "    q, k ,v  = self.atten(x).split(self.n_embd, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "\n",
        "    # manual implementation of attention\n",
        "    # from karpathy\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.projection(y)\n",
        "    return y\n",
        "\n",
        "dropout = 0.2\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),\n",
        "    nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "### A simple Transformer Block\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.attention = Attention(config)\n",
        "    self.feed_forward = FeedForward(config)\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x + self.attention(self.layer_norm_1(x))\n",
        "    x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BabyGPTmodel, self).__init__()\n",
        "\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "\n",
        "        self.config = config\n",
        "        self.token = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Transformer(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, eps = 1e-12) # final layer norm\n",
        "        self.lnum_heads = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        ## init all weights\n",
        "        ## from karpathy\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "          if pn.endswith('projection.weight'):\n",
        "            torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token(idx)\n",
        "        position_ids = torch.arange(0, T, dtype = torch.long, device = device).unsqueeze(0)\n",
        "        pos_emb =  self.positional_embeddings(position_ids)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "          x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lnum_heads(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    ## from karpathy's youtube videos.\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -config.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "config = GPTConfig(\n",
        "    block_size = 4,\n",
        "    vocab_size = len(chars),\n",
        "    n_head = 4,\n",
        "    n_layer = 4,\n",
        "    n_embd = 16)\n",
        "\n",
        "model = BabyGPTmodel(config)\n",
        "\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "z1Y6ZGLQjIx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Inspection: visualize which input token the last layer attends to"
      ],
      "metadata": {
        "id": "cPru6tHMjkbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention(gpt, x):\n",
        "    attn_maps = []\n",
        "    # Store hook handles to remove them later\n",
        "    hook_handles = []\n",
        "    # Iterate through each Transformer layer\n",
        "    for layer in gpt.layers1:\n",
        "        # Register hook on the Attention module within the Transformer layer\n",
        "        def hook(module, input, output):\n",
        "            # Access the stored attention weights\n",
        "            if module.attention_weights is not None:\n",
        "                 # Apply softmax before storing for visualization (already done in forward)\n",
        "                 attn_maps.append(module.attention_weights.detach())\n",
        "        handle = layer.attention.register_forward_hook(hook)\n",
        "        hook_handles.append(handle) # Store the hook handle\n",
        "\n",
        "    _ = gpt(x)\n",
        "\n",
        "    # Remove hooks after use\n",
        "    for handle in hook_handles:\n",
        "        handle.remove()\n",
        "\n",
        "\n",
        "    return attn_maps\n",
        "\n",
        "# To visualize the attention weights, you would call the function like this:\n",
        "attention_weights = get_attention(gpt = gpt, x = x[: 1])\n",
        "print(attention_weights[1].shape) # Example: print shape of attention weights from the first layer\n",
        "\n",
        "# You can visualize the attention weights for a specific example\n",
        "# For example, to see the attention weights for the first input in the batch (X[0])\n",
        "# and for the first layer and first head:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(attention_weights[1][0][0].squeeze().numpy(), cmap=\"viridis\")\n",
        "plt.title(\"Attention weights for first input, first layer, first head\")\n",
        "plt.xlabel(\"Key Position\")\n",
        "plt.ylabel(\"Query Position\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "RXG_wTUwjdjB",
        "outputId": "05fd2016-34c6-4950-ef7d-d279ceed3b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 8, 8])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Probe"
      ],
      "metadata": {
        "id": "jZba9P47jpYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Probe whether hidden states linearly encode (a+b) mod N\n",
        "with torch.no_grad():\n",
        "    # Corrected to use gpt.token for token embeddings\n",
        "    h = gpt.token(x)\n",
        "    # Corrected to iterate through gpt.layers1 for transformer layers\n",
        "    for layer in gpt.layers1:\n",
        "        h = layer(h)\n",
        "    # Select the hidden state for the last token\n",
        "    h = h[:, -1, :]\n",
        "\n",
        "\n",
        "probe = nn.Linear(embedded_dim, vocab_size , bias=False)\n",
        "# Corrected loss function to F.cross_entropy\n",
        "opt = optim.Adam(probe.parameters(), lr=1e-2)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    logits = probe(h)\n",
        "    # Corrected loss function to F.cross_entropy\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "# Corrected print statement to include a space after the colon\n",
        "print(\"Probe accuracy:\", (logits.argmax(dim=-1) == y).float().mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxbhBZQAjqA5",
        "outputId": "072352c4-81a4-44cb-db1d-1a06458d8e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probe accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original babygpt With KV-cache, MQA and rotary embed (inspired by karpathy ) with MFU."
      ],
      "metadata": {
        "id": "vwxGOvPpjQc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, sys\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# words = \"I have become death, destroyer of worlds\"\n",
        "\n",
        "# words = open(r\"/content/ALL_eminem.txt\", 'r', encoding='utf-8').read()\n",
        "\n",
        "\n",
        "words = open(r\"/content/shakespeare.txt\", 'r', encoding='utf-8').read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i: ch for ch, i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype=torch.long)\n",
        "\n",
        "# config\n",
        "block_size = 128\n",
        "batch_size = 16\n",
        "vocab_size = len(chars)\n",
        "embedded_dim = 64\n",
        "num_heads = 4\n",
        "num_kv_heads = 4\n",
        "num_layers = 4\n",
        "print(vocab_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def apply_rotary_embed(x, cos, sin):\n",
        "    # x: (B, H, T, D) where D is head_dim (even)\n",
        "    B,H,T,D = x.shape\n",
        "    assert D % 2 == 0\n",
        "    D2 = D // 2\n",
        "    x1 = x[..., :D2]\n",
        "    x2 = x[..., D2:]\n",
        "    # cos,sin: (1, T, 1, D2) or broadcastable\n",
        "    y1 = x1 * cos + x2 * sin\n",
        "    y2 = x1 * (-sin) + x2 * cos\n",
        "    return torch.cat([y1, y2], dim=-1)\n",
        "\n",
        "def repeat_kv(x, n_rep):\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    B, H, T, D = x.shape\n",
        "    return x.unsqueeze(2).expand(B, H, n_rep, T, D).reshape(B, H * n_rep, T, D)\n",
        "\n",
        "class KVCache:\n",
        "    def __init__(self, num_layers, max_len, device, dtype=torch.float32):\n",
        "        self.num_layers = num_layers\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "        self.k = [None]*num_layers\n",
        "        self.v = [None]*num_layers\n",
        "        self.pos = 0\n",
        "    def insert_kv(self, layer_idx, k_curr, v_curr):\n",
        "        B,H,Tcurr,D = k_curr.shape\n",
        "        if self.k[layer_idx] is None:\n",
        "            self.k[layer_idx] = torch.zeros(B, H, self.max_len, D, device=self.device, dtype=self.dtype)\n",
        "            self.v[layer_idx] = torch.zeros(B, H, self.max_len, D, device=self.device, dtype=self.dtype)\n",
        "        start = self.pos\n",
        "        end = start + Tcurr\n",
        "        if end > self.max_len:\n",
        "            raise IndexError(\"KV cache overflow\")\n",
        "        self.k[layer_idx][:,:,:end,:] = self.k[layer_idx][:,:,:end,:]  # no-op to keep shape ok\n",
        "        # insert at positions start:end\n",
        "        self.k[layer_idx][:,:,start:end,:] = k_curr\n",
        "        self.v[layer_idx][:,:,start:end,:] = v_curr\n",
        "        return self.k[layer_idx][:,:,:end,:], self.v[layer_idx][:,:,:end,:]\n",
        "    def get_pos(self):\n",
        "        return self.pos\n",
        "    def advance(self, n):\n",
        "        self.pos += n\n",
        "    def reset(self):\n",
        "        self.k = [None]*self.num_layers\n",
        "        self.v = [None]*self.num_layers\n",
        "        self.pos = 0\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embedded_dim, num_heads, num_kv_heads, block_size):\n",
        "        super().__init__()\n",
        "        assert embedded_dim % num_heads == 0\n",
        "        assert num_heads % num_kv_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.head_dim = embedded_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.q_proj = nn.Linear(embedded_dim, num_heads * self.head_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(embedded_dim, num_kv_heads * self.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(embedded_dim, num_kv_heads * self.head_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(num_heads * self.head_dim, embedded_dim, bias=False)\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).bool())\n",
        "        self.attention_weights = None # Initialize attention_weights\n",
        "\n",
        "    def forward(self, x, cos_sin, layer_idx, kv_cache=None):\n",
        "        B,T,C = x.shape\n",
        "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).permute(0,2,1,3)  # (B,Hq,T,D)\n",
        "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).permute(0,2,1,3)  # (B,Hk,T,D)\n",
        "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).permute(0,2,1,3)  # (B,Hk,T,D)\n",
        "        cos, sin = cos_sin  # cos: (1, seq_len, 1, D2) where D2 = head_dim/2\n",
        "        # ensure cos/sin last dim matches head_dim//2\n",
        "        D2 = self.head_dim // 2\n",
        "        assert cos.shape[-1] == D2, f\"cos last dim {cos.shape[-1]} != {D2}\"\n",
        "        q = apply_rotary_embed(q, cos[:, :, :T, :], sin[:, :, :T, :])  # (B,Hq,T,D)\n",
        "        k = apply_rotary_embed(k, cos[:, :, :T, :], sin[:, :, :T, :])  # (B,Hk,T,D)\n",
        "        if kv_cache is not None:\n",
        "            k_full, v_full = kv_cache.insert_kv(layer_idx, k, v)  # (B,Hk,Tfull,D)\n",
        "            Tq = q.size(2); Tk = k_full.size(2)\n",
        "            nrep = self.num_heads // self.num_kv_heads\n",
        "            k_rep = repeat_kv(k_full, nrep)  # (B,Hq,Tk,D)\n",
        "            v_rep = repeat_kv(v_full, nrep)\n",
        "            attn_scores = torch.matmul(q, k_rep.transpose(-2,-1)) * self.scale  # (B,Hq,Tq,Tk)\n",
        "            prefix_len = Tk - Tq\n",
        "            attn_mask = torch.zeros((Tq, Tk), dtype=torch.bool, device=attn_scores.device)\n",
        "            if prefix_len > 0:\n",
        "                attn_mask[:, :prefix_len] = True\n",
        "            tri = torch.tril(torch.ones((Tq, Tq), dtype=torch.bool, device=attn_scores.device))\n",
        "            attn_mask[:, prefix_len:] = tri\n",
        "            attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n",
        "            attn_scores = attn_scores.masked_fill(~attn_mask, float('-inf'))\n",
        "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "            self.attention_weights = attn_probs # Store attention weights\n",
        "            out = torch.matmul(attn_probs, v_rep)\n",
        "        else:\n",
        "            nrep = self.num_heads // self.num_kv_heads\n",
        "            k_rep = repeat_kv(k, nrep)  # (B,Hq,T,D)\n",
        "            v_rep = repeat_kv(v, nrep)\n",
        "            attn_scores = torch.matmul(q, k_rep.transpose(-2,-1)) * self.scale  # (B,Hq,T,T)\n",
        "            mask = self.mask[:T,:T].to(attn_scores.device)\n",
        "            attn_scores = attn_scores.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "            self.attention_weights = attn_probs # Store attention weights\n",
        "            out = torch.matmul(attn_probs, v_rep)  # (B,Hq,T,D)\n",
        "        out = out.permute(0,2,1,3).contiguous().view(B, T, -1)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedded_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embedded_dim, 4*embedded_dim)\n",
        "        self.fc2 = nn.Linear(4*embedded_dim, embedded_dim)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedded_dim, num_heads, num_kv_heads, block_size):\n",
        "        super().__init__()\n",
        "        self.attn = Attention(embedded_dim, num_heads, num_kv_heads, block_size)\n",
        "        self.ff = FeedForward(embedded_dim)\n",
        "        self.norm1 = nn.LayerNorm(embedded_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedded_dim)\n",
        "    def forward(self, x, cos_sin, layer_idx, kv_cache=None):\n",
        "        x = x + self.attn(self.norm1(x), cos_sin, layer_idx, kv_cache=kv_cache)\n",
        "        x = x + self.ff(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class BabyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads, num_kv_heads, device):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "        self.positional = nn.Embedding(block_size, embedded_dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embedded_dim, num_heads, num_kv_heads, block_size) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embedded_dim)\n",
        "        self.head = nn.Linear(embedded_dim, vocab_size, bias=False)\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.embedded_dim = embedded_dim\n",
        "        self.block_size = block_size\n",
        "        head_dim = embedded_dim // num_heads\n",
        "        D2 = head_dim // 2\n",
        "        seq_max = block_size * 10\n",
        "        # compute cos/sin with last dim D2\n",
        "        channel_range = torch.arange(0, D2, dtype=torch.float32, device=device)\n",
        "        inv_freq = 1.0 / (10000 ** (channel_range / D2))\n",
        "        t = torch.arange(seq_max, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "        cos = freqs.cos()[None, None, :, :].to(torch.float32)\n",
        "        sin = freqs.sin()[None, None, :, :].to(torch.float32)\n",
        "        self.register_buffer(\"cos\", cos)\n",
        "        self.register_buffer(\"sin\", sin)\n",
        "\n",
        "    def forward(self, idx, targets=None, kv_cache=None):\n",
        "        device = idx.device\n",
        "        B,T = idx.shape\n",
        "        assert T <= self.block_size\n",
        "        pos = torch.arange(0, T, device=device).unsqueeze(0)\n",
        "        x = self.token(idx) + self.positional(pos)\n",
        "        cos = self.cos[:, :, :T, :]\n",
        "        sin = self.sin[:, :, :T, :]\n",
        "        cos_sin = (cos, sin)\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            x = layer(x, cos_sin, layer_idx, kv_cache=kv_cache)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "            return logits, loss\n",
        "        return logits\n",
        "\n",
        "    def model_flops(self, batch_size, dt):\n",
        "        # from https://arxiv.org/pdf/2204.02311.pdf section B\n",
        "        N = sum(p.numel() for p in self.parameters())  # Total number of parameters\n",
        "\n",
        "        H = self.num_heads  # Number of attention heads\n",
        "        Q = self.embedded_dim // self.num_heads  # Dimension per head\n",
        "        T = self.block_size  # Block size / sequence length\n",
        "        L = self.num_layers  # Number of transformer layers\n",
        "\n",
        "        # FLOPs for one forward pass (approximate)\n",
        "        # 6N: for MLP and embedding operations related to parameters N\n",
        "        # 12LHQT: for attention mechanism across layers\n",
        "        flops = 6 * N + 12 * L * H * Q * T\n",
        "        flops_per_for_back = flops * T  # FLOPs per forward/backward pass for a sequence of length T\n",
        "        flops_per_iteration = batch_size * flops_per_for_back  # Total FLOPs for one training iteration\n",
        "        flops_received = flops_per_iteration * (1.0 / dt)  # FLOPs per second\n",
        "        theoretical_flops = 8e12  # A common value for a Tesla T4 GPU\n",
        "        mfu = flops_received / theoretical_flops\n",
        "\n",
        "        return mfu\n",
        "\n",
        "# data\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "\n",
        "model = BabyGPT(vocab_size, block_size, num_layers, embedded_dim, num_heads, num_kv_heads, device).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Model param count:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "for step in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Step {step:02d} Loss {loss.item():.4f}\")\n",
        "\n",
        "# KV cache demo\n",
        "model.eval()\n",
        "kv = KVCache(num_layers, max_len=block_size*5, device=device, dtype=next(model.parameters()).dtype)\n",
        "kv.reset()\n",
        "# start with single token from batch 0\n",
        "start = x[0:1, 0:1]\n",
        "generated = [int(start[0,0].item())]\n",
        "for t in range(block_size-1):\n",
        "    logits = model(start, kv_cache=kv)  # (1, T, V)\n",
        "    last = logits[:, -1, :]\n",
        "    probs = F.softmax(last, dim=-1)\n",
        "    nxt = torch.multinomial(probs, num_samples=1)\n",
        "    generated.append(int(nxt.item()))\n",
        "    # insert next token into start sequence (autoregressive)\n",
        "    start = torch.cat([start, nxt], dim=1)\n",
        "    kv.advance(1)\n",
        "print(\"Generated indices:\", generated)\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKBPIXGjWnS",
        "outputId": "74836e96-c798-4428-995b-069eae462bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84\n",
            "Device: cpu\n",
            "Model param count: 217984\n",
            "Step 00 Loss 4.5633\n",
            "Step 01 Loss 4.3805\n",
            "Step 02 Loss 4.2045\n",
            "Step 03 Loss 4.0358\n",
            "Step 04 Loss 3.8795\n",
            "Step 05 Loss 3.7442\n",
            "Step 06 Loss 3.6363\n",
            "Step 07 Loss 3.5547\n",
            "Step 08 Loss 3.4906\n",
            "Step 09 Loss 3.4355\n",
            "Step 10 Loss 3.3851\n",
            "Step 11 Loss 3.3393\n",
            "Step 12 Loss 3.2993\n",
            "Step 13 Loss 3.2658\n",
            "Step 14 Loss 3.2374\n",
            "Step 15 Loss 3.2121\n",
            "Step 16 Loss 3.1876\n",
            "Step 17 Loss 3.1629\n",
            "Step 18 Loss 3.1378\n",
            "Step 19 Loss 3.1126\n",
            "Step 20 Loss 3.0880\n",
            "Step 21 Loss 3.0640\n",
            "Step 22 Loss 3.0404\n",
            "Step 23 Loss 3.0167\n",
            "Step 24 Loss 2.9922\n",
            "Step 25 Loss 2.9666\n",
            "Step 26 Loss 2.9405\n",
            "Step 27 Loss 2.9149\n",
            "Step 28 Loss 2.8911\n",
            "Step 29 Loss 2.8695\n",
            "Step 30 Loss 2.8498\n",
            "Step 31 Loss 2.8307\n",
            "Step 32 Loss 2.8112\n",
            "Step 33 Loss 2.7911\n",
            "Step 34 Loss 2.7707\n",
            "Step 35 Loss 2.7505\n",
            "Step 36 Loss 2.7307\n",
            "Step 37 Loss 2.7109\n",
            "Step 38 Loss 2.6909\n",
            "Step 39 Loss 2.6705\n",
            "Step 40 Loss 2.6501\n",
            "Step 41 Loss 2.6301\n",
            "Step 42 Loss 2.6104\n",
            "Step 43 Loss 2.5906\n",
            "Step 44 Loss 2.5705\n",
            "Step 45 Loss 2.5498\n",
            "Step 46 Loss 2.5290\n",
            "Step 47 Loss 2.5081\n",
            "Step 48 Loss 2.4870\n",
            "Step 49 Loss 2.4657\n",
            "Step 50 Loss 2.4439\n",
            "Step 51 Loss 2.4217\n",
            "Step 52 Loss 2.3991\n",
            "Step 53 Loss 2.3757\n",
            "Step 54 Loss 2.3514\n",
            "Step 55 Loss 2.3264\n",
            "Step 56 Loss 2.3008\n",
            "Step 57 Loss 2.2742\n",
            "Step 58 Loss 2.2464\n",
            "Step 59 Loss 2.2177\n",
            "Step 60 Loss 2.1880\n",
            "Step 61 Loss 2.1570\n",
            "Step 62 Loss 2.1249\n",
            "Step 63 Loss 2.0913\n",
            "Step 64 Loss 2.0561\n",
            "Step 65 Loss 2.0196\n",
            "Step 66 Loss 1.9813\n",
            "Step 67 Loss 1.9416\n",
            "Step 68 Loss 1.9003\n",
            "Step 69 Loss 1.8577\n",
            "Step 70 Loss 1.8135\n",
            "Step 71 Loss 1.7680\n",
            "Step 72 Loss 1.7215\n",
            "Step 73 Loss 1.6739\n",
            "Step 74 Loss 1.6252\n",
            "Step 75 Loss 1.5758\n",
            "Step 76 Loss 1.5260\n",
            "Step 77 Loss 1.4770\n",
            "Step 78 Loss 1.4327\n",
            "Step 79 Loss 1.3929\n",
            "Step 80 Loss 1.3386\n",
            "Step 81 Loss 1.2838\n",
            "Step 82 Loss 1.2487\n",
            "Step 83 Loss 1.1950\n",
            "Step 84 Loss 1.1487\n",
            "Step 85 Loss 1.1087\n",
            "Step 86 Loss 1.0562\n",
            "Step 87 Loss 1.0180\n",
            "Step 88 Loss 0.9708\n",
            "Step 89 Loss 0.9290\n",
            "Step 90 Loss 0.8882\n",
            "Step 91 Loss 0.8446\n",
            "Step 92 Loss 0.8072\n",
            "Step 93 Loss 0.7658\n",
            "Step 94 Loss 0.7272\n",
            "Step 95 Loss 0.6898\n",
            "Step 96 Loss 0.6520\n",
            "Step 97 Loss 0.6165\n",
            "Step 98 Loss 0.5816\n",
            "Step 99 Loss 0.5481\n",
            "Step 100 Loss 0.5152\n",
            "Step 101 Loss 0.4838\n",
            "Step 102 Loss 0.4530\n",
            "Step 103 Loss 0.4240\n",
            "Step 104 Loss 0.3954\n",
            "Step 105 Loss 0.3693\n",
            "Step 106 Loss 0.3442\n",
            "Step 107 Loss 0.3199\n",
            "Step 108 Loss 0.2986\n",
            "Step 109 Loss 0.2774\n",
            "Step 110 Loss 0.2582\n",
            "Step 111 Loss 0.2406\n",
            "Step 112 Loss 0.2238\n",
            "Step 113 Loss 0.2082\n",
            "Step 114 Loss 0.1943\n",
            "Step 115 Loss 0.1808\n",
            "Step 116 Loss 0.1683\n",
            "Step 117 Loss 0.1573\n",
            "Step 118 Loss 0.1466\n",
            "Step 119 Loss 0.1369\n",
            "Step 120 Loss 0.1282\n",
            "Step 121 Loss 0.1198\n",
            "Step 122 Loss 0.1126\n",
            "Step 123 Loss 0.1055\n",
            "Step 124 Loss 0.0994\n",
            "Step 125 Loss 0.0936\n",
            "Step 126 Loss 0.0883\n",
            "Step 127 Loss 0.0835\n",
            "Step 128 Loss 0.0790\n",
            "Step 129 Loss 0.0750\n",
            "Step 130 Loss 0.0712\n",
            "Step 131 Loss 0.0677\n",
            "Step 132 Loss 0.0645\n",
            "Step 133 Loss 0.0615\n",
            "Step 134 Loss 0.0588\n",
            "Step 135 Loss 0.0562\n",
            "Step 136 Loss 0.0538\n",
            "Step 137 Loss 0.0516\n",
            "Step 138 Loss 0.0496\n",
            "Step 139 Loss 0.0476\n",
            "Step 140 Loss 0.0459\n",
            "Step 141 Loss 0.0442\n",
            "Step 142 Loss 0.0426\n",
            "Step 143 Loss 0.0412\n",
            "Step 144 Loss 0.0398\n",
            "Step 145 Loss 0.0385\n",
            "Step 146 Loss 0.0373\n",
            "Step 147 Loss 0.0362\n",
            "Step 148 Loss 0.0352\n",
            "Step 149 Loss 0.0342\n",
            "Step 150 Loss 0.0333\n",
            "Step 151 Loss 0.0324\n",
            "Step 152 Loss 0.0316\n",
            "Step 153 Loss 0.0308\n",
            "Step 154 Loss 0.0300\n",
            "Step 155 Loss 0.0293\n",
            "Step 156 Loss 0.0287\n",
            "Step 157 Loss 0.0281\n",
            "Step 158 Loss 0.0275\n",
            "Step 159 Loss 0.0269\n",
            "Step 160 Loss 0.0264\n",
            "Step 161 Loss 0.0258\n",
            "Step 162 Loss 0.0254\n",
            "Step 163 Loss 0.0249\n",
            "Step 164 Loss 0.0244\n",
            "Step 165 Loss 0.0240\n",
            "Step 166 Loss 0.0236\n",
            "Step 167 Loss 0.0232\n",
            "Step 168 Loss 0.0228\n",
            "Step 169 Loss 0.0225\n",
            "Step 170 Loss 0.0221\n",
            "Step 171 Loss 0.0218\n",
            "Step 172 Loss 0.0215\n",
            "Step 173 Loss 0.0212\n",
            "Step 174 Loss 0.0209\n",
            "Step 175 Loss 0.0206\n",
            "Step 176 Loss 0.0203\n",
            "Step 177 Loss 0.0200\n",
            "Step 178 Loss 0.0198\n",
            "Step 179 Loss 0.0195\n",
            "Step 180 Loss 0.0193\n",
            "Step 181 Loss 0.0190\n",
            "Step 182 Loss 0.0188\n",
            "Step 183 Loss 0.0186\n",
            "Step 184 Loss 0.0184\n",
            "Step 185 Loss 0.0182\n",
            "Step 186 Loss 0.0180\n",
            "Step 187 Loss 0.0178\n",
            "Step 188 Loss 0.0176\n",
            "Step 189 Loss 0.0174\n",
            "Step 190 Loss 0.0172\n",
            "Step 191 Loss 0.0170\n",
            "Step 192 Loss 0.0168\n",
            "Step 193 Loss 0.0167\n",
            "Step 194 Loss 0.0165\n",
            "Step 195 Loss 0.0163\n",
            "Step 196 Loss 0.0162\n",
            "Step 197 Loss 0.0160\n",
            "Step 198 Loss 0.0159\n",
            "Step 199 Loss 0.0157\n",
            "Step 200 Loss 0.0156\n",
            "Step 201 Loss 0.0155\n",
            "Step 202 Loss 0.0153\n",
            "Step 203 Loss 0.0152\n",
            "Step 204 Loss 0.0151\n",
            "Step 205 Loss 0.0149\n",
            "Step 206 Loss 0.0148\n",
            "Step 207 Loss 0.0147\n",
            "Step 208 Loss 0.0146\n",
            "Step 209 Loss 0.0144\n",
            "Step 210 Loss 0.0143\n",
            "Step 211 Loss 0.0142\n",
            "Step 212 Loss 0.0141\n",
            "Step 213 Loss 0.0140\n",
            "Step 214 Loss 0.0139\n",
            "Step 215 Loss 0.0138\n",
            "Step 216 Loss 0.0137\n",
            "Step 217 Loss 0.0136\n",
            "Step 218 Loss 0.0135\n",
            "Step 219 Loss 0.0134\n",
            "Step 220 Loss 0.0133\n",
            "Step 221 Loss 0.0132\n",
            "Step 222 Loss 0.0131\n",
            "Step 223 Loss 0.0130\n",
            "Step 224 Loss 0.0129\n",
            "Step 225 Loss 0.0128\n",
            "Step 226 Loss 0.0128\n",
            "Step 227 Loss 0.0127\n",
            "Step 228 Loss 0.0126\n",
            "Step 229 Loss 0.0125\n",
            "Step 230 Loss 0.0124\n",
            "Step 231 Loss 0.0123\n",
            "Step 232 Loss 0.0123\n",
            "Step 233 Loss 0.0122\n",
            "Step 234 Loss 0.0121\n",
            "Step 235 Loss 0.0120\n",
            "Step 236 Loss 0.0120\n",
            "Step 237 Loss 0.0119\n",
            "Step 238 Loss 0.0118\n",
            "Step 239 Loss 0.0118\n",
            "Step 240 Loss 0.0117\n",
            "Step 241 Loss 0.0116\n",
            "Step 242 Loss 0.0116\n",
            "Step 243 Loss 0.0115\n",
            "Step 244 Loss 0.0114\n",
            "Step 245 Loss 0.0114\n",
            "Step 246 Loss 0.0113\n",
            "Step 247 Loss 0.0112\n",
            "Step 248 Loss 0.0112\n",
            "Step 249 Loss 0.0111\n",
            "Step 250 Loss 0.0111\n",
            "Step 251 Loss 0.0110\n",
            "Step 252 Loss 0.0109\n",
            "Step 253 Loss 0.0109\n",
            "Step 254 Loss 0.0108\n",
            "Step 255 Loss 0.0108\n",
            "Step 256 Loss 0.0107\n",
            "Step 257 Loss 0.0107\n",
            "Step 258 Loss 0.0106\n",
            "Step 259 Loss 0.0105\n",
            "Step 260 Loss 0.0105\n",
            "Step 261 Loss 0.0104\n",
            "Step 262 Loss 0.0104\n",
            "Step 263 Loss 0.0103\n",
            "Step 264 Loss 0.0103\n",
            "Step 265 Loss 0.0102\n",
            "Step 266 Loss 0.0102\n",
            "Step 267 Loss 0.0102\n",
            "Step 268 Loss 0.0101\n",
            "Step 269 Loss 0.0101\n",
            "Step 270 Loss 0.0100\n",
            "Step 271 Loss 0.0100\n",
            "Step 272 Loss 0.0099\n",
            "Step 273 Loss 0.0099\n",
            "Step 274 Loss 0.0098\n",
            "Step 275 Loss 0.0098\n",
            "Step 276 Loss 0.0097\n",
            "Step 277 Loss 0.0097\n",
            "Step 278 Loss 0.0097\n",
            "Step 279 Loss 0.0096\n",
            "Step 280 Loss 0.0096\n",
            "Step 281 Loss 0.0095\n",
            "Step 282 Loss 0.0095\n",
            "Step 283 Loss 0.0095\n",
            "Step 284 Loss 0.0094\n",
            "Step 285 Loss 0.0094\n",
            "Step 286 Loss 0.0093\n",
            "Step 287 Loss 0.0093\n",
            "Step 288 Loss 0.0093\n",
            "Step 289 Loss 0.0092\n",
            "Step 290 Loss 0.0092\n",
            "Step 291 Loss 0.0092\n",
            "Step 292 Loss 0.0091\n",
            "Step 293 Loss 0.0091\n",
            "Step 294 Loss 0.0091\n",
            "Step 295 Loss 0.0090\n",
            "Step 296 Loss 0.0090\n",
            "Step 297 Loss 0.0090\n",
            "Step 298 Loss 0.0089\n",
            "Step 299 Loss 0.0089\n",
            "Step 300 Loss 0.0089\n",
            "Step 301 Loss 0.0088\n",
            "Step 302 Loss 0.0088\n",
            "Step 303 Loss 0.0088\n",
            "Step 304 Loss 0.0087\n",
            "Step 305 Loss 0.0087\n",
            "Step 306 Loss 0.0087\n",
            "Step 307 Loss 0.0086\n",
            "Step 308 Loss 0.0086\n",
            "Step 309 Loss 0.0086\n",
            "Step 310 Loss 0.0085\n",
            "Step 311 Loss 0.0085\n",
            "Step 312 Loss 0.0085\n",
            "Step 313 Loss 0.0085\n",
            "Step 314 Loss 0.0084\n",
            "Step 315 Loss 0.0084\n",
            "Step 316 Loss 0.0084\n",
            "Step 317 Loss 0.0083\n",
            "Step 318 Loss 0.0083\n",
            "Step 319 Loss 0.0083\n",
            "Step 320 Loss 0.0083\n",
            "Step 321 Loss 0.0082\n",
            "Step 322 Loss 0.0082\n",
            "Step 323 Loss 0.0082\n",
            "Step 324 Loss 0.0082\n",
            "Step 325 Loss 0.0081\n",
            "Step 326 Loss 0.0081\n",
            "Step 327 Loss 0.0081\n",
            "Step 328 Loss 0.0081\n",
            "Step 329 Loss 0.0080\n",
            "Step 330 Loss 0.0080\n",
            "Step 331 Loss 0.0080\n",
            "Step 332 Loss 0.0080\n",
            "Step 333 Loss 0.0079\n",
            "Step 334 Loss 0.0079\n",
            "Step 335 Loss 0.0079\n",
            "Step 336 Loss 0.0079\n",
            "Step 337 Loss 0.0078\n",
            "Step 338 Loss 0.0078\n",
            "Step 339 Loss 0.0078\n",
            "Step 340 Loss 0.0078\n",
            "Step 341 Loss 0.0078\n",
            "Step 342 Loss 0.0077\n",
            "Step 343 Loss 0.0077\n",
            "Step 344 Loss 0.0077\n",
            "Step 345 Loss 0.0077\n",
            "Step 346 Loss 0.0077\n",
            "Step 347 Loss 0.0076\n",
            "Step 348 Loss 0.0076\n",
            "Step 349 Loss 0.0076\n",
            "Step 350 Loss 0.0076\n",
            "Step 351 Loss 0.0075\n",
            "Step 352 Loss 0.0075\n",
            "Step 353 Loss 0.0075\n",
            "Step 354 Loss 0.0075\n",
            "Step 355 Loss 0.0075\n",
            "Step 356 Loss 0.0074\n",
            "Step 357 Loss 0.0074\n",
            "Step 358 Loss 0.0074\n",
            "Step 359 Loss 0.0074\n",
            "Step 360 Loss 0.0074\n",
            "Step 361 Loss 0.0074\n",
            "Step 362 Loss 0.0073\n",
            "Step 363 Loss 0.0073\n",
            "Step 364 Loss 0.0073\n",
            "Step 365 Loss 0.0073\n",
            "Step 366 Loss 0.0073\n",
            "Step 367 Loss 0.0072\n",
            "Step 368 Loss 0.0072\n",
            "Step 369 Loss 0.0072\n",
            "Step 370 Loss 0.0072\n",
            "Step 371 Loss 0.0072\n",
            "Step 372 Loss 0.0072\n",
            "Step 373 Loss 0.0071\n",
            "Step 374 Loss 0.0071\n",
            "Step 375 Loss 0.0071\n",
            "Step 376 Loss 0.0071\n",
            "Step 377 Loss 0.0071\n",
            "Step 378 Loss 0.0071\n",
            "Step 379 Loss 0.0070\n",
            "Step 380 Loss 0.0070\n",
            "Step 381 Loss 0.0070\n",
            "Step 382 Loss 0.0070\n",
            "Step 383 Loss 0.0070\n",
            "Step 384 Loss 0.0070\n",
            "Step 385 Loss 0.0069\n",
            "Step 386 Loss 0.0069\n",
            "Step 387 Loss 0.0069\n",
            "Step 388 Loss 0.0069\n",
            "Step 389 Loss 0.0069\n",
            "Step 390 Loss 0.0069\n",
            "Step 391 Loss 0.0069\n",
            "Step 392 Loss 0.0068\n",
            "Step 393 Loss 0.0068\n",
            "Step 394 Loss 0.0068\n",
            "Step 395 Loss 0.0068\n",
            "Step 396 Loss 0.0068\n",
            "Step 397 Loss 0.0068\n",
            "Step 398 Loss 0.0068\n",
            "Step 399 Loss 0.0067\n",
            "Step 400 Loss 0.0067\n",
            "Step 401 Loss 0.0067\n",
            "Step 402 Loss 0.0067\n",
            "Step 403 Loss 0.0067\n",
            "Step 404 Loss 0.0067\n",
            "Step 405 Loss 0.0067\n",
            "Step 406 Loss 0.0066\n",
            "Step 407 Loss 0.0066\n",
            "Step 408 Loss 0.0066\n",
            "Step 409 Loss 0.0066\n",
            "Step 410 Loss 0.0066\n",
            "Step 411 Loss 0.0066\n",
            "Step 412 Loss 0.0066\n",
            "Step 413 Loss 0.0066\n",
            "Step 414 Loss 0.0065\n",
            "Step 415 Loss 0.0065\n",
            "Step 416 Loss 0.0065\n",
            "Step 417 Loss 0.0065\n",
            "Step 418 Loss 0.0065\n",
            "Step 419 Loss 0.0065\n",
            "Step 420 Loss 0.0065\n",
            "Step 421 Loss 0.0065\n",
            "Step 422 Loss 0.0064\n",
            "Step 423 Loss 0.0064\n",
            "Step 424 Loss 0.0064\n",
            "Step 425 Loss 0.0064\n",
            "Step 426 Loss 0.0064\n",
            "Step 427 Loss 0.0064\n",
            "Step 428 Loss 0.0064\n",
            "Step 429 Loss 0.0064\n",
            "Step 430 Loss 0.0063\n",
            "Step 431 Loss 0.0063\n",
            "Step 432 Loss 0.0063\n",
            "Step 433 Loss 0.0063\n",
            "Step 434 Loss 0.0063\n",
            "Step 435 Loss 0.0063\n",
            "Step 436 Loss 0.0063\n",
            "Step 437 Loss 0.0063\n",
            "Step 438 Loss 0.0063\n",
            "Step 439 Loss 0.0062\n",
            "Step 440 Loss 0.0062\n",
            "Step 441 Loss 0.0062\n",
            "Step 442 Loss 0.0062\n",
            "Step 443 Loss 0.0062\n",
            "Step 444 Loss 0.0062\n",
            "Step 445 Loss 0.0062\n",
            "Step 446 Loss 0.0062\n",
            "Step 447 Loss 0.0062\n",
            "Step 448 Loss 0.0062\n",
            "Step 449 Loss 0.0061\n",
            "Step 450 Loss 0.0061\n",
            "Step 451 Loss 0.0061\n",
            "Step 452 Loss 0.0061\n",
            "Step 453 Loss 0.0061\n",
            "Step 454 Loss 0.0061\n",
            "Step 455 Loss 0.0061\n",
            "Step 456 Loss 0.0061\n",
            "Step 457 Loss 0.0061\n",
            "Step 458 Loss 0.0061\n",
            "Step 459 Loss 0.0060\n",
            "Step 460 Loss 0.0060\n",
            "Step 461 Loss 0.0060\n",
            "Step 462 Loss 0.0060\n",
            "Step 463 Loss 0.0060\n",
            "Step 464 Loss 0.0060\n",
            "Step 465 Loss 0.0060\n",
            "Step 466 Loss 0.0060\n",
            "Step 467 Loss 0.0060\n",
            "Step 468 Loss 0.0060\n",
            "Step 469 Loss 0.0060\n",
            "Step 470 Loss 0.0059\n",
            "Step 471 Loss 0.0059\n",
            "Step 472 Loss 0.0059\n",
            "Step 473 Loss 0.0059\n",
            "Step 474 Loss 0.0059\n",
            "Step 475 Loss 0.0059\n",
            "Step 476 Loss 0.0059\n",
            "Step 477 Loss 0.0059\n",
            "Step 478 Loss 0.0059\n",
            "Step 479 Loss 0.0059\n",
            "Step 480 Loss 0.0059\n",
            "Step 481 Loss 0.0059\n",
            "Step 482 Loss 0.0058\n",
            "Step 483 Loss 0.0058\n",
            "Step 484 Loss 0.0058\n",
            "Step 485 Loss 0.0058\n",
            "Step 486 Loss 0.0058\n",
            "Step 487 Loss 0.0058\n",
            "Step 488 Loss 0.0058\n",
            "Step 489 Loss 0.0058\n",
            "Step 490 Loss 0.0058\n",
            "Step 491 Loss 0.0058\n",
            "Step 492 Loss 0.0058\n",
            "Step 493 Loss 0.0058\n",
            "Step 494 Loss 0.0057\n",
            "Step 495 Loss 0.0057\n",
            "Step 496 Loss 0.0057\n",
            "Step 497 Loss 0.0057\n",
            "Step 498 Loss 0.0057\n",
            "Step 499 Loss 0.0057\n",
            "Step 500 Loss 0.0057\n",
            "Step 501 Loss 0.0057\n",
            "Step 502 Loss 0.0057\n",
            "Step 503 Loss 0.0057\n",
            "Step 504 Loss 0.0057\n",
            "Step 505 Loss 0.0057\n",
            "Step 506 Loss 0.0057\n",
            "Step 507 Loss 0.0056\n",
            "Step 508 Loss 0.0056\n",
            "Step 509 Loss 0.0056\n",
            "Step 510 Loss 0.0056\n",
            "Step 511 Loss 0.0056\n",
            "Step 512 Loss 0.0056\n",
            "Step 513 Loss 0.0056\n",
            "Step 514 Loss 0.0056\n",
            "Step 515 Loss 0.0056\n",
            "Step 516 Loss 0.0056\n",
            "Step 517 Loss 0.0056\n",
            "Step 518 Loss 0.0056\n",
            "Step 519 Loss 0.0056\n",
            "Step 520 Loss 0.0056\n",
            "Step 521 Loss 0.0056\n",
            "Step 522 Loss 0.0055\n",
            "Step 523 Loss 0.0055\n",
            "Step 524 Loss 0.0055\n",
            "Step 525 Loss 0.0055\n",
            "Step 526 Loss 0.0055\n",
            "Step 527 Loss 0.0055\n",
            "Step 528 Loss 0.0055\n",
            "Step 529 Loss 0.0055\n",
            "Step 530 Loss 0.0055\n",
            "Step 531 Loss 0.0055\n",
            "Step 532 Loss 0.0055\n",
            "Step 533 Loss 0.0055\n",
            "Step 534 Loss 0.0055\n",
            "Step 535 Loss 0.0055\n",
            "Step 536 Loss 0.0055\n",
            "Step 537 Loss 0.0054\n",
            "Step 538 Loss 0.0054\n",
            "Step 539 Loss 0.0054\n",
            "Step 540 Loss 0.0054\n",
            "Step 541 Loss 0.0054\n",
            "Step 542 Loss 0.0054\n",
            "Step 543 Loss 0.0054\n",
            "Step 544 Loss 0.0054\n",
            "Step 545 Loss 0.0054\n",
            "Step 546 Loss 0.0054\n",
            "Step 547 Loss 0.0054\n",
            "Step 548 Loss 0.0054\n",
            "Step 549 Loss 0.0054\n",
            "Step 550 Loss 0.0054\n",
            "Step 551 Loss 0.0054\n",
            "Step 552 Loss 0.0054\n",
            "Step 553 Loss 0.0054\n",
            "Step 554 Loss 0.0053\n",
            "Step 555 Loss 0.0053\n",
            "Step 556 Loss 0.0053\n",
            "Step 557 Loss 0.0053\n",
            "Step 558 Loss 0.0053\n",
            "Step 559 Loss 0.0053\n",
            "Step 560 Loss 0.0053\n",
            "Step 561 Loss 0.0053\n",
            "Step 562 Loss 0.0053\n",
            "Step 563 Loss 0.0053\n",
            "Step 564 Loss 0.0053\n",
            "Step 565 Loss 0.0053\n",
            "Step 566 Loss 0.0053\n",
            "Step 567 Loss 0.0053\n",
            "Step 568 Loss 0.0053\n",
            "Step 569 Loss 0.0053\n",
            "Step 570 Loss 0.0053\n",
            "Step 571 Loss 0.0053\n",
            "Step 572 Loss 0.0052\n",
            "Step 573 Loss 0.0052\n",
            "Step 574 Loss 0.0052\n",
            "Step 575 Loss 0.0052\n",
            "Step 576 Loss 0.0052\n",
            "Step 577 Loss 0.0052\n",
            "Step 578 Loss 0.0052\n",
            "Step 579 Loss 0.0052\n",
            "Step 580 Loss 0.0052\n",
            "Step 581 Loss 0.0052\n",
            "Step 582 Loss 0.0052\n",
            "Step 583 Loss 0.0052\n",
            "Step 584 Loss 0.0052\n",
            "Step 585 Loss 0.0052\n",
            "Step 586 Loss 0.0052\n",
            "Step 587 Loss 0.0052\n",
            "Step 588 Loss 0.0052\n",
            "Step 589 Loss 0.0052\n",
            "Step 590 Loss 0.0052\n",
            "Step 591 Loss 0.0052\n",
            "Step 592 Loss 0.0051\n",
            "Step 593 Loss 0.0051\n",
            "Step 594 Loss 0.0051\n",
            "Step 595 Loss 0.0051\n",
            "Step 596 Loss 0.0051\n",
            "Step 597 Loss 0.0051\n",
            "Step 598 Loss 0.0051\n",
            "Step 599 Loss 0.0051\n",
            "Step 600 Loss 0.0051\n",
            "Step 601 Loss 0.0051\n",
            "Step 602 Loss 0.0051\n",
            "Step 603 Loss 0.0051\n",
            "Step 604 Loss 0.0051\n",
            "Step 605 Loss 0.0051\n",
            "Step 606 Loss 0.0051\n",
            "Step 607 Loss 0.0051\n",
            "Step 608 Loss 0.0051\n",
            "Step 609 Loss 0.0051\n",
            "Step 610 Loss 0.0051\n",
            "Step 611 Loss 0.0051\n",
            "Step 612 Loss 0.0051\n",
            "Step 613 Loss 0.0051\n",
            "Step 614 Loss 0.0050\n",
            "Step 615 Loss 0.0050\n",
            "Step 616 Loss 0.0050\n",
            "Step 617 Loss 0.0050\n",
            "Step 618 Loss 0.0050\n",
            "Step 619 Loss 0.0050\n",
            "Step 620 Loss 0.0050\n",
            "Step 621 Loss 0.0050\n",
            "Step 622 Loss 0.0050\n",
            "Step 623 Loss 0.0050\n",
            "Step 624 Loss 0.0050\n",
            "Step 625 Loss 0.0050\n",
            "Step 626 Loss 0.0050\n",
            "Step 627 Loss 0.0050\n",
            "Step 628 Loss 0.0050\n",
            "Step 629 Loss 0.0050\n",
            "Step 630 Loss 0.0050\n",
            "Step 631 Loss 0.0050\n",
            "Step 632 Loss 0.0050\n",
            "Step 633 Loss 0.0050\n",
            "Step 634 Loss 0.0050\n",
            "Step 635 Loss 0.0050\n",
            "Step 636 Loss 0.0050\n",
            "Step 637 Loss 0.0050\n",
            "Step 638 Loss 0.0049\n",
            "Step 639 Loss 0.0049\n",
            "Step 640 Loss 0.0049\n",
            "Step 641 Loss 0.0049\n",
            "Step 642 Loss 0.0049\n",
            "Step 643 Loss 0.0049\n",
            "Step 644 Loss 0.0049\n",
            "Step 645 Loss 0.0049\n",
            "Step 646 Loss 0.0049\n",
            "Step 647 Loss 0.0049\n",
            "Step 648 Loss 0.0049\n",
            "Step 649 Loss 0.0049\n",
            "Step 650 Loss 0.0049\n",
            "Step 651 Loss 0.0049\n",
            "Step 652 Loss 0.0049\n",
            "Step 653 Loss 0.0049\n",
            "Step 654 Loss 0.0049\n",
            "Step 655 Loss 0.0049\n",
            "Step 656 Loss 0.0049\n",
            "Step 657 Loss 0.0049\n",
            "Step 658 Loss 0.0049\n",
            "Step 659 Loss 0.0049\n",
            "Step 660 Loss 0.0049\n",
            "Step 661 Loss 0.0049\n",
            "Step 662 Loss 0.0049\n",
            "Step 663 Loss 0.0049\n",
            "Step 664 Loss 0.0048\n",
            "Step 665 Loss 0.0048\n",
            "Step 666 Loss 0.0048\n",
            "Step 667 Loss 0.0048\n",
            "Step 668 Loss 0.0048\n",
            "Step 669 Loss 0.0048\n",
            "Step 670 Loss 0.0048\n",
            "Step 671 Loss 0.0048\n",
            "Step 672 Loss 0.0048\n",
            "Step 673 Loss 0.0048\n",
            "Step 674 Loss 0.0048\n",
            "Step 675 Loss 0.0048\n",
            "Step 676 Loss 0.0048\n",
            "Step 677 Loss 0.0048\n",
            "Step 678 Loss 0.0048\n",
            "Step 679 Loss 0.0048\n",
            "Step 680 Loss 0.0048\n",
            "Step 681 Loss 0.0048\n",
            "Step 682 Loss 0.0048\n",
            "Step 683 Loss 0.0048\n",
            "Step 684 Loss 0.0048\n",
            "Step 685 Loss 0.0048\n",
            "Step 686 Loss 0.0048\n",
            "Step 687 Loss 0.0048\n",
            "Step 688 Loss 0.0048\n",
            "Step 689 Loss 0.0048\n",
            "Step 690 Loss 0.0048\n",
            "Step 691 Loss 0.0048\n",
            "Step 692 Loss 0.0048\n",
            "Step 693 Loss 0.0048\n",
            "Step 694 Loss 0.0047\n",
            "Step 695 Loss 0.0047\n",
            "Step 696 Loss 0.0047\n",
            "Step 697 Loss 0.0047\n",
            "Step 698 Loss 0.0047\n",
            "Step 699 Loss 0.0047\n",
            "Step 700 Loss 0.0047\n",
            "Step 701 Loss 0.0047\n",
            "Step 702 Loss 0.0047\n",
            "Step 703 Loss 0.0047\n",
            "Step 704 Loss 0.0047\n",
            "Step 705 Loss 0.0047\n",
            "Step 706 Loss 0.0047\n",
            "Step 707 Loss 0.0047\n",
            "Step 708 Loss 0.0047\n",
            "Step 709 Loss 0.0047\n",
            "Step 710 Loss 0.0047\n",
            "Step 711 Loss 0.0047\n",
            "Step 712 Loss 0.0047\n",
            "Step 713 Loss 0.0047\n",
            "Step 714 Loss 0.0047\n",
            "Step 715 Loss 0.0047\n",
            "Step 716 Loss 0.0047\n",
            "Step 717 Loss 0.0047\n",
            "Step 718 Loss 0.0047\n",
            "Step 719 Loss 0.0047\n",
            "Step 720 Loss 0.0047\n",
            "Step 721 Loss 0.0047\n",
            "Step 722 Loss 0.0047\n",
            "Step 723 Loss 0.0047\n",
            "Step 724 Loss 0.0047\n",
            "Step 725 Loss 0.0047\n",
            "Step 726 Loss 0.0047\n",
            "Step 727 Loss 0.0047\n",
            "Step 728 Loss 0.0046\n",
            "Step 729 Loss 0.0046\n",
            "Step 730 Loss 0.0046\n",
            "Step 731 Loss 0.0046\n",
            "Step 732 Loss 0.0046\n",
            "Step 733 Loss 0.0046\n",
            "Step 734 Loss 0.0046\n",
            "Step 735 Loss 0.0046\n",
            "Step 736 Loss 0.0046\n",
            "Step 737 Loss 0.0046\n",
            "Step 738 Loss 0.0046\n",
            "Step 739 Loss 0.0046\n",
            "Step 740 Loss 0.0046\n",
            "Step 741 Loss 0.0046\n",
            "Step 742 Loss 0.0046\n",
            "Step 743 Loss 0.0046\n",
            "Step 744 Loss 0.0046\n",
            "Step 745 Loss 0.0046\n",
            "Step 746 Loss 0.0046\n",
            "Step 747 Loss 0.0046\n",
            "Step 748 Loss 0.0046\n",
            "Step 749 Loss 0.0046\n",
            "Step 750 Loss 0.0046\n",
            "Step 751 Loss 0.0046\n",
            "Step 752 Loss 0.0046\n",
            "Step 753 Loss 0.0046\n",
            "Step 754 Loss 0.0046\n",
            "Step 755 Loss 0.0046\n",
            "Step 756 Loss 0.0046\n",
            "Step 757 Loss 0.0046\n",
            "Step 758 Loss 0.0046\n",
            "Step 759 Loss 0.0046\n",
            "Step 760 Loss 0.0046\n",
            "Step 761 Loss 0.0046\n",
            "Step 762 Loss 0.0046\n",
            "Step 763 Loss 0.0046\n",
            "Step 764 Loss 0.0046\n",
            "Step 765 Loss 0.0046\n",
            "Step 766 Loss 0.0045\n",
            "Step 767 Loss 0.0045\n",
            "Step 768 Loss 0.0045\n",
            "Step 769 Loss 0.0045\n",
            "Step 770 Loss 0.0045\n",
            "Step 771 Loss 0.0045\n",
            "Step 772 Loss 0.0045\n",
            "Step 773 Loss 0.0045\n",
            "Step 774 Loss 0.0045\n",
            "Step 775 Loss 0.0045\n",
            "Step 776 Loss 0.0045\n",
            "Step 777 Loss 0.0045\n",
            "Step 778 Loss 0.0045\n",
            "Step 779 Loss 0.0045\n",
            "Step 780 Loss 0.0045\n",
            "Step 781 Loss 0.0045\n",
            "Step 782 Loss 0.0045\n",
            "Step 783 Loss 0.0045\n",
            "Step 784 Loss 0.0045\n",
            "Step 785 Loss 0.0045\n",
            "Step 786 Loss 0.0045\n",
            "Step 787 Loss 0.0045\n",
            "Step 788 Loss 0.0046\n",
            "Step 789 Loss 0.0046\n",
            "Step 790 Loss 0.0098\n",
            "Step 791 Loss 0.0195\n",
            "Step 792 Loss 1.0650\n",
            "Step 793 Loss 2.7022\n",
            "Step 794 Loss 3.1814\n",
            "Step 795 Loss 1.6503\n",
            "Step 796 Loss 1.1788\n",
            "Step 797 Loss 1.1949\n",
            "Step 798 Loss 1.1157\n",
            "Step 799 Loss 1.0183\n",
            "Step 800 Loss 0.8430\n",
            "Step 801 Loss 0.7035\n",
            "Step 802 Loss 0.5871\n",
            "Step 803 Loss 0.4938\n",
            "Step 804 Loss 0.4329\n",
            "Step 805 Loss 0.3670\n",
            "Step 806 Loss 0.3171\n",
            "Step 807 Loss 0.2788\n",
            "Step 808 Loss 0.2288\n",
            "Step 809 Loss 0.1935\n",
            "Step 810 Loss 0.1744\n",
            "Step 811 Loss 0.1511\n",
            "Step 812 Loss 0.1272\n",
            "Step 813 Loss 0.1094\n",
            "Step 814 Loss 0.0960\n",
            "Step 815 Loss 0.0848\n",
            "Step 816 Loss 0.0753\n",
            "Step 817 Loss 0.0676\n",
            "Step 818 Loss 0.0612\n",
            "Step 819 Loss 0.0553\n",
            "Step 820 Loss 0.0497\n",
            "Step 821 Loss 0.0448\n",
            "Step 822 Loss 0.0409\n",
            "Step 823 Loss 0.0378\n",
            "Step 824 Loss 0.0352\n",
            "Step 825 Loss 0.0329\n",
            "Step 826 Loss 0.0308\n",
            "Step 827 Loss 0.0289\n",
            "Step 828 Loss 0.0271\n",
            "Step 829 Loss 0.0255\n",
            "Step 830 Loss 0.0241\n",
            "Step 831 Loss 0.0229\n",
            "Step 832 Loss 0.0218\n",
            "Step 833 Loss 0.0209\n",
            "Step 834 Loss 0.0201\n",
            "Step 835 Loss 0.0193\n",
            "Step 836 Loss 0.0186\n",
            "Step 837 Loss 0.0180\n",
            "Step 838 Loss 0.0173\n",
            "Step 839 Loss 0.0168\n",
            "Step 840 Loss 0.0163\n",
            "Step 841 Loss 0.0158\n",
            "Step 842 Loss 0.0154\n",
            "Step 843 Loss 0.0150\n",
            "Step 844 Loss 0.0146\n",
            "Step 845 Loss 0.0142\n",
            "Step 846 Loss 0.0139\n",
            "Step 847 Loss 0.0136\n",
            "Step 848 Loss 0.0133\n",
            "Step 849 Loss 0.0131\n",
            "Step 850 Loss 0.0128\n",
            "Step 851 Loss 0.0126\n",
            "Step 852 Loss 0.0124\n",
            "Step 853 Loss 0.0121\n",
            "Step 854 Loss 0.0119\n",
            "Step 855 Loss 0.0118\n",
            "Step 856 Loss 0.0116\n",
            "Step 857 Loss 0.0114\n",
            "Step 858 Loss 0.0113\n",
            "Step 859 Loss 0.0111\n",
            "Step 860 Loss 0.0110\n",
            "Step 861 Loss 0.0108\n",
            "Step 862 Loss 0.0107\n",
            "Step 863 Loss 0.0106\n",
            "Step 864 Loss 0.0105\n",
            "Step 865 Loss 0.0104\n",
            "Step 866 Loss 0.0103\n",
            "Step 867 Loss 0.0102\n",
            "Step 868 Loss 0.0101\n",
            "Step 869 Loss 0.0100\n",
            "Step 870 Loss 0.0099\n",
            "Step 871 Loss 0.0098\n",
            "Step 872 Loss 0.0097\n",
            "Step 873 Loss 0.0096\n",
            "Step 874 Loss 0.0096\n",
            "Step 875 Loss 0.0095\n",
            "Step 876 Loss 0.0094\n",
            "Step 877 Loss 0.0093\n",
            "Step 878 Loss 0.0093\n",
            "Step 879 Loss 0.0092\n",
            "Step 880 Loss 0.0091\n",
            "Step 881 Loss 0.0091\n",
            "Step 882 Loss 0.0090\n",
            "Step 883 Loss 0.0089\n",
            "Step 884 Loss 0.0089\n",
            "Step 885 Loss 0.0088\n",
            "Step 886 Loss 0.0088\n",
            "Step 887 Loss 0.0087\n",
            "Step 888 Loss 0.0087\n",
            "Step 889 Loss 0.0086\n",
            "Step 890 Loss 0.0086\n",
            "Step 891 Loss 0.0085\n",
            "Step 892 Loss 0.0085\n",
            "Step 893 Loss 0.0084\n",
            "Step 894 Loss 0.0084\n",
            "Step 895 Loss 0.0083\n",
            "Step 896 Loss 0.0083\n",
            "Step 897 Loss 0.0083\n",
            "Step 898 Loss 0.0082\n",
            "Step 899 Loss 0.0082\n",
            "Step 900 Loss 0.0081\n",
            "Step 901 Loss 0.0081\n",
            "Step 902 Loss 0.0081\n",
            "Step 903 Loss 0.0080\n",
            "Step 904 Loss 0.0080\n",
            "Step 905 Loss 0.0079\n",
            "Step 906 Loss 0.0079\n",
            "Step 907 Loss 0.0079\n",
            "Step 908 Loss 0.0078\n",
            "Step 909 Loss 0.0078\n",
            "Step 910 Loss 0.0078\n",
            "Step 911 Loss 0.0077\n",
            "Step 912 Loss 0.0077\n",
            "Step 913 Loss 0.0077\n",
            "Step 914 Loss 0.0076\n",
            "Step 915 Loss 0.0076\n",
            "Step 916 Loss 0.0076\n",
            "Step 917 Loss 0.0076\n",
            "Step 918 Loss 0.0075\n",
            "Step 919 Loss 0.0075\n",
            "Step 920 Loss 0.0075\n",
            "Step 921 Loss 0.0074\n",
            "Step 922 Loss 0.0074\n",
            "Step 923 Loss 0.0074\n",
            "Step 924 Loss 0.0074\n",
            "Step 925 Loss 0.0073\n",
            "Step 926 Loss 0.0073\n",
            "Step 927 Loss 0.0073\n",
            "Step 928 Loss 0.0073\n",
            "Step 929 Loss 0.0072\n",
            "Step 930 Loss 0.0072\n",
            "Step 931 Loss 0.0072\n",
            "Step 932 Loss 0.0072\n",
            "Step 933 Loss 0.0071\n",
            "Step 934 Loss 0.0071\n",
            "Step 935 Loss 0.0071\n",
            "Step 936 Loss 0.0071\n",
            "Step 937 Loss 0.0070\n",
            "Step 938 Loss 0.0070\n",
            "Step 939 Loss 0.0070\n",
            "Step 940 Loss 0.0070\n",
            "Step 941 Loss 0.0070\n",
            "Step 942 Loss 0.0069\n",
            "Step 943 Loss 0.0069\n",
            "Step 944 Loss 0.0069\n",
            "Step 945 Loss 0.0069\n",
            "Step 946 Loss 0.0068\n",
            "Step 947 Loss 0.0068\n",
            "Step 948 Loss 0.0068\n",
            "Step 949 Loss 0.0068\n",
            "Step 950 Loss 0.0068\n",
            "Step 951 Loss 0.0068\n",
            "Step 952 Loss 0.0067\n",
            "Step 953 Loss 0.0067\n",
            "Step 954 Loss 0.0067\n",
            "Step 955 Loss 0.0067\n",
            "Step 956 Loss 0.0067\n",
            "Step 957 Loss 0.0066\n",
            "Step 958 Loss 0.0066\n",
            "Step 959 Loss 0.0066\n",
            "Step 960 Loss 0.0066\n",
            "Step 961 Loss 0.0066\n",
            "Step 962 Loss 0.0066\n",
            "Step 963 Loss 0.0065\n",
            "Step 964 Loss 0.0065\n",
            "Step 965 Loss 0.0065\n",
            "Step 966 Loss 0.0065\n",
            "Step 967 Loss 0.0065\n",
            "Step 968 Loss 0.0065\n",
            "Step 969 Loss 0.0064\n",
            "Step 970 Loss 0.0064\n",
            "Step 971 Loss 0.0064\n",
            "Step 972 Loss 0.0064\n",
            "Step 973 Loss 0.0064\n",
            "Step 974 Loss 0.0064\n",
            "Step 975 Loss 0.0064\n",
            "Step 976 Loss 0.0063\n",
            "Step 977 Loss 0.0063\n",
            "Step 978 Loss 0.0063\n",
            "Step 979 Loss 0.0063\n",
            "Step 980 Loss 0.0063\n",
            "Step 981 Loss 0.0063\n",
            "Step 982 Loss 0.0063\n",
            "Step 983 Loss 0.0062\n",
            "Step 984 Loss 0.0062\n",
            "Step 985 Loss 0.0062\n",
            "Step 986 Loss 0.0062\n",
            "Step 987 Loss 0.0062\n",
            "Step 988 Loss 0.0062\n",
            "Step 989 Loss 0.0062\n",
            "Step 990 Loss 0.0061\n",
            "Step 991 Loss 0.0061\n",
            "Step 992 Loss 0.0061\n",
            "Step 993 Loss 0.0061\n",
            "Step 994 Loss 0.0061\n",
            "Step 995 Loss 0.0061\n",
            "Step 996 Loss 0.0061\n",
            "Step 997 Loss 0.0061\n",
            "Step 998 Loss 0.0060\n",
            "Step 999 Loss 0.0060\n",
            "Generated indices: [37, 34, 27, 26, 39, 10, 1, 32, 70, 70, 59, 1, 68, 80, 1, 67, 70, 73, 10, 0, 1, 62, 64, 75, 63, 56, 75, 63, 63, 60, 60, 73, 70, 32, 34, 78, 63, 56, 75, 62, 63, 56, 77, 60, 1, 56, 75, 1, 1, 62, 62, 70, 73, 68, 1, 75, 70, 68, 64, 75, 10, 1, 1, 75, 70, 73, 73, 61, 70, 70, 70, 0, 1, 1, 1, 1, 1, 61, 73, 45, 10, 1, 68, 80, 1, 75, 63, 60, 1, 68, 61, 1, 75, 63, 60, 70, 77, 64, 67, 67, 1, 59, 80, 1, 75, 73, 70, 67, 10, 0, 1, 73, 70, 70, 75, 63, 60, 1, 75, 75, 63, 56, 73, 30, 45, 40, 39, 34]\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model FLOP utilization"
      ],
      "metadata": {
        "id": "78Ui6fHVkWeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = 1.0 # Placeholder: In a real scenario, this would be the time taken for one iteration in seconds\n",
        "mfu = model.model_flops(batch_size * 1, dt)\n",
        "print(mfu)\n",
        "print(f\" Model Flop Utilization: {mfu*100:.10f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPYkKsCqkVoq",
        "outputId": "c746b641-da3c-41ab-af4c-4e00054e31b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00043548672\n",
            " Model Flop Utilization: 0.0435486720%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Text"
      ],
      "metadata": {
        "id": "6gppzbAtjzya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# -------------------- Text Generation --------------------\n",
        "def generate_text(model, start_seq, idx2char, length= 100, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    kv = KVCache(model.num_layers, max_len=model.block_size * 5,\n",
        "                 device=device, dtype=next(model.parameters()).dtype)\n",
        "    kv.reset()\n",
        "\n",
        "    generated = [int(start_seq[0, 0].item())]\n",
        "    input_seq = start_seq.clone()\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        # Only pass the last block_size tokens to the model\n",
        "        input_for_model = input_seq[:, -model.block_size:]\n",
        "\n",
        "        logits = model(input_for_model, kv_cache=kv)\n",
        "        last = logits[:, -1, :] / temperature\n",
        "        probs = F.softmax(last, dim=-1)\n",
        "        nxt = torch.multinomial(probs, num_samples=1)\n",
        "        generated.append(int(nxt.item()))\n",
        "        input_seq = torch.cat([input_seq, nxt], dim=1)\n",
        "        kv.advance(1)\n",
        "\n",
        "    decoded_text = ''.join([idx2char[i] for i in generated])\n",
        "    return generated, decoded_text\n",
        "\n",
        "# Start generation with the first token of batch 0\n",
        "start = x[0:1, 0:1]\n",
        "indices, text = generate_text(model, start, integer2string, length=100, temperature=0.8)\n",
        "print(\"Generated indices:\", indices)\n",
        "print(\"Generated text:\", repr(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlhXj7TUj0gR",
        "outputId": "dc0a2336-b2f2-4334-fbe1-2e5b616c246a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated indices: [37, 34, 27, 26, 39, 10, 1, 32, 70, 70, 59, 1, 68, 80, 1, 67, 70, 73, 70, 80, 1, 62, 64, 73, 70, 61, 1, 40, 68, 60, 60, 73, 80, 56, 75, 63, 80, 1, 75, 63, 80, 1, 63, 56, 75, 63, 56, 71, 1, 1, 75, 70, 73, 1, 75, 63, 70, 68, 64, 75, 63, 56, 67, 67, 56, 8, 1, 61, 70, 73, 34, 74, 1, 58, 63, 64, 75, 60, 77, 64, 68, 80, 1, 75, 78, 9, 75, 63, 56, 73, 60, 1, 61, 70, 73, 70, 70, 70, 73, 70]\n",
            "Generated text: 'LIBAN. Good my loroy girof Omeeryathy thy hathap  tor thomithalla, forIs chitevimy tw-thare foroooro'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Symbolic Entropy from Deepseek paper"
      ],
      "metadata": {
        "id": "Cs4WEb3fkInw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "ent = entropy(indices, base=2)\n",
        "print(\"Entropy:\", ent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yszJ6G1pkLIJ",
        "outputId": "fed24b51-d2b3-41e6-daec-26ace81b5719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy: 6.386784803153935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLOP estimation"
      ],
      "metadata": {
        "id": "Dnd_9bh_kBvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- FLOPs Estimation --------------------\n",
        "def estimate_flops(seq_len, training=True):\n",
        "    d = embedded_dim\n",
        "    h = num_heads\n",
        "    L = num_layers\n",
        "\n",
        "    attn = 2 * h * seq_len * (d // h) * seq_len   # QK + AV\n",
        "    mlp = 8 * d * d                              # FFN\n",
        "    proj = 4 * d * d                             # Q,K,V,O\n",
        "\n",
        "    flops_per_layer = attn + mlp + proj\n",
        "    total = L * flops_per_layer * seq_len\n",
        "\n",
        "    if training:\n",
        "        total *= 3  # forward + backward + grad\n",
        "    return total/1e15\n",
        "\n",
        "# -------------------- Training --------------------\n",
        "model = BabyGPT(vocab_size, block_size, num_layers, embedded_dim, num_heads, num_kv_heads, device).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Parameters:\", sum(p.numel() for p in model.parameters()))\n",
        "print(\"Estimated training FLOPs per batch:\", estimate_flops(block_size))\n",
        "\n",
        "for step in range(200):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]).to(device)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    _, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if step % 5 == 0:\n",
        "        print(f\"Step {step:02d} | Loss {loss.item():.4f}\")\n",
        "\n",
        "# -------------------- KV Inference Demo --------------------\n",
        "kv = KVCache(\n",
        "    num_layers,\n",
        "    max_len=block_size * 4,\n",
        "    device=device,\n",
        "    dtype=next(model.parameters()).dtype,\n",
        ")\n",
        "kv.reset()\n",
        "\n",
        "start = x[:1, :1]\n",
        "generated = [start.item()]\n",
        "for _ in range(100):\n",
        "    logits = model(start, kv_cache=kv)\n",
        "    nxt = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), 1)\n",
        "    generated.append(int(nxt))\n",
        "    start = torch.cat([start, nxt], dim=1)\n",
        "    kv.advance(1)\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(decode(generated))\n",
        "print(\"Estimated inference FLOPs per token:\", estimate_flops(1, training=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj3M2y72kErx",
        "outputId": "0d79f22e-c19a-46f6-d3d2-8bef5f77353f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 217984\n",
            "Estimated training FLOPs per batch: 3.296722944e-06\n",
            "Step 00 | Loss 4.5977\n",
            "Step 05 | Loss 3.8462\n",
            "Step 10 | Loss 3.3426\n",
            "Step 15 | Loss 3.2117\n",
            "Step 20 | Loss 3.2306\n",
            "Step 25 | Loss 3.1958\n",
            "Step 30 | Loss 3.0174\n",
            "Step 35 | Loss 3.0532\n",
            "Step 40 | Loss 2.8810\n",
            "Step 45 | Loss 2.9393\n",
            "Step 50 | Loss 2.9168\n",
            "Step 55 | Loss 2.8632\n",
            "Step 60 | Loss 2.9080\n",
            "Step 65 | Loss 2.8209\n",
            "Step 70 | Loss 2.7476\n",
            "Step 75 | Loss 2.8372\n",
            "Step 80 | Loss 2.7588\n",
            "Step 85 | Loss 2.6830\n",
            "Step 90 | Loss 2.6403\n",
            "Step 95 | Loss 2.6477\n",
            "Step 100 | Loss 2.6126\n",
            "Step 105 | Loss 2.4996\n",
            "Step 110 | Loss 2.4822\n",
            "Step 115 | Loss 2.5283\n",
            "Step 120 | Loss 2.4235\n",
            "Step 125 | Loss 2.4566\n",
            "Step 130 | Loss 2.4275\n",
            "Step 135 | Loss 2.4796\n",
            "Step 140 | Loss 2.3579\n",
            "Step 145 | Loss 2.4036\n",
            "Step 150 | Loss 2.3455\n",
            "Step 155 | Loss 2.3454\n",
            "Step 160 | Loss 2.4982\n",
            "Step 165 | Loss 2.5053\n",
            "Step 170 | Loss 2.2623\n",
            "Step 175 | Loss 2.2802\n",
            "Step 180 | Loss 2.2444\n",
            "Step 185 | Loss 2.2987\n",
            "Step 190 | Loss 2.2363\n",
            "Step 195 | Loss 2.3210\n",
            "Generated text:\n",
            "et\n",
            "    A.\n",
            "    Nhe| amy fey tngave cheveles'sg tish neath yath, tid ponor Whe andumjavee tird Whire )o\n",
            "Estimated inference FLOPs per token: 1.9712e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kv cache with mqa, rope\n",
        "\n",
        "\n",
        "RoPE (Rotary Positional Embeddings) and Multi-Query Attention (MQA) are two of the most critical optimizations used in modern Large Language Models (LLMs) like Llama 3 and Mistral. While RoPE changes how the model understands the order of words, MQA changes how the model remembers those words during generation.\n",
        "\n",
        "1. Rotary Positional Embeddings (RoPE)\n",
        "In standard Transformers, the model has no inherent sense of word order. To fix this, we use positional embeddings. RoPE is a \"relative\" positional encoding that uses geometry to help the model understand how far apart two words are.\n",
        "\n",
        "\n",
        "Instead of adding a fixed vector to the word embedding (like the original Transformer did), RoPE rotates the Query ($Q$) and Key ($K$) vectors in a high-dimensional space.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w3MPSIS6rNe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "words = open(r\"/content/shakespeare.txt\", 'r', encoding='utf-8').read().split()\n",
        "\n",
        "chars = sorted(list(set(words)))\n",
        "string2integer = {ch: i for i, ch in enumerate(chars)}\n",
        "integer2string = {i: ch for ch, i in string2integer.items()}\n",
        "encode = lambda s: [string2integer[c] for c in s]\n",
        "decode = lambda l: ''.join([integer2string[i] for i in l])\n",
        "data = torch.tensor(encode(words), dtype=torch.long)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 16\n",
        "embedded_dim = 16\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "# Rotary Position Embedding (RoPE)\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, seq_len, device):\n",
        "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb.cos(), emb.sin()\n",
        "\n",
        "\n",
        "def apply_rotary_emb(x, cos, sin):\n",
        "    \"\"\"Apply rotary embeddings to input tensor\"\"\"\n",
        "    # x shape: (B, num_heads, T, head_dim)\n",
        "    B, nh, T, hd = x.shape\n",
        "\n",
        "    # Split into even and odd features\n",
        "    x1 = x[..., 0::2]  # (B, nh, T, hd//2)\n",
        "    x2 = x[..., 1::2]  # (B, nh, T, hd//2)\n",
        "\n",
        "    # cos, sin shape: (T, hd)\n",
        "    # We need them to be (T, hd//2) since we split x\n",
        "    cos = cos[:T, :hd//2]  # (T, hd//2)\n",
        "    sin = sin[:T, :hd//2]  # (T, hd//2)\n",
        "\n",
        "    # Reshape for broadcasting: (1, 1, T, hd//2)\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply rotation\n",
        "    rotated_x1 = x1 * cos - x2 * sin\n",
        "    rotated_x2 = x1 * sin + x2 * cos\n",
        "\n",
        "    # Interleave back\n",
        "    rotated = torch.stack([rotated_x1, rotated_x2], dim=-1)  # (B, nh, T, hd//2, 2)\n",
        "    rotated = rotated.flatten(-2)  # (B, nh, T, hd)\n",
        "\n",
        "    return rotated\n",
        "\n",
        "\n",
        "# Multi-Query Attention with KV Cache\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, embedded_dim, num_heads):\n",
        "        super(MultiQueryAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embedded_dim = embedded_dim\n",
        "        self.head_dim = embedded_dim // num_heads\n",
        "\n",
        "        assert embedded_dim % num_heads == 0, \"embedded_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Query has multiple heads, Key and Value have single head (MQA)\n",
        "        self.q_proj = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.k_proj = nn.Linear(embedded_dim, self.head_dim)\n",
        "        self.v_proj = nn.Linear(embedded_dim, self.head_dim)\n",
        "\n",
        "        self.projection = nn.Linear(embedded_dim, embedded_dim)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(1000, 1000)))  # Large buffer\n",
        "\n",
        "        # Rotary embeddings\n",
        "        self.rotary_emb = RotaryEmbedding(self.head_dim)\n",
        "\n",
        "        # Store attention weights\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, x, use_cache=False, past_kv=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        q = self.q_proj(x)  # (B, T, C)\n",
        "        k = self.k_proj(x)  # (B, T, head_dim)\n",
        "        v = self.v_proj(x)  # (B, T, head_dim)\n",
        "\n",
        "        # Reshape Q to multi-head\n",
        "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, nh, T, hd)\n",
        "\n",
        "        # Reshape K and V to single head\n",
        "        k = k.view(B, T, 1, self.head_dim).transpose(1, 2)  # (B, 1, T, hd)\n",
        "        v = v.view(B, T, 1, self.head_dim).transpose(1, 2)  # (B, 1, T, hd)\n",
        "\n",
        "        # Apply rotary embeddings to Q and K\n",
        "        cos, sin = self.rotary_emb(T, x.device)\n",
        "        q = apply_rotary_emb(q, cos, sin)\n",
        "        k = apply_rotary_emb(k, cos, sin)\n",
        "\n",
        "        # Handle KV cache for inference\n",
        "        if use_cache and past_kv is not None:\n",
        "            past_k, past_v = past_kv\n",
        "            k = torch.cat([past_k, k], dim=2)  # Concatenate along sequence dimension\n",
        "            v = torch.cat([past_v, v], dim=2)\n",
        "\n",
        "        current_kv = (k, v) if use_cache else None\n",
        "\n",
        "        # Get current sequence lengths\n",
        "        q_len = q.size(2)\n",
        "        kv_len = k.size(2)\n",
        "\n",
        "        # Expand K and V to match number of query heads (MQA)\n",
        "        k = k.expand(B, self.num_heads, kv_len, self.head_dim)  # (B, nh, kv_len, hd)\n",
        "        v = v.expand(B, self.num_heads, kv_len, self.head_dim)  # (B, nh, kv_len, hd)\n",
        "\n",
        "        # Compute attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))  # (B, nh, q_len, kv_len)\n",
        "\n",
        "        # Apply causal mask\n",
        "        mask = self.tril[:q_len, :kv_len]  # (q_len, kv_len)\n",
        "        att = att.masked_fill(mask == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        self.attention_weights = att\n",
        "\n",
        "        y = att @ v  # (B, nh, q_len, hd)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, q_len, C)  # (B, q_len, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.projection(y)\n",
        "\n",
        "        if use_cache:\n",
        "            return y, current_kv\n",
        "        return y\n",
        "\n",
        "\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedded_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedded_dim, 4 * embedded_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embedded_dim, embedded_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedded_dim, num_heads):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.attention = MultiQueryAttention(embedded_dim, num_heads)\n",
        "        self.feed_forward = FeedForward(embedded_dim)\n",
        "        self.layer_norm_1 = nn.LayerNorm(embedded_dim)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embedded_dim)\n",
        "\n",
        "    def forward(self, x, use_cache=False, past_kv=None):\n",
        "        if use_cache:\n",
        "            attn_out, current_kv = self.attention(self.layer_norm_1(x), use_cache=True, past_kv=past_kv)\n",
        "            x = x + attn_out\n",
        "            x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "            return x, current_kv\n",
        "        else:\n",
        "            x = x + self.attention(self.layer_norm_1(x))\n",
        "            x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "            return x\n",
        "\n",
        "\n",
        "class BabyGPTmodel(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, num_layers, embedded_dim, num_heads):\n",
        "        super(BabyGPTmodel, self).__init__()\n",
        "        self.token = nn.Embedding(vocab_size, embedded_dim)\n",
        "        self.positional_embeddings = nn.Embedding(1000, embedded_dim)  # Large position buffer\n",
        "        self.layers1 = nn.ModuleList([Transformer(embedded_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embedded_dim, eps=1e-12)\n",
        "        self.ln_head = nn.Linear(embedded_dim, vocab_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Init weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('projection.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_layers))\n",
        "\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, use_cache=False, past_kvs=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "\n",
        "        # Get token embeddings\n",
        "        tok_emb = self.token(idx)  # (B, T, C)\n",
        "\n",
        "        # Handle position embeddings\n",
        "        if use_cache and past_kvs is not None and past_kvs[0] is not None:\n",
        "            # When using cache, calculate position based on cached sequence length\n",
        "            past_len = past_kvs[0][0].size(2)  # Get length from cached keys\n",
        "            position_ids = torch.arange(past_len, past_len + t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        else:\n",
        "            # Normal forward pass or first cached pass\n",
        "            position_ids = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        pos_emb = self.positional_embeddings(position_ids)  # (1, T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        current_kvs = []\n",
        "        for i, layer in enumerate(self.layers1):\n",
        "            past_kv = past_kvs[i] if (past_kvs is not None and i < len(past_kvs)) else None\n",
        "            if use_cache:\n",
        "                x, current_kv = layer(x, use_cache=True, past_kv=past_kv)\n",
        "                current_kvs.append(current_kv)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        # Final layer norm and output projection\n",
        "        x = self.ln_f(x)  # (B, T, C)\n",
        "        logits = self.ln_head(x[:, -1, :])  # (B, vocab_size) - only last token\n",
        "\n",
        "        if use_cache:\n",
        "            return logits, current_kvs\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Generate text with KV caching for efficiency\"\"\"\n",
        "        past_kvs = None\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # For first iteration, use full context (up to block_size)\n",
        "            # For subsequent iterations with cache, only use the last token\n",
        "            if past_kvs is None:\n",
        "                idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "                logits, past_kvs = self(idx_cond, use_cache=True, past_kvs=None)\n",
        "            else:\n",
        "                # Only pass the last token when using cache\n",
        "                logits, past_kvs = self(idx[:, -1:], use_cache=True, past_kvs=past_kvs)\n",
        "\n",
        "            # Apply temperature\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Optionally apply top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Sample from distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "# Generate training data\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "y = torch.stack([data[i+block_size] for i in ix])\n",
        "print(\"Training data shapes:\", x.shape, y.shape)\n",
        "\n",
        "# Initialize model\n",
        "gpt = BabyGPTmodel(vocab_size, block_size, num_layers, embedded_dim, num_heads)\n",
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nStarting training...\")\n",
        "for i in range(1000):\n",
        "    logits = gpt(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Step {i}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Example generation with KV cache\n",
        "print(\"\\nGenerating text with KV cache...\")\n",
        "context = torch.tensor([encode(words[:block_size])], dtype=torch.long)\n",
        "generated = gpt.generate(context, max_new_tokens=20, temperature=0.8, top_k=10)\n",
        "generated_text = decode(generated[0].tolist())\n",
        "print(f\"Generated: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emSxY6n3qfkx",
        "outputId": "c0875c42-fda0-4d13-8ce9-7289985d155f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shapes: torch.Size([16, 8]) torch.Size([16])\n",
            "number of parameters: 2242051\n",
            "\n",
            "Starting training...\n",
            "Step 0, Loss: 11.1751\n",
            "Step 100, Loss: 6.8822\n",
            "Step 200, Loss: 2.8090\n",
            "Step 300, Loss: 1.8427\n",
            "Step 400, Loss: 0.8607\n",
            "Step 500, Loss: 0.3431\n",
            "Step 600, Loss: 0.1717\n",
            "Step 700, Loss: 0.1065\n",
            "Step 800, Loss: 0.0753\n",
            "Step 900, Loss: 0.0575\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Generating text with KV cache...\n",
            "Generated: Fromfairestcreatureswedesireincrease,Thattherebypurse.thythythyeightYetthythyathythythyamutualthyeightpurse.eightYetthy\n"
          ]
        }
      ]
    }
  ]
}